{
  "video_id": "DJjZzzPANBY",
  "title": "Every AI Founder Should Be Asking These Questions",
  "url": "https://www.youtube.com/watch?v=DJjZzzPANBY",
  "channel": "Y Combinator",
  "channel_category": "strategy",
  "duration": 2436,
  "upload_date": "20251007",
  "description": "Jordan Fisher is the co-founder & CEO of Standard AI and now leads an AI alignment research team at Anthropic. In his talk at AI Startup School on June 17th, 2025, he frames the future of startups through questions rather than answers\u2014asking how founders should navigate a world where AGI may be just a few years away.\n\nHe surfaces the big questions startups should be asking in the age of AGI: Should you even start a company right now? What happens when software becomes commoditized? How do you build trust as teams shrink and AI takes on more responsibility?\n\nChapters:\n0:00 \u2013 Why He's More Confused Than Ever\n2:00 \u2013 Should You Even Start a Startup?\n5:00 \u2013 Planning Beyond the Next Model\n8:00 \u2013 Will Software Be Commoditized?\n11:00 \u2013 Building AI-Native Teams\n14:00 \u2013 Trust, Security, and Agents\n18:00 \u2013 Auditing & Guardrails\n21:00 \u2013 Alignment & Economic Pressure\n24:00 \u2013 Data, Defensibility, and Hard Problems\n28:00 \u2013 Neutrality and Control\n32:00 \u2013 Changing the World vs. Just Making Money\n36:00 \u2013 Build Something People Want (and Need)\n39:00 \u2013 Audience Q&A",
  "view_count": 30591,
  "like_count": 657,
  "transcript": "Welcome to this this talk. I'm extremely confused. Uh I think maybe more confused than I've ever been in my entire life. Probably definitely more confused than I've ever been in my entire life. Uh and I want to talk about that because I think I don't know when you're confused. That's the start of something interesting. If you're trained as a scientist, that's like the thing you want to pay most attention to is that moment where you're confused like, \"Oh, this is interesting. I don't know what's about to happen.\" I've been in technology for like my whole life and I feel like I've always had this advantage where it's like, oh, I understand what's happening. I actually kind of think I know what's going to happen over the next five or 10 years and I've used that to my advantage. Like I've planned my career around it. I've founded companies around it. Uh, you know, I've kind of gotten ahead of uh the trend and gotten involved at the right time in a lot of different places. And that's like really been great for me. I no longer feel that way though. Like I can't see five years into the future. I can see like three weeks or less. Uh, and I really just don't know what's going to happen. So, I I want to have a series of questions that hopefully help me figure out what's going on. Uh, maybe it'll help you all figure out what's going on. Uh, maybe not. Uh, not all questions are useful. Uh, but I think asking good questions is extremely important to running a startup, running a research team, running your life, whatever you're doing. I think we all need to like stop sometimes and just ask questions. But I think the moment in time that we're going through right now with AI is like extremely challenging and and fast. And if there was ever a time to stop and ask questions, it's probably right now. Uh all these are my own obviously. I think we're like required to say that. Um my day job is running a alignment research team at Enthropic. Um I've also been through YC. I've done multiple startups in my life. So maybe my perspective on like AI plus startups is useful for all if you're thinking about um startup life or AI life. Okay. Uh that's enough of a preamble. Uh let's jump in. This is sort of like the main question that I want to ask. Everything's changing. How should that impact everything about my life? Honestly, like should you even start a startup like is a big question. But let's assume that you're starting a startup or you're running a startup already. How should it impact your strategy? How should it impact your product? how should it impact how you're building your team? Like these are big open questions. Uh and I think AI is probably going to change how you answer them today and probably differently tomorrow. And there's like this kind of paradox that I've seen throughout my career running startups and talking to startup founders. Everyone always tells you focus is everything. Like that's the advantage that startups have. Focus, focus, focus. Big companies can't focus. That's why you can out compete them and run circles around them. But despite the fact that focus is everything, the other truth of running a startup is you have to focus on everything, right? Like it's your job to think about hiring and fundraising and product and strategy and go to market and just like everything, right? Um and then all of a sudden in the middle of like a product launch, someone on your team quits, someone else threatens to quit and it's just like it's madness and it's happening all the time. This is just an extreme paradox but I think it it kind of uh makes founders suitable to this biggest question that we're all facing as a society which is what's going to happen with AI and what should we be doing because founders are the people that have to just answer every single question always. So I think it's it's a good positioning. There's been sort of like common quote unquote common just like in the last six months um best practices like what do you think about AI for your your product and people say like oh you should like think about what happens over the next six months. think about what the next foundation models are going to be able to do and make sure you're planning your product anticipating what those capabilities are going to do. Don't like plan for the capabilities of today. I think it's like extremely valid advice. You should definitely take it seriously. Uh but I want to like up the ante a little bit and say actually you should be planning two years in advance. Uh because it's extremely likely that we will have AGI in the next few years. Maybe it's not two, maybe it's three, right? But I think you should be planning your company uh and your strategy around this fact, right? And like there's extreme uncertainty, right? So don't like take it too seriously. Don't have like literally a two-year plan. But if you're not thinking a little bit about how this is going to change everything from hiring to marketing to go to market, etc. I don't think you're doing your job as a founder. Okay. So that's really the the the theme here today is I want to like go through a series of questions and take this lens of AGI arriving and also just take the lens of like what's changing near-term in AI over the next 6 months. I think there's been a lot of conversations around like, oh, actually the impact of AI is going to be slower than we think. And the reason is that big companies suck and they have to like they take a lot of time to buy things, right? They they don't realize the trend and then, you know, the enterprise sales cycle is really slow. So all the Fortune 500 companies are just going to take years and years to like digest all the SAS products that you all might want to be building. And I think that's actually extremely nearsighted because what's going to happen I think open question is the buy side the enterprises they're going to get armed with AGI or strong agents over the next couple years as well right and that's not just via the SAS products that they might be building it'll just be natively inside like their teams are going to be using the next versions of LLM to make buying decisions right and to figure out how to accelerate their adoption cycle the force of AI is not just on this like product revolution that the startups are building. It's also on the buy side, right? And I think this is like an interesting weird thing about AI is that it's going to rise, you know, the water rises and all ships rise with it. It's not just the startups. The incumbents benefit from AI, too. And I think we're seeing that in other places, too, where large enterprises sometimes aren't even going out and buying software from from SAS providers anymore. They're like, I can just like throw two people at cloud code and they'll build it and it'll be dedicated to the the capabilities that I need for my organization. So buy side is going to be evolving really quickly and I think it's an open question what what does that mean right I think another angle on this is you know when you have AI powered outbound sales we think about what that is going to do to the marketplace but there's going to be this in this buy side too of like receiving those sale calls from AIS and trying to parse what's going on so the dynamics aren't really clear how it's going to play out this is a kind of related question like is software going to fully commoditize is it even going to make sense to run a SAS provider in like two years or three years Or is it like going to be the case that enterprises really do just build all their software in-house because it's just one, you know, one prompt to cloud code, the nextg version of cloud code, right? And actually you just need in-house product managers and they're going to do everything for you. Like that's one real outcome that could happen to consumer side too. Like maybe consumers eventually are just like not downloading apps anymore. They're just building apps on demand for themselves. They don't even think about it that way. Like they don't even think about them as apps anymore. It's just, yeah, I want my phone to do something for me. I ask it and yeah, it made an app for me. Sure. um what's the point of downloading an app at this point? So that's one outcome, but I think another outcome actually is the opposite. It's like maybe all of this automation on on generating code makes it easier to just raise the quality bar extremely high. And sure, you can make like the equivalent of today's apps very easily just by prompting. But can you make an exceptional app tomorrow uh the equivalent of a great team that's working with AI uh to raise the bar? I don't actually know the answer to this question, but I think it might be different depending on the vertical. Um, and you should be thinking about this for your product. I think it's an interesting question too, like if you can write software on demand, do you even make apps anymore or is it is it really on demand, right? Like why like we're taking cloud code and and things like it to ahead of time figure out what a user needs and then build as much software as we need to support all those use cases. But if you can do code on demand, why not do it on demand? you know, on the fly, here's this user, they're doing something in your app potentially, and the app realizes that the app can't support what they want and on demand, you generate code for that user. I think this is a really interesting pattern if you can make it work. Um, but it raises all sorts of issues around like trust, right? It's one thing to do generative UI where maybe you're changing the shape of the interface for the user, but like real new behaviors would require going down to the database level, right? Or the back end. And if you want your AI to be able to do that on demand, you better trust that that AI can do its job, right? And obviously right now AIS are not trustable enough to make that happen. So I think trust is going to be a big theme over how these different questions play out. Similar question like what should UI look like? You know, I think people talk about generative UI hasn't really happened yet, but I'm like bullish to see when it does happen. Is a ondemand UI the right interface or is there something even more different that we haven't thought about? Um, I think about this specifically in the context of multimodality, weaving together auditory and images and video and text and what's the right input back from the user. You know, as a user, sometimes I want to speak, sometimes I want to use touch interfaces. It's really contextual depending on whether I'm in a crowded area, etc. Right? I think you want to meet the user where they are. Like what's the easiest thing? What's the easiest way for them to inter interface with your product right now? That's the question you should be thinking about. Uh, and I think all this ties into this big question they have, which is like we have all these products today that have great distribution and people realize that we need to insert AI into it. All the major players are doing this, right? Slapping like a chatbot on, slapping on some like agentic behavior, whatever, right? They're like retrofitting their products to try to enable something with AI, right? But that's like trying to like sprinkle pixie dust on something. It's natural to think it's better to build it a new product from the ground up that's AI native. Like that's that's like my startup mentality. like new technology revolution, you're going to need to start from scratch if you want to build this, right? Uh but that might not be true. It actually might be the case that retrofitting existing products with the benefit of distribution wins. And I think again it might not it might not be universal. This might be vertical by vertical. But I think this is a really important question that you should be asking yourself and not like don't just have an opinion like figure out the causal mechanisms that allow you to validate your hypothesis because I think these types of questions will make or break different products. Okay, so that's a lot about like products uh product strategy, but I think every part of a a startup is like extremely important. Uh the most important thing is your culture. It's your team. Will team sizes get even smaller? I think like that's like the default that people are assuming. But I think similar to products where like you can retrofit or you can build from scratch. I think there's going to be this question which is are like AI native teams that were built from the scratch to be AI native. Are they gonna have some advantage over large companies that are like downsizing and finding ways to make themselves more efficient with with AI? Right? Are there like team patterns where you you kind of like just operate differently if you started from an AI native place? And actually that might be different every six, 12 or 18 months, right? Because the capabilities of AI are changing. So like an AI native company today might be different than what an AI native company to tomorrow looks like in 12 months. You might be outdated if you're not like thinking about how to retrofit yourself. Like I said, I think trust is going to be extremely important, right? So, like how does the security model change? We like I mentioned this already in in the case of on demand code where you want your LLM to be able to go all the way back down to the database layer and do something on demand for that customer, for that consumer. But you can't do that if you can't trust the controls that you have in place and if you can't trust the capabilities of the model to do the right thing, right? I think about this also in the context of like assistance like the whole point of an assistant is to be useful to the to the consumer to that user. Uh, but we have all these walled gardens and you're starting to see like maybe you need different agents in different settings, right? But that's not what you want as a user. You want one agent for all of your things, right? So me, I want my personal agent and my professional agent at work to be able to work together, right? But that raises all sorts of concerns like there's stuff that I do on my personal time uh or you do on your personal time that maybe you don't want your employer to know about, right? Uh so how do you make sure that the information is segregated while still allowing those agents to collaborate in the right way? Those are all hard questions. Can you do it right? Can you get to the same level of of usefulness? Um or do you have to compromise somehow on on some of these uh security aspects? A a deeper question about agents is, you know, we think about alignment as can we trust the AI, but the truth is even if you have a perfectly aligned, you know, quote unquote, like intent aligned model, it's going to be used by a corporation or a startup uh to build an agent for that user. And then the question is, can you trust the startup that's building the agent, right? Uh is is this agent that they've de developed for you actually acting on your behalf? If this is like an adbased company and you're using this agent to search for like a, you know, new brand of shoes or whatever, right? Like is it going to be biased? Is it going to be like pumping you towards one direction? That's not what you want as the user, right? But it might be what you want as the corporation. So, and then how do we start even asking these questions, right? Then going back to the previous slide around like personal versus professional agents. I really care if I'm using that agent. I really care that it's operating on my behalf, right? If it's like secretly operating on behalf of my company and it's like optimizing things in my personal life that's better for the company than me, that's like extremely scary. And I think it gets more scary the more capable the models get. So I think it's important to start thinking about trust, right? Like how do we how do we instill trust uh not just in the models but in the agents and in the companies that are building these agents, right? And I I think ultimately what that really comes down to is trust in in you all or trust in the companies that you're building, right? How can a user trust this like company that's building this this product? Especially, you know, already today we have these extremely small teams. Tomorrow the teams might be even smaller. You might have these like semi-automated teams almost. How can you trust that team? You might say, well, I don't know, like we trust companies today. One of the core reasons we trust companies today is because they're composed of a diversity of people. You can trust to some extent that if there's reasonable culture at a company that if the company decides, if the CEO decides to do something bad, that someone in the company is going to raise their hand and say, \"No, I don't support this. I'm going to whistleblow. I'm going to leak this. I'm going to quit. I'm going to take a bunch of people with me. I'm pissed. I hate this.\" Right? And without the people supporting the company, the company doesn't have a product. Right? But in a semi-automated world, that's no longer true. And it could be the fact that a single person could make a decision that changes the entire impact of a product. And there's no single person that might be aware of that except themselves. It gets extremely easy for bad actors to do bad things. And if you've followed along the history of Silicon Valley or the history of humanity, the truth is a huge majority of people are misaligned, right? Especially when money is on the line. Uh, so I think this is a this is a really important question and the truth is like we already think about this. Like large enterprises for example already distrust startups partially for this reason. There's a lot of reasons why large enterprises distrust startups. One of them is they're worried they're going to go out of business tomorrow, right? But it's just a lot easier for a small startup to do the wrong thing compared to a big company, right? That's part of what makes small startups um successful sometimes. So I think this is already like on the top of minds of enterprises, but it'll increasingly be on the top of minds of everyday people. So that kind of leads me to this next question which is well how do we instill trust? Like what does a new set of guardrails need to look like? If you don't have all these human guardrails inside of your company where you've you know worked really hard to build great culture and build it build it a collection of people who care about doing the right thing. If that's not the thing that makes a company ethical anymore, how else can we make sure that there are guard rails? Like what does that look like? A lot of people have been throwing around ideas on this. There's like this notion of um AI powered auditing like what should an audit look like uh in an AGR world there's this huge advantage that AI has over humans uh when it comes to bringing comfort to an audit which is that they can be less biased and they can also have no memory if you ag for an example right if you agree as a company to let an AI audit you uh and you can say hey if you don't find any malfeasants if if after your audit you've you decided that we didn't do anything wrong as we as we claimed maybe have like a public uh mission statement that like the AI is like confirming uh then the AI deletes itself right all of its notes etc get deleted and that's a big advantage over having human auditors where they could take information with them right there's a lot of there's already auditing happening today right for legal reasons for financial reasons you know where you want to be like certified organic or whatever it is right um but you have to let people come into your company and audit you and there's danger there right they might take with them like IP uh or they might find sensitive things and then like you know which were unrelated to the thing you wanted to get audited and all these bad things can happen, right? So there there's like this potential I think for AI to give us a much stronger auditing system. Um that can be part of how we build trust, how we let people build trust in us and the companies that and products that we're building. So that kind of leads me to this next question which is should we be doing that? Like are there some other ways that you plan on having um on building trust with your users? Um or should we be doing this? Should we be making like not just public commitments? Companies make public commitments all the time like, \"Oh, we care about this. We care about open source. We care about uh doing the right thing for the user.\" But are you actually willing to like make that a binding statement? Are you really really willing to like stick your neck out and say, \"Yeah, not only do I say that this is what I believe to be valuable, I'm willing to commit to an ongoing audit from some neutral arbiter, from some neutral AI powered system that will come in and inspect every single thing that happened in my company, every Slack message, everything, and ensure that actually we are making decisions that abide by the mission statement of the company.\" That would be something that might have teeth. Maybe this isn't the right way to do it, but I think things like this are going to be possible soon. They're not possible today and they might be the flavor of things that we need to start building trust once we're in this world where we've lost a lot of trust. Related question around alignment. So I think a lot about alignment and I think there's this question of like what parts of alignment do we have to solve? You know, one reason we're working on alignment is because of the control issue. We want to make sure AI's, you know, stay under the control of of humans. But I think there's this like extremely high pressure question for the next 12 months which is what parts of alignment do we have to solve just to make these models more economically viable, right? Just to make sure that the agents that all the startups are building uh as their horizons get longer and longer uh that we can actually trust those agents are like not going off the rails, right? And if you use cloud code today and it works like five minutes at a time for you, that's like one thing because you're going to review a lot of what Claude does. But if you're going to trust an LLM to work for a day at a time or a week at a time before you intervene, you better have some degree of certainty that it's not going completely off the rails, right? So I think this is like a huge open question. I'm actually really positive and bullish that there is this economic pressure in a good way to make progress on alignment uh because long horizon agents require it. But I think it's an open question how much and what aspects of alignment have to be solved uh for this. Changing topics a little bit. I think this is like a question that we all like maybe think has been answered which is is there a set of data that can give you an advantage and if you know if you go back in time just a handful of years before Frontier models before LLM the assumption not just the assumption that the fact was that custom data mattered right like if you wanted to build a AI startup or if you're an enterprise and you were trying to deploy AI you had this massive advantage if you had a massive data set that was custom to your need and you that was actually the only way to get useful AIs more than a handful of years ago uh was by training models on your custom data set and then very quickly what we saw was LLMs got extremely powerful and extremely general and actually it was just better to use the general LLM than it was to train or even fine-tune on your custom data right what I I think is true is maybe is true this is my open question is there might be industries where that's not the case uh where actually AI is maybe not great uh an example might be like material science can an LLM do really good material science does a company that specializes in material science that has decades of of data can they do better. Uh, and I think potentially yes, right? Like LLMs are great at everything that they've found on the internet, but are they great at all this like tacet knowledge that's locked up in companies that actually hasn't bled out? Uh, I think about like TSMC or ASML, right? Like they make these multibillion dollar bets and they do an incredibly good job of keeping all that tacid knowledge they build inhouse. It doesn't leak out. Frontier LMS do not know how to build a cutting edge semiconductor fab. Um, that's that's an important fact actually. Um, and I think if you're a startup thinking about where there's a defensible position, it's and I'll touch on this again on in a few slides. I think that's like an important question to keep in mind. Okay, different different tact here is you're probably all aware of like capacity issues. Everyone's trying to scale extremely quickly. I think there's demand from consumers, from your consumers potentially, from your startups to scale extremely fast, right? Uh, maybe we want to scale like 100x over the next couple years. That's faster than we can scale GPU production. So, what are we going to do? And I think there's a lot of uh open questions around like does fine-tuning actually matter? I think a lot of people have abandoned fine-tuning and said actually I'm just going to do better context management or is there like a role to play in like better routers between which model small versus large etc. Right? So I think this is like a place where if you're interested in the technical details you can have a competitive advantage at least for the next year or two uh because capacity is really going to matter right you know often from a product perspective I like to say make it great then make it scale. uh if you're already starting to work on the make it scale part, this is really important to you and like this can be like some of the technical um moat that you can build that that gets you ahead of the competitors. Getting ahead of the competitors though is like it's a rat race. So at some point the models will get better. Uh the capacity problems will improve and then that advantage might go away. So you need to have a better answer to your question to this question of like what's your mode? How are you going to stay ahead? What makes a durable advantage in a postagi world, right? In two years or three years, if I can just prompt, you know, Claude 7 or GPT7 to just replicate your startup, what's your advantage going to be, right? And if I'm a mega corp and I have more money than you and I can throw more tokens at that, like are you really going to have a durable advantage or you just going to get clobbered by mega corp? I think it's like a real question uh that is extremely serious. Even before AGI, I like to work on hard problems. Like that's the moat for me. Everyone has a different type of mode. Some people are great at marketing, whatever, right? I like solving hard problems. Uh but I think about what does it mean like to be a hard problem. Like what's going to be hard in a post AGI world and I think a lot of things actually like TSMC, like ASML, those are hard problems that eventually will get easy uh with robotics etc. But robotics are lagging behind. So I think there's these hard problems that will exist even in two years. And if you're willing to go after hard problems, like if you have the guts to do that, then you can have a massive competitive advantage, right? So like what are what is that set of hard problems? You know, infrastructure and energy and manufacturing and chips. What else? Like those are the things that are top of mind for me, but like what are you what are your answers to this question? Like what's still going to be hard and worth doing? I think about this question a lot too. Like is there an intelligence ceiling to what you need for various different tasks, right? Like, you know, if you look at image gen, for example, you know, just recently with Veo3, uh, with videogen, it's like I can I finally am getting fooled by videos and like my Instagram feed is starting to get full up of like copy bars singing songs and [ __ ] and and I'm like, actually, this is great. I love these videos. Uh, that wasn't true three months ago, right? But can it get even better or is there like for a specific task, for a specific use case, is there like some max where it's like, yeah, that's it's good enough. We've saturated, right? like that's the best you can get at writing a poem. That's the best you can get at writing a a git diff uh for a PR. That's you know an important question because if there is a ceiling then the commoditization for that task is going to hit much sooner right you actually can't stay on the edge longer by moving to the next model figuring out how to prompt the next model the task saturates right so the commoditization pressure will be even more extreme so I think it's important to to think about this and again I think it's going to depend on the task and the vertical like some things maybe there won't be a ceiling some things maybe there will this is changing tack a little bit here but you know is there going to be a need for neutrality I you know since the the dawn of chat bots two two or three years ago or whatever like people have been complaining about refusals as like an example oh like I asked the model to do this and it refused. Uh if we end up relying on these models that's a massive question for society right that there's going to be a handful of of corporations that get to decide what is okay and not okay for an AI to do for you. And if we start relying on AI to do everything then those companies become arbiters of what gets built right that's extremely important. So is is there is there going to need to be a notion of neutrality like we have today with some forms of infrastructure, right? Electrical infrastructure is is neutral. Like if GE owned all the electrical grid and they were like you can only use our electrical grid if you promised to plug in our toaster oven to it. Like no other toaster ovens allowed to work on our electrical grid. That would be a problem. Um and it's it's good that that didn't happen. Uh obviously we like fought and lost this battle for the for the web. Um what's going to happen with AI? Do we need AI neutrality? Token neutrality? I don't know what we call it. I'll like wrap up my set of questions here. I have an infinite number of questions and you know I hope that these just like stimulated you to think a little bit as well about not just these questions but your own questions right I want to just like touch on one one thing that's like always inspired me about Silicon Valley and it's this like cringe thing that has always like you you only see it in ironic sense but actually I think it's really great which is like in Silicon Valley we always used to say like what's your startup doing and someone's like oh I'm trying to change the world. Everyone always wanted to like think big and do the big thing and like at the end of the day was a cat sharing app or whatever, right? But like the desire to change the world was there. I think startup founders really meant it when they said I want to change the world. I think the thing that's concerning to me is you know I I' I've been on the bleeding edge of AI for like ever and I've you know been worried about AI risk forever and you know five years ago when I talked to someone about AI they'd be like what the [ __ ] are you talking about? You think these things are going to maybe kill us or whatever like this is insane. Uh now when I talk to people, you know, I sit them down and I explain everything that's happening. They already are aware obviously, but I'm like, \"Let me give you some more details.\" And suddenly they're like, \"Oh, you can see the gears start turning.\" And just regular folks that I talk to uh in my life, they get it. And they they're like viscerally aware that the things that we're about to go through are extremely important. They're like humanity defining, society defining. Like people get this, right? It's not it's not a nuanced thing, right? We're for the first time ever bringing a second intelligence in the world that matches humanity and will eventually uh exceed uh human intelligence, right? Like that's just obviously extremely important, right? Uh but like I get a lot of people to the end of that chain of thought, you can see the gears turning and then they they ask a question and the question is okay, how do we make money off of this? And I get extremely disappointed in that person. Uh and I totally understand why they're asking this question, right? Like you know there's a lot of fear in the world. I think the fear of like all of us losing our jobs or not being able to build a startup that's going to be competitive anymore or maybe the door to build a startup closes forever in the next few years. Who knows, right? Like there's a lot of fear. Uh and I think AI is scary, right? So like people start thinking, well, I need to make money right now. like if the economy is going to be fundamentally different in the next few years and I can't guarantee my place in it, like I better like make my mark right now, I better make my money right now. And I think that's totally understandable. So like actually I I'm like, yeah, okay, like here, let's brainstorm some ideas for how your next company can make money or if you're, you know, an investor or you're an employee, whatever it is, let's brainstorm your career. So I'm always happy to help people with that. But I I also think that in that same moment we this is the last opportunity that we might have to make a difference to change the world. If you're building a product, you need to use this moment, right? Like this might be the last product you build. This might be the last company you build. If you're inside of a company, the same applies. That might this might be the last chance that you have over the next couple years to make that impact that could change the world even in a small way. So if you have something you care about, now is the time to do it. I think about YC a lot and you know the slogan is first part is YC slogan build something people want right that's what we all want to do and I think the truth is what people want often is something that's good for society but you do need to think a little bit deeper like I think people do want things they can trust they want agents they can trust they want bots they can trust they want they want to know that when they use a product it's not just going to like delight them for the next 20 seconds that it's going to be good for their mental health for the the next 20 years or the mental health of their their children or their neighbors, right? Like we want good things. We as consumers and users, we want good things, right? And when we say build something people want, don't just think about what people will consume. Like what does society need? I think if you build the right thing, a lot of people will want it. Uh so I'll wrap up and I'll say like I know there's a lot like I get stressed out thinking about this sometimes. Uh but I I really think as founders or as people who are interested in being founders like we have this unique perspective like we think about things we in a way that most other people don't right like it's kind of the whole job of being a founder is finding an edge that's the whole thing and I think over the next couple years things are just going to move extremely fast and the rules are going to change every six months and you're going to have to think again right and if there's anyone that's positioned to stay at the bleeding edge of that understand how those changes are impacting some of these questions and other questions that we're talking about and then use the insights from the answers to those questions to drive positive change. If there's anyone that can do that, it's it's you all here. It's it's folks that care about thinking about these these things and care about building things that people want. Uh so I hope that that's what you all do. Uh I hope you make money doing it, too. Uh while you can. Uh and yeah, thanks for thanks for listening. I think we got time for some questions. Thank you so much for the talk. I think that was probably one of the more grounded and down to earth talks I've heard, especially the last point about build something people want. I've always thought it's build something the world needs is perhaps a bit more important for a time like this. >> Yeah. And so I guess my question for you coming up with all those questions, how to approach AGI and like the insights that come along with it, what are a couple sources of information that like inspire you to do this like whether that's people, podcast, books, and things like that? What what have been most helpful for building your mental model? >> Yeah, that's a great question. I I uh hesitate to say it, but I think the honest answer is Twitter. I just I'm really religious about curating my Twitter, though. like, you know, if I see someone who I think has good takes, I I follow them. If they have dumb takes, I unfollow them. Um, I think you really need to be like the master of your information diet. Not because like I'm not worried about like being influenced by bad opinions. It's more that I've just got a limited limited energy budget to digest new ideas, right? So like don't just maximize for um people you agree with. Maximize for diver diversity, right? I'm like in reinforcement learning context all the time and there's this like notion of like diversity in RL like exploration versus exploitation. You want to make sure you're doing a lot of exploration in your energ in your like information diet uh before you're doing the exploitation which is like starting a company for example. >> Yeah. Thanks for the amazing talk guys. It's probably my favorite talk today. Uh something I think about quite a bit is like in a world where AGI is coming say in two or three years. Traditionally, like the questions I ask myself is if I have a couple startup ideas, should I work on something that I'm really passionate about and have experience with expertise in? Should I work on some place an idea where the market's underserved or not as competitive in a place where AGI is coming? Should the most singular important question be what idea is the most offensible against AGI? >> Yeah, it's I mean it's a great set of questions. I think even before all this AGI stuff, a lot of those were good questions. Like personally, my opinion is that once you're like 6 months into 100 hour work weeks, I don't care how passionate you are about an idea, you're going to [ __ ] hate it. And the only thing that's going to keep you going is like your desire to have the impact, your commitment to the company, your commitment to your founder, your co-founders, and your team, right? Like, so does it really matter to be extremely passionate about the the domain that you're going after? In my opinion, no. But I think other people feel differently. So like that's a personal question. But I do think like being impactoriented is really important. and whether or not you're trying to have an impact or not. I think defensibility really is in my opinion one of the key questions, right? I mean, that's kind of half the talk, I guess, right? It's like what's going to change? And, you know, is the thing you're building just going to be a rounding error over the next six months? And, you know, I I honestly think there's a lot of money to be made in the ne in on a six to 18 month horizon. If all you're optimizing for is like, you know, hockey stick curve, grow your ARR, flip the company, uh, and make a quick buck, like, uh, you might you might not need something long-term defensible, right? But if you want to build something that's going to like stand the test of time and be part of this transition through the singularity and all the craziness of that, like I would say think harder about the defensibility. That's probably the most important thing. >> Thank you so much. >> Yeah, good question. >> Some of those questions are really mind-bending. Been thinking about them a lot as well. My question is sort of I know you've tried to keep it open, but what's like your s your personal opinion on like the value of money? Do you think as the cost of goods and services go down, money will become like less valuable because you know everything's free or it will become more valuable because we can do like way more stuff? >> Yeah, it's a great question. Yeah, I mean I think there's going to be policy decisions that impact this a lot, right? like do we need some form of UBI? um or like you know maybe like slightly more weird like universal basic compute like if comput is the thing that powers everything are we all entitled to some form of compute these are like open policy questions that will start impacting that answer and I think we should think seriously about them because they will also change the nature of our society and the sort of like um checks and balances that we have over our democratic institutions right like if the government's giving out a UBI to everyone that's an extreme amount of power suddenly the government has over the entirety of of of society right but on the flip side if don't do something like that. Uh I do think that we might end up in a world where, you know, there's there's two primary forces at work today. It's capital and labor, right? You know, I won't say the names of people that I wouldn't want to work with, but there are people in this world that I wouldn't want to work with. And a lot of people choose not to work for those people, right? That's like a massive uh hit to those people, right? But once AGI arrives, you don't need labor buyin anymore, right? Like you don't need folks like me to approve of the thing that you're building or of your morals or whatever, right? like capital begets capital in that world. Uh and that can easily spiral out of control, right? We already have a lot of concentration of wealth. Um that could really spiral out of control. So I think we're like in a between a rock and a hard space and I don't know what the answer is and but I think the policy decisions we make will have an huge impact on the question that you asked. >> Yeah, thank you. >> Hey Jordan, thanks so much for the time. Yeah. >> Um I guess the question I wanted to ask is how do you think about alignment at individual uh at the level of individual users and I guess how important is that for trust especially given that you know preferences evolve over time and you don't want to keep retraining models especially also when you have a lot of users. Yeah, I mean I think like everything I have like a startup product lens on everything, right? And it's like startup mantra is that users don't know what they want, right? And I think that's true to a large extent, right? But I think users still have values and you want to discover those values and be and honor them to some extent, right? So I think like something that's top of mind for me is you know you probably saw like the sickopentic behavior from the recent you know other other AI provider and like I think if you put in front of a user two responses and one of them is more sick aentic one's like glazing I guess is what the kids say nowadays right uh then a lot of users will pick the sickopentic response right like in that moment they're like yeah of course like of course the question I'm asking is a great [ __ ] question like thanks for recognizing it right but I think if you take a step back and you say, \"Hey, hold on a second. Here's two principles.\" And you can choose the AI that follows this principle or this principle. And the first principle is that we're never going to blow smoke up your ass. Uh we're only going to tell you if we like something or if an idea is good, if it really is. Uh and the other principle is actually we're just going to like, you know, we're just going to glaze you all day. Like that's what we're going to do. If you ask the user which principle do they want, almost everyone's going to say the first one, right? So, I I I think doing what the user wants, you you get to a different answer depending on what level of the engagement you're asking them. And I think that's really important because people can abuse that to their advantage by only asking the user the question in certain ways, right? And I think you need to really ask yourself what what's the right way to ask the user this question to get at the heart of what is going to be best for them. I don't know if that gets you. I kind of went off on a diet tribe, but >> that makes sense. Thanks so much. >> Cool. Great question. Yeah. >> Thank you for the talk. I really enjoyed it. It seems to me like you value critical thinking a lot. So I was wondering what topics or what like what opinions does like the general crowd and like the tech field have that you disagree with? >> I I guess my my high level general statement that I'll I'll tell you and I'm happy to like talk talk more offline. I think despite the fact that we say our industry is this like forward-looking industry uh and we're like bold and we love taking risks and putting everything on the line and seeing what other people don't see. I think the truth truth is that like there's an extreme amount of group think, right? Uh like the products that we build, what gets funded by VCs, etc., right? Uh and I think even today like you talk to a bunch of VCs and they're like, \"Oh yeah, like I'm ahead of the curve. Like I'm investing in AI.\" I'm like, \"No, like you're two years behind already, right? like tell me about what you think needs to happen in two years, right? Like are you asking these types of questions of like what do I need to be investing in today so that it's resilient in two years? And I almost never see a VC asking that that question, right? But if I was a VC, which I'm not, that's that's that would be my investment thesis as a as a founder, that's my investment thesis. Like I don't know if that answers your your question, but happy to go to >> Thank you. >> Yeah. Cool. Great question. Yeah. >> You mentioned how trust is going to be a bigger issue in the future. So, do you think that blockchain might be a part of the solution there? Thank you. >> I uh I will just preface by saying that I I I'm a huge blockchain doubter. I don't know. But nonetheless, the price keeps going up and I've you know, I don't get any of that because I refuse to buy it. Uh but I I do think, you know, in this world where we need trust, yeah, like those that's the right set of ideas. We need ideas like that in others, right? like you know something that is I was touching on is like you know AI powered audits can like two different AI companies audit each other like like how do how do we like get to places where we can build trust um if we do end up with some form of like universal basic income or basic compute basic tokens does that need to be mediated by a blockchain like so that it's not just at the behest of a central government like yeah I think those are like reasonable questions that maybe I could get behind a blockchain on good question yeah >> thank you >> so Google recently released um an ha protocol to standardize how agents would talk to agents and we talked a little bit about um trust um on agents. >> Yeah. >> So I'm curious how like you would envision a world of agents talking to agents and how would that would affect applications and would there be like a a gentic permit scheme? >> Yeah, I it's a great question. Um, I I could talk about this all all day and I'm almost out of time, so maybe I'll just like I'll I'll give you one example where it's like not obvious why this was hard, but like think about like a personal assistant agent that's just scheduling meetings for you, right? Seems trivial, right? Like, oh yeah, I'm just going to look at this person's calendar and then I'm going to like suggest times, right? But actually, like you've already lost the game of being a good personal assistant because there's a game theory component to how you schedule meetings, right? And if you're like too liberal with like showing that there's free slots, you're like communicating that this person's not busy, right? Uh or you're communicating that like it's really important to have this meeting with the person that you're scheduling. Uh whereas like if you say, \"Yeah, like I'm happy to schedule a meeting between you and and Joe, but it's like two weeks out, three weeks out, right? Like there's a power dynamic that you're you're doing, right?\" And like the truth is like all these game theoretic things matter. A good assist, a good human assistant knows all these things, right? And like abides by them. Um but it's all implicit, right? It's not like, oh, there's this like concrete piece of information that the agent has access to and that's the important part of the security. It's like way more subtle and semantic. So, I think it's just hard, but I think it's a great question. Yeah. >> Cool. And I think uh out of time unfortunately, but feel free to like shoot me a message online or I don't know how to get like my Twitter is Jordan Fisher, Jordan Ezra Fischer. Shoot me a message. Always happy to chat. And thanks for the great questions and and hope you all enjoy the talk.",
  "topics": [
    "startup strategy",
    "AI implementation"
  ],
  "collected_at": "2025-10-17T21:10:50.326349",
  "source": "youtube",
  "source_type": "video"
}