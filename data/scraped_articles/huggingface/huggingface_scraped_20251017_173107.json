{
  "source_id": "huggingface",
  "source_name": "Hugging Face Blog",
  "scrape_method": "requests",
  "collected_at": "2025-10-17T17:31:07.789153",
  "article_count": 40,
  "articles": [
    {
      "content_id": "cafd600f4b93c258f8d8453e60fc2499",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "Ring-flash-linear-2.0: A Highly Efficient Hybrid Architecture for Test-Time Scaling",
      "url": "https://huggingface.co/blog/RichardBian/ring-flash-linear-2-moe-release",
      "author": "Unknown",
      "published_at": "2025-10-17T17:30:31.476825",
      "fetched_at": "2025-10-17T17:30:31.476896",
      "content_type": "article",
      "content_text": "Back to Articles Ring-flash-linear-2.0: A Highly Efficient Hybrid Architecture for Test-Time Scaling Community Article Published\n\t\t\t\tOctober 9, 2025 Upvote 9 +3 longfei li long0x0 Follow Yang XinxingYang Follow Yue Zhang York-Z Follow xuan curryandsun Follow MENG LI lemonpiece Follow C.Z. Tang caizhi1 Follow Chen Liang erichen Follow zibin lin SmiIemaker Follow Richard Bian RichardBian Follow Test-Time Scaling has emerged as a crucial trend for raising the capability ceiling of large models. However, the computational and memory overhead associated with ultra-long context reasoning grows exponentially, particularly in attention computation and IO consumption . To break this bottleneck, we have built the ultra-efficient Ling 2.0 Linear Hybrid Architecture by integrating a high-sparsity MoE structure with hybrid linear attention, based on the Ling 2.0 architecture . Today, we are officially open-sourcing two highly efficient reasoning models: Ring-flash-linear-2.0 and Ring-mini-linear-2.0 . We are also simultaneously releasing two self-developed high-performance fused operators: the FP8 Fused Operator and the Linear Attention Inference Fused Operator . Thanks to the synergy between architectural optimization and high-performance operators, the inference cost of these two models in deep reasoning scenarios is only 1/10 that of similarly-sized Dense models, and a reduction of over 50% compared to the original Ring series. The strong alignment of operators between the training and inference engines enables long-cycle, stable, and highly efficient optimization during the reinforcement learning phase, allowing the models to maintain SOTA performance on multiple high-difficulty complex reasoning benchmarks. Ring-flash-linear-2.0 Performance on High-Difficulty Reasoning Benchmarks Ring-mini-linear-2.0 Performance on High-Difficulty Reasoning Benchmarks The following shows the generation speed comparison between the Ring-mini-linear-2.0 (which uses the hybrid linear architecture) and Ring-mini-2.0 (which uses the standard attention mechanism). Demo Video https://vimeo.com/1125491539 for the comparison Testing conditions: Single H20, batch size = 256, generation length around 16k tokens. Shows the streamed output of the last request (an identical math problem). Ring-mini-linear-2.0 reduces end-to-end generation time by 40% compared to Ring-mini-2.0. Ling 2.0 Linear: A More Efficient Hybrid Linear Architecture Ling 2.0 Linear Architecture Diagram The Ling 2.0 Linear hybrid linear architecture is specifically designed for two future trends in Large Language Models: Context Length Scaling and Test-Time Scaling. The Ling 2.0 Linear architecture uses a hybrid structure of linear attention and standard attention, overcoming the weakness of poor recall capability found in purely linear attention mechanisms. By increasing the proportion of linear attention, the model achieves near-linear computational complexity, which significantly reduces the training and inference compute cost in high-concurrency and long-context scenarios. We also conducted systematic experiments to introduce several improvements to the linear attention layers, such as: Adding Rotary Positional Embedding (RoPE) to the q and k inputs of the linear attention. Adopting a grouped and non-shared RMSNorm for the output of the linear attention. Experiments show that these seemingly subtle changes result in higher training stability and better extrapolation capability. The Ling 2.0 Linear architecture inherits the efficient MoE design of the Ling 2.0 architecture, achieving an architectural performance leverage of more than sevenfold through optimizations like the 1/32 expert activation ratio and the MTP layer. This means Ring-flash-linear-2.0, with only 6.1B active parameters, can rival dense model architectures below 40B in performance. High-Performance Fused Operators In recent years, FP8 mixed-precision training has garnered widespread attention. However, during model training, we found that most existing FP8 training solutions primarily focus on saving VRAM, with no significant improvement in actual computational efficiency. To address this, we developed a more efficient FP8 Fused Operator through fine-grained operator fusion and adaptive re-quantization techniques. This substantially enhances the computational efficiency of FP8 mixed-precision training, achieving speedups of 1.57x and 1.77x on the two models, respectively. Left -> Right: baseline, mixed-precision, acceleration ratio On the inference side, although linear attention is inherently more computationally efficient, existing inference solutions often lack support for highly efficient frameworks like SGLang and vLLM v1. Furthermore, the decode phase is often broken down into multiple kernels, further degrading inference efficiency. Therefore, we have adapted our released models for frameworks such as SGLang and vLLM v1. We also developed a more efficient Linear Attention Fused Operator (see PR: https://github.com/sgl-project/sglang/pull/10917 ), supporting more inference modes and further boosting the throughput of the inference engine. Thanks to the Ling 2.0 Linear architecture and the high-performance fused operators mentioned above, Ring-mini-linear-2.0 and Ring-flash-linear-2.0 achieve near-linear time complexity and constant space complexity, maximizing inference efficiency! In the prefill and decode stages, the overall advantage compared to the previous GQA rapidly expands as input and output lengths increase, with the cost for ultra-long output being only 1/10 that of dense models! Ring-flash-linear-2.0 Prefill Throughput (batch size = 1) Ring-flash-linear-2.0 Decode Throughput (batch size = 64) Testing conditions: 4 H20 GPUs, TP4, SGLang framework version v0.5.2, MTP disabled, tested using SGLang\u2019s built-in sglang.bench_offline_throughput script. Ring-mini-linear-2.0 Prefill Throughput (batch size = 1) Ring-mini-linear-2.0 Decode Throughput (batch size = 64) Testing conditions: 1 H20 GPU, TP1, SGLang framework version v0.5.2, MTP disabled, tested using SGLang\u2019s built-in sglang.bench_offline_throughput script. More Stable RL Training Unlike pre-training and supervised fine-tuning, Reinforcement Learning for LLMs requires reliance on both training and inference. However, even standard components like RMSNorm and RoPE often have slight implementation differences across common training frameworks (e.g., Megatron, FSDP) and inference frameworks (e.g., vLLM, SGLang). These differences accumulate and amplify layer-by-layer, leading to discrepancies between the training and rollout outputs. This problem is further exacerbated by the dynamic routing of experts in MoE models. The training-inference (Train-Inference, or TI) discrepancy means the theoretical on-policy assumption does not hold in practice, resulting in RL instability and lower performance ceilings. While this issue has recently received attention in some research, most proposed solutions attempt to mitigate it at the algorithmic level. We, however, have not only improved RL stability algorithmically but have also focused on fundamentally solving the TI discrepancy by addressing the training and inference framework level. To ensure TI consistency, we align the training and inference frameworks across three dimensions: identical logic implementation, maintaining appropriate precision, and eliminating non-determinism. RL Long-Run Curve After Fixing TI Discrepancy Issues in Different Modules As shown above, aligning each component leads to a more stable final training curve. Furthermore, we found that once TI alignment is achieved, no extra algorithmic modification is needed; the optimization can directly use rollout probabilities. The figure below demonstrates that this approach effectively enhances both RL training efficiency and stability. Comparison of PPO Clip using Rollout Probs vs. Training Probs After TI Alignment (Up) Reward (Down) Percentage of tokens where the absolute Train-Inference probability difference is greater than 0.8 Use Case Demos Sudoku Game Write a web application for a Sudoku game. Demo Video https://vimeo.com/1125491509 Tank Battle Use Python to create a simplified tank battle game. Users use the up, down, left, and right keys on the keyboard to control the free movement of a tank. The spacebar fires bullets to defeat enemy tanks in the game scene. The scene contains five freely moving enemy tanks, which fire bullets in the direction of the current tank\u2019s movement. Each time an enemy tank is defeated, one point is awarded, and a new enemy tank is randomly generated. The game ends when the user\u2019s tank is hit by an enemy tank. Demo Video https://vimeo.com/1125491475 Stock Trading Software Please generate a page for a simulated stock trading software. The data can be randomly generated, and the page should include five sections: 1. Intraday second-level data, which needs to update every second and be displayed as a line chart. 2. Daily K-line chart, which can display 60 days of OHLC data using a candlestick chart (red for up, green for down). 3. Real-time trading volume, also updating every second, displayed as a number. 4. Daily trading volume data, displayed as a bar chart. 5. Company introduction, which can be randomly generated. Key requirements: 1. Please use Canvas to draw all curves and candlestick charts, ensuring image clarity and preparation for high-resolution devices. 2. The Canvas window should be able to automatically adjust its size based on the browser window size. 3. Use native JavaScript and HTML5 attributes without external libraries. 4. Please ensure all randomly generated price data is usable. Demo Video https://vimeo.com/1125491443 Where to Find Us We welcome everyone to visit our open-source repositories for download and use. Kindly provide your invaluable feedback through discussions or issues or PRs! Ring-flash-linear-2.0 \ud83e\udd17 Hugging Face https://huggingface.co/inclusionAI/Ring-flash-linear-2.0 \ud83e\udd16 ",
      "content_length": 1404,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.74,
      "scrape_method": "requests"
    },
    {
      "content_id": "273474049df5176a93239c0702e34119",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "How to Train an Antibody Developability Model",
      "url": "https://huggingface.co/blog/ginkgo-datapoints/making-antibody-embeddings-and-predictions",
      "author": "Unknown",
      "published_at": "2025-10-17T17:30:32.375700",
      "fetched_at": "2025-10-17T17:30:32.375772",
      "content_type": "article",
      "content_text": "Back to Articles How to Train an Antibody Developability Model Community Article Published\n\t\t\t\tSeptember 17, 2025 Upvote 20 +14 Georgia Channing cgeorgiaw Follow ginkgo-datapoints Lood van Niekerk loodvanniekerkginkgo Follow ginkgo-datapoints (using foundation model embeddings) Introduction Antibody development is the process of engineering therapeutic antibodies with desirable biophysical properties such as high expression, stability, and low aggregation propensity. Unlike simple point mutations in proteins, antibody optimization is especially complex: the pairing of VH (variable heavy) and VL (variable light) domains creates a vast combinatorial space where even small changes can dramatically alter folding, binding, or developability. Predicting these effects experimentally is costly and time-consuming, which is why machine learning on antibody sequence data has become such an important tool. By learning representations of antibody sequences, protein language models can capture sequence\u2013structure\u2013function relationships that help guide design choices. This Dataset The GDPa1 dataset provides paired VH/VL antibody sequences with experimentally measured developability assays , including expression yield, hydrophobicity, stability, and self-interaction. This makes it a valuable benchmark for testing whether protein language models like p-IgGen can generate embeddings that generalize across assays and predict developability outcomes. In the sections below, we demonstrate how to: Embed VH/VL pairs with p-IgGen, Train simple regression models on different assay targets, and Evaluate generalization with cluster- and isotype-aware cross-validation . Quick Primer Antibody Sequences: VH and VL Antibodies are Y-shaped proteins built from two chains: VH (Variable Heavy) : the heavy-chain variable region that contributes to antigen binding. VL (Variable Light) : the light-chain variable region that pairs with VH to form the complete binding site. Together, VH and VL encode the amino acid sequences that define an antibody\u2019s specificity and biochemical properties. In this dataset, each row includes both VH and VL sequences for a given antibody. Image from https://www.antibody-creativebiolabs.com/antibody-structure-isotypes.htm Available Properties Each antibody has associated experimental measurements across 5 assays (not all measurements are available for every sequence): Titer : Expression yield of the antibody in mammalian cells. HIC ( Hydrophobic Interaction Chromatography ): A proxy for hydrophobicity and aggregation propensity. PR_CHO : Polyreactivity in CHO ( Chinese hamster ovary ) cells - this measures how much an antibody binds to other proteins that it's not supposed to. Tm2 : Thermal stability (melting temperature of the CH2 domain). AC-SINS_pH7.4 : Self-interaction propensity (higher values often correlate with poor developability). A Note on Isotype Effects The antibodies in this GDPa1 dataset have different IgG subclasses (IgG1, IgG2, IgG4). Some of the measurements, such as thermal stability (Tm2), are strongly influenced by the antibody\u2019s isotype . Different IgG subclasses (e.g., IgG1, IgG2, IgG4) have systematic differences in CH2 domain stability, which can overshadow subtle sequence-level variation---so consider including the subclass as a feature in your models! To illustrate this, we can make a simple boxplot of Tm2 values grouped by isotype. plt.figure(figsize=( 6 , 4 ))\nsns.boxplot(data=df, x= \"hc_subtype\" , y= \"Tm2\" )\nplt.title( \"Distribution of Tm2 scores by antibody isotype\" )\nplt.xlabel( \"Isotype\" )\nplt.ylabel( \"Tm2 (\u00b0C)\" )\nplt.show() More information about the assays is available in the PROPHET-Ab preprint . In this tutorial, we select one target at a time (e.g., HIC ) and train a simple linear model on pooled sequence embeddings to assess predictivity. We start by importing the core libraries for data handling, visualization, statistics, and modeling. Using Hugging Face Datasets, we load the GDPa1 dataset into a Pandas DataFrame, inspect the available assay columns, and check for missing values. For this tutorial, we select a single developability target (here \"HIC\" , though others like \"Titer\" could also be chosen) and drop rows with missing measurements so that the dataset is ready for model training and evaluation. from datasets import load_dataset import matplotlib.pyplot as plt import numpy as np from scipy.stats import spearmanr import seaborn as sns from sklearn.linear_model import Ridge from sklearn.model_selection import train_test_split import torch from tqdm.auto import tqdm from transformers import AutoModelForCausalLM, AutoTokenizer model_name = \"ollieturnbull/p-IgGen\" df = load_dataset( \"ginkgo-datapoints/GDPa1\" )[ \"train\" ].to_pandas() # Show number of NaNs per assay print (df[[ \"Titer\" , \"HIC\" , \"PR_CHO\" , \"Tm2\" , 'AC-SINS_pH7.4' ]].isna(). sum ())\ntarget = \"HIC\" # Example: Just predict HIC, so we'll drop NaN rows for that df = df.dropna(subset=[target]) # output\nTiter 7 \nHIC 4 \nPR_CHO 49 \nTm2 53 \nAC-SINS_pH7.4 4 The VH and VL protein sequences for each antibody are combined into a single tokenized string input for p-IgGen . We prepend a \"1\" token at the start and append a \"2\" token at the end to mark sequence boundaries. # Tokenize the sequences tokenizer = AutoTokenizer.from_pretrained(model_name) # Paired sequence handling: Concatenate heavy and light chains and add beginning (\"1\") and end (\"2\") tokens # (e.g. [\"EVQLV...\", \"DIQMT...\"] -> \"1E V Q L V ... D I Q M T ... 2\") sequences = [ \"1\" + \" \" .join(heavy) + \" \" .join(light) + \"2\" for heavy, light in zip (\n        df[ \"vh_protein_sequence\" ],\n        df[ \"vl_protein_sequence\" ],\n    )\n] print (sequences[ 0 ]) # output\n1Q V K L Q E S G A E L A R P G A S V K L S C K A S G Y T F T N Y W M Q W V K Q R P G Q G L D W I G A I Y P G D G N T R Y T H K F K G K A T L T A D K S S S T A Y M Q L S S L A S E D S G V Y Y C A R G E G N Y A W F A Y W G Q G T T V T V S SD I E L T Q S P A S L S A S V G E T V T I T C Q A S E N I Y S Y L A W H Q Q K Q G K S P Q L L V Y N A K T L A G G V S S R F S G S G S G T H F S L K I K S L Q P E D F G I Y Y C Q H H Y G I L P T F G G G T K L E I K2 We load the p-IgGen model and pass in batches of tokenized VH/VL sequences to obtain hidden-state embeddings. For each sequence, we compute the mean-pooled representation across tokens, resulting in a fixed-length vector that captures sequence-level information about the antibody. # Load model device = torch.device( \"cuda\" if torch.cuda.is_available() else \"cpu\" )\nmodel = AutoModelForCausalLM.from_pretrained(model_name).to(device) # Takes about 60 seconds for 242 sequences on my CPU, and 1.1s on GPU batch_size = 16 mean_pooled_embeddings = [] for i in tqdm( range ( 0 , len (sequences), batch_size)):\n    batch = tokenizer(sequences[i:i+batch_size], return_tensors= \"pt\" , padding= True , truncation= True )\n    outputs = model(batch[ \"input_ids\" ].to(device), return_rep_layers=[- 1 ], output_hidden_states= True )\n    embeddings = outputs[ \"hidden_states\" ][- 1 ].detach().cpu().numpy()\n    mean_pooled_embeddings.append(embeddings.mean(axis= 1 ))\nmean_pooled_embeddings = np.concatenate(mean_pooled_embeddings) With embeddings as features ( X ) and the chosen assay measurement as the target ( y ), we split the data into training and test sets. A ridge regression model is fit on the training set, providing a straightforward baseline to evaluate how informative p-IgGen embeddings are for predicting antibody developability properties. # Train a linear regression on these X = mean_pooled_embeddings\ny = df[target].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2 , random_state= 42 )\n\nlm = Ridge()\nlm.fit(X_train, y_train)\n\ny_pred = lm.predict(X_test) We evaluate the model using Spearman\u2019s rank correlation between predicted and true assay values, which is more robust than linear correlation when the relationship is non-linear. Finally, we visualize predictions against ground truth in a scatter plot to assess how well the model generalizes. We get approximately 0.41 Spearman correlation using these embeddings with linear regression to predict HIC . # Calculate score print (spearmanr(y_pred, y_test))\n\nsns.scatterplot(x=y_test, y=y_pred)\nplt.title( f\"Scatter plot of predicted vs. true {target} \\nSpearman's rho: {spearmanr(y_pred, y_test)[ 0 ]: .2 f} \" )\nplt.xlabel( f\"True {target} \" )\nplt.ylabel( f\"Predicted {target} \" )\nplt.show() # output\nSignificanceResult(statistic=np.float64(0.41341124897148246), pvalue=np.float64(0.0031521527808128337)) Cross-Validation with Isotype-Stratified Folds In our previous experiment, we allowed the training and test sets to be randomly generated. The problem with this is that there may be near-identical antibodies split between training and test, essentially creating label leakage. To understand how our model behaves when presented with totally new antibodies, we want to evaluate our regressor with cluster + isotype-aware\ncross-validation . Each fold holds out entire sequence clusters (but still containing a mix of different IgG subclasses) , so the model is tested on novel families rather than near-duplicates. Below, we reuse the previously computed embeddings and our target y , but we use the pre-computed fold assignment in df['hierarchical_cluster_IgG_isotype_stratified_fold'] to define train/test splits. Now, we train fresh Ridge models on each training split and evaluate with Spearman correlation on the held-out fold. fold_col = \"hierarchical_cluster_IgG_isotype_stratified_fold\" X = mean_pooled_embeddings\ny = df[target].to_numpy(dtype= float ) # sanity check assert len (X) == len (df) == len (y)\n\nfold_values = df[fold_col].to_numpy()\nunique_folds = [f for f in np.unique(fold_values) if f == f] # drop NaN per_fold_stats = []\ny_pred_all = np.full( len (df), np.nan) # align with df rows y_true_all = np.full( len (df), np.nan) # optional, for plotting/metrics for f in unique_folds:\n    test_idx = np.where(fold_values =",
      "content_length": 1997,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.74,
      "scrape_method": "requests"
    },
    {
      "content_id": "d3820f2afafa397178f034a6395578d0",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "Sasha Luccioni\nPRO",
      "url": "https://huggingface.co/sasha",
      "author": "Unknown",
      "published_at": "2025-06-18T00:00:00",
      "fetched_at": "2025-10-17T17:30:33.317773",
      "content_type": "article",
      "content_text": "47 11 21 Sasha Luccioni PRO sasha Follow anil1979's profile picture SelmaNajih001's profile picture mrchrisadams's profile picture 338\n\t\t\t\t\tfollowers \u00b7 68 following https://www.sashaluccioni.com/ sashaMTL sashavor AI & ML interests Natural Language Processing, Data, Ethics, Climate Change Organizations Articles 29 Article 10 Ethics + Sustainability = Responsible AI Article 12 \ud83c\udf0e What kind of environmental impacts are AI companies disclosing? (And can we compare them?) \ud83c\udf0e View all Articles Papers 22 arxiv: 2504.00797 arxiv: 2502.02649 arxiv: 2501.16548 arxiv: 2409.14160 Expand 22 papers spaces 19 Sort:\u00a0\n\t\tRecently updated Running 5 Environmental Transparency \ud83d\udcc8 Data from \"Misinformation by Omission\" sasha Jun 18 Running 10 Find My Pedro Pascal \ud83d\ude0d \ud83d\ude0d Find Pedro Pascal lookalikes in images sasha Jun 16 Runtime error 21 CO2 Inference \ud83c\udf0e Explore carbon emissions of ML models with interactive plots sasha Jun 16 Sleeping Leaderboard Yourbench Sasha Who2024report \ud83d\udc20 Explore task performance with leaderboard analytics sasha May 23 Sleeping Leaderboard Yourbench Sasha Worldbank2024report \u26a1 Explore task performance with leaderboard analytics sasha May 23 Sleeping Leaderboard Yourbench Sasha Ipcc Docs Test \ud83c\udfe2 Explore task performance with leaderboard analytics sasha May 23 View 19\n\t\t\t\t\t\t\tSpaces models 39 Sort:\u00a0\n\t\tRecently updated sasha/frugalAIaudiobaseline_whisper Updated Jan 21 sasha/khipu-finetuned-amazon_reviews_multi Text Classification \u2022 Updated Mar 6, 2023 \u2022 4 sasha/autotrain-sea-slug-similarity-2498977005 Image Classification \u2022 Updated Dec 16, 2022 \u2022 11 sasha/autotrain-butterfly_similarity_swin-2490776951 Image Classification \u2022 Updated Dec 16, 2022 \u2022 6 sasha/autotrain-butterfly-similarity-2490576840 Image Classification \u2022 Updated Dec 16, 2022 \u2022 15 sasha/autotrain-RobertaBaseTweetEval-1281048989 Text Classification \u2022 Updated Aug 19, 2022 \u2022 2 sasha/autotrain-RobertaBaseTweetEval-1281048990 Text Classification \u2022 Updated Aug 19, 2022 \u2022 3 sasha/autotrain-BERTBase-TweetEval-1281248999 Text Classification \u2022 Updated Aug 19, 2022 \u2022 6 sasha/autotrain-DistilBERT-TweetEval-1281148991 Text Classification \u2022 Updated Aug 19, 2022 \u2022 4 sasha/autotrain-BERTBase-TweetEval-1281248998 Text Classification \u2022 Updated Aug 19, 2022 \u2022 2 View 39\n\t\t\t\t\t\t\tmodels datasets 42 Sort:\u00a0\n\t\tRecently updated sasha/insectarium-butterflies Viewer \u2022 Updated 9 days ago \u2022 9.98k \u2022 12 sasha/details_CohereLabs__c4ai-command-r-plus-08-2024_private Viewer \u2022 Updated May 23 \u2022 184 \u2022 73 sasha/ipcc_docs_test Viewer \u2022 Updated May 23 \u2022 137 \u2022 29 sasha/ipcc_full_eval Viewer \u2022 Updated May 22 \u2022 149 \u2022 18 sasha/AIEnvironmentPrimer Updated Sep 3, 2024 \u2022 8 \u2022 1 sasha/co2_models Viewer \u2022 Updated Feb 20, 2024 \u2022 1.5k \u2022 40 sasha/co2_energy_data Viewer \u2022 Updated Feb 20, 2024 \u2022 695 \u2022 33 sasha/prof_images_blip__wavymulder-Analog-Diffusion Viewer \u2022 Updated Jun 3, 2023 \u2022 14.6k \u2022 1.24k sasha/prof_images_blip__SG161222-Realistic_Vision_V1.4 Viewer \u2022 Updated Jun 3, 2023 \u2022 14.6k \u2022 4.81k sasha/prof_images_blip__stabilityai-stable-diffusion-2 Viewer \u2022 Updated Jun 3, 2023 \u2022 14.6k \u2022 481 View 42\n\t\t\t\t\t\t\tdatasets",
      "content_length": 399,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.5900000000000001,
      "scrape_method": "requests"
    },
    {
      "content_id": "18b47833bf931f5e18185678197cac28",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "Hugging Science",
      "url": "https://huggingface.co/hugging-science",
      "author": "Unknown",
      "published_at": "2025-10-16T00:00:00",
      "fetched_at": "2025-10-17T17:30:34.261567",
      "content_type": "article",
      "content_text": "Hugging Science Community-driven open science for the age of AI About Many of the deepest challenges in advancing AI for scientific discovery are not purely technical\u2014they are social and organizational. Progress is often limited not by algorithms or computational power, but by how effectively we coordinate efforts, share resources, and collaborate across disciplinary boundaries. Hugging Science brings together a global community of researchers, developers, and practitioners committed to accelerating breakthroughs in physics, biology, chemistry, neuroscience, and beyond through open collaboration. Our vision is grounded in the argument presented in our position paper : democratizing AI for science requires treating it as a collective social project where equitable participation and sustainable collaboration are prerequisites for technical progress. What We Do Launch collaborative challenges and open problem calls to identify and mobilize collective effort around upstream computational bottlenecks with broad applicability across scientific domains\u2014such as efficient PDE solvers, multi-scale coupling, and high-dimensional sampling\u2014rather than fragmenting resources across narrow, domain-specific applications Build open toolkits, benchmarks, and workflows that address data fragmentation through standardized formats and shared evaluation metrics, making it easier for researchers at institutions of all resource levels to collaborate and build on each other's work Support cross-disciplinary exchange and education by creating resources that bridge the communication gap between domain scientists who prioritize mechanistic understanding and ML researchers who focus on predictive performance, enabling more effective collaboration Nurture a community that values contributions to data curation, infrastructure development, and educational resources alongside algorithmic innovation\u2014recognizing that datasets and infrastructure often have far greater long-term impact than individual models Learn together through open discussion of both technical advances and the social and institutional barriers that constrain progress, working to align incentives and build sustainable practices for scientific AI Join Us Discord : Join our server for real-time collaboration, discussion, and community coordination. GitHub : Explore and contribute to our open-source projects and initiatives. New to the community? Join the Discord to connect with other members and explore current initiatives. We'll help you find your place. Have a project idea? You don't need permission to start building. If you'd like community support, computational resources, or collaborative connections, reach out via Discord or GitHub. Learn More Read our position paper: AI for Scientific Discovery is a Social Problem Explore the Science Release Heatmap to visualize open models, datasets, and applications from organizations contributing to AI4Science, tracking release patterns across materials science, biology, physics, chemistry, and more! Hugging Science is an open community effort. We believe that by working openly and collaboratively, we can make AI-enabled scientific discovery more accessible, equitable, and impactful.",
      "content_length": 427,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.7100000000000001,
      "scrape_method": "requests"
    },
    {
      "content_id": "47f507fb5870c183daf9e9bff9c3ee86",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "Treble Technologies",
      "url": "https://huggingface.co/treble-technologies",
      "author": "Unknown",
      "published_at": "2025-10-13T00:00:00",
      "fetched_at": "2025-10-17T17:30:34.963426",
      "content_type": "article",
      "content_text": "Treble Technologies is an Icelandic software company pioneering next-generation sound simulation technology for audio algorithm development and acoustic virtual prototyping. Our vision is simple yet ambitious: to make the world a better sounding and less noisy place. Our simulation engine supports complex geometries, irregularly shaped spaces, and detailed CAD device models\u2014allowing for realistic acoustic environments and accurate multi-channel device modeling at scale. In audio R&D, quality isn\u2019t optional, it defines the product. That\u2019s why Treble is building powerful tools that enable researchers and engineers to simulate high-quality audio data, to refine audio AI and ML models, and prototype innovative sound experiences with unmatched precision and confidence.",
      "content_length": 106,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.68,
      "scrape_method": "requests"
    },
    {
      "content_id": "8194bcba2ae838e056fa6af41227a55d",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "Nemotron-Personas-India: Synthesized Data for Sovereign AI",
      "url": "https://huggingface.co/blog/nvidia/nemotron-personas-india",
      "author": "Unknown",
      "published_at": "2025-10-17T17:30:35.752429",
      "fetched_at": "2025-10-17T17:30:35.752493",
      "content_type": "article",
      "content_text": "Back to Articles Nemotron-Personas-India: Synthesized Data for Sovereign AI Community Article Published\n\t\t\t\tOctober 13, 2025 Upvote 9 +3 Kiran Praveen kipraveen Follow nvidia Utkarsh Vaidya uvaidya Follow nvidia Evan A eacharya-nv Follow nvidia Lipika Ramaswamy lipikaxnv Follow nvidia Dhruv Nathawani dnathawani Follow nvidia Dane Corneil dcorneil Follow nvidia Yev Meyer nv-3mei Follow nvidia A compound AI approach to Indian personas grounded in real-world distributions Open Data for India's AI Future India represents one of the world's largest AI opportunities \u2014 with over 700 million internet users, a multitude of languages, and a rapidly growing developer ecosystem. Yet, most open datasets reflect Western norms and English-only contexts, creating a data gap that limits AI adoption in India's multilingual, multi-script environment. Today, we're releasing Nemotron-Personas-India , the first open synthetic dataset of Indic personas aligned to India's real-world demographic, geographic, and cultural distributions. Licensed under CC BY 4.0 , this dataset offers a privacy-preserving, regulation-ready foundation for scaling AI systems that reflect Indian society\u2014without relying on sensitive personal data. Built with NeMo Data Designer , NVIDIA's enterprise-grade synthetic data generation microservice, Nemotron-Personas-India extends our global collection of Sovereign AI datasets. It builds on the success of our US and Japan persona datasets and includes new features designed specifically for India's culturally rich landscape. This dataset integrates seamlessly with Nemotron models and other open-source LLMs, making it easy to fine-tune AI systems for Indian use cases\u2014from multilingual chatbots to culturally-grounded specialized copilots. This release complements our earlier suite of Hindi evaluation datasets \u2014 including ChatRAG-Hi , IFEval-Hi , MT-Bench-Hi , GSM8K-Hi , and BFCL-Hi \u2014 supporting a complete pipeline from synthetic data generation to rigorous model evaluation for Indian AI systems. What\u2019s in the Dataset? 21 million personas total (3M records \u00d7 7 personas each) Multilingual support: English and Hindi, in both Devanagari and Latin scripts 27 fields per record: Persona traits + contextual attributes grounded in official census and labor statistics, including age, gender, education, occupation, state, district, and more 7.7 billion tokens total , including 2.9B persona tokens English: 1B tokens total, 394M persona tokens Hindi (Devanagari): 4.7B tokens total, 1.8B persona tokens Hindi (Latin): 2B tokens total, 746M persona tokens ~560k unique full names , reflecting India's vast linguistic diversity 2.9k occupational categories , including informal, formal, and traditional sectors All 36 states of India and 640 districts represented Natural language fields cultural background, linguistic background, skills and expertise, hobbies and interests Persona types: Includes general, professional, linguistic, culinary, sports, arts, and travel personas Licensed under CC BY 4.0 for commercial and non-commercial use How We Built It Data Generation Pipeline Produced using NeMo Data Designer , NVIDIA's microservice for synthetic data generation. This compound AI system enables generation with complex Jinja templating, Pydantic validation, structured outputs, automated retries, and supports multiple generation backends \u2013 the necessary tooling to scale a synthetic dataset of this size. We also leveraged the following models: Probabilistic Graphical Model (Apache-2.0) for statistical grounding GPT-OSS-120B (Apache-2.0) for narrative generation in English, Hindi (Devanagari), and Hindi (Latin) Embedded Cultural Context This dataset was aligned to India\u2019s official demographic distributions from the 2011 Census and expanded to include attributes essential for trustworthy AI training: Education: Expanded degree levels to reflect India\u2019s diverse academic pathways Occupations: Includes formal, informal, and traditional sectors like farming, tailoring, and street vending Life Stages: Student, homemaker, retired, and unemployed categories included Cultural Traits: Family structures, regional festivals, marriage traditions, and norms Digital Divide: Modeled usage patterns across urban/rural, age, and income lines Linguistic Diversity: Included incredible diversity with respect to first, second, and third spoken languages for each synthetic persona Private By Design No real names. No re-identification risk. All personas are fully synthetic. While grounded in real-world distributions from the 2011 Census and Parsed Indian Electoral Rolls data, no data is tied to any living or deceased individual. This ensures developers can safely train AI systems without privacy risks or regulatory barriers. Who This Is For Built for India, Ready for the World Nemotron\u2011Personas\u2011India is designed for developers building Sovereign AI systems for the Indian market , as well as global teams looking to adapt models to India\u2019s unique linguistic, cultural, and social context. Most open datasets today reflect English-speaking, Western norms\u2014limiting AI performance in India\u2019s multilingual, multi-script, and demographically complex environments. Practical AI Applications With Nemotron\u2011Personas\u2011India, teams can: Generate diverse, realistic training data in Indian languages and scripts Fine-tune models to capture local social, occupational, and cultural nuance Build region-aware AI agents that generalize across India\u2019s many communities Develop domain-specific copilots tuned to Indian professional and civic workflows Create multilingual systems capable of handling complex multi-turn conversations and varying levels of digital fluency Why It Matters India's 1.4 billion people speak hundreds of languages and live across vast cultural, economic, and geographic divides. India's National AI Portal estimates over 7,000 AI startups and research institutions are working to build locally relevant AI systems, and the Digital India initiative and government programs like IndiaAI are accelerating adoption. But progress is constrained by a fundamental gap: high-quality, culturally grounded training data that reflects India's demographic reality. Without representative datasets, AI systems struggle with code-switching between English and Hindi, fail to understand regional occupational categories, and miss cultural context essential for trust and adoption. The dataset improves diversity of synthetically-generated data, mitigates biases, and prevents model collapse (degradation caused by uncurated training on another model's outputs) by reflecting India's real geographic and demographic distributions. Nemotron-Personas-India supports Indian model builders in developing Sovereign AI systems that incorporate important region-specific demographics and cultural context. Start Building with Nemotron-Personas-India Want to build AI systems that understand India's culture, languages, and people? To start experimenting today: from datasets import load_dataset # English personas nemotron_personas_en = load_dataset( \"nvidia/Nemotron-Personas-India\" , \"en_IN\" ) # Hindi personas in Devanagari nemotron_personas_hi_deva = load_dataset( \"nvidia/Nemotron-Personas-India\" , \"hi_Deva_IN\" ) # Hindi personas in Latin nemotron_personas_hi_latn = load_dataset( \"nvidia/Nemotron-Personas-India\" , \"hi_Latn_IN\" ) Whether you're an Indian model builder developing Sovereign AI or a global developer seeking better regional adoption, Nemotron-Personas-India provides the authentic, privacy-safe foundation your applications need. Download it. Fine-tune it. Build AI that understands India. If you\u2019re ready to go deeper, an extended version of Nemotron-Personas-India (which includes e.g., first/last names, religion, and synthetic addresses) is available in NeMo Data Designer .",
      "content_length": 1039,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.74,
      "scrape_method": "requests"
    },
    {
      "content_id": "0c9889211d85a7fdc6beae8863d67ed2",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "DeepSeek-R1 Dissection: Understanding PPO & GRPO Without Any Prior Reinforcement Learning Knowledge",
      "url": "https://huggingface.co/blog/NormalUhr/grpo",
      "author": "Unknown",
      "published_at": "2025-02-13T00:00:00",
      "fetched_at": "2025-10-17T17:30:36.757770",
      "content_type": "article",
      "content_text": "Back to Articles DeepSeek-R1 Dissection: Understanding PPO & GRPO Without Any Prior Reinforcement Learning Knowledge Community Article Published\n\t\t\t\tFebruary 7, 2025 Upvote 235 +229 Yihua Zhang NormalUhr Follow 1. Introduction In Reinforcement Learning (RL), simply knowing \u201chow many points you score\u201d often isn\u2019t enough. Pursuing high scores alone can lead to various side effects, such as excessive exploration, instability in the model, or even \u201cshortcutting\u201d behaviors that deviate from reasonable policies. To address these challenges, RL incorporates several mechanisms\u2014such as the Critic (value function), Clip operation, Reference Model, and the more recent Group Relative Policy Optimization (GRPO). To make these concepts more intuitive, let\u2019s draw an analogy: think of the RL training process as an elementary school exam scenario. We (the model being trained) are like students trying to get high grades, the teacher who grades our exams are like the reward model, while our parents handing out pocket money based on our grades is similar to the Critic. Next, let\u2019s walk step by step through why final scores alone are insufficient, how Critic, Clip, and Reference Model come into play, and finally how GRPO extends these ideas. 2. The Naive Approach of Only Using Reward: What\u2019s the Problem? Suppose my younger brother and I are in the same elementary school class. The teacher grades our exams and gives an \u201cabsolute score.\u201d I typically score above 80 out of 100, while my brother often gets around 30. We then take these scores directly to our dad to ask for pocket money\u2014meaning our \u201creward\u201d (in RL terms) is simply our raw exam score. Whoever gets a higher score receives more pocket money. At first glance, that seems fine. But two big issues quickly arise: Unfairness : If my brother improves from 30 to 60 points through a lot of hard work, he still pales in comparison to my usual 80+. He doesn\u2019t get the encouragement he deserves. Instability : Chasing higher scores myself could lead me to extreme study methods (e.g., cramming at all hours, staying up very late). Sometimes I might get 95, other times only 60, so my score\u2014and hence the reward signal\u2014fluctuates dramatically. As a result, using absolute scores as Reward causes large reward fluctuations, and my brother ends up feeling it\u2019s not worth trying to improve in small increments. Mathematical Correspondence In RL, if we simply do: J naive ( \u03b8 ) = E ( q , o ) \u223c ( data , \u03c0 \u03b8 ) [ r ( o ) ] , \\mathcal{J}_{\\text{naive}}(\\theta) \n= \\mathbb{E}_{(q, o) \\sim (\\text{data}, \\pi_{\\theta})}\\big[r(o)\\big], J naive \u200b ( \u03b8 ) = E ( q , o ) \u223c ( data , \u03c0 \u03b8 \u200b ) \u200b [ r ( o ) ] , which means \u201coptimize only the final reward,\u201d we can run into high variance and insufficient incentives for partial improvements. In other words, the Actor lacks a baseline that matches its own current level, and that hinders training efficiency. 3. Introducing the Critic: Using a \u201cPredicted Score Line\u201d to Improve Rewards Recognizing this problem, Dad realizes that \u201cit\u2019s not just about the absolute score; it\u2019s about how much you\u2019ve improved relative to your current level.\u201d So he decides: Set my \u201cpredicted score line\u201d at 80 points and my brother\u2019s at 40. If we exceed these lines on an exam, we get more pocket money; if not, we get very little or none. Hence, if my brother works hard and jumps from 30 to 60, he\u2019s 20 points above his \u201cpredicted score line,\u201d which translates into a hefty reward. Meanwhile, if I remain around the 80s, the incremental gain is smaller, so I won\u2019t necessarily receive much more than he does. This arrangement encourages each person to improve from their own baseline instead of purely comparing absolute scores. Of course, Dad is busy, so once a line is set, it doesn\u2019t just remain static\u2014he needs to keep \u201creadjusting\u201d as we progress. If my brother levels up to the 60 range, then a 40-point baseline is no longer fair. Likewise, if I consistently hover around 85, Dad might need to tweak my line as well. In other words, Dad also has to learn , specifically about the pace at which my brother and I are improving. Mathematical Correspondence In RL, this \u201cscore line\u201d is known as the value function , V \u03c8 ( s ) V_{\\psi}(s) V \u03c8 \u200b ( s ) . It acts as a baseline. Our training objective evolves from \u201cjust reward\u201d to \u201chow much we outperform that baseline,\u201d expressed by the Advantage: A t = r t \u2212 V \u03c8 ( s t ) . A_t = r_t - V_{\\psi}(s_t). A t \u200b = r t \u200b \u2212 V \u03c8 \u200b ( s t \u200b ) . For a given state s t s_t s t \u200b and action o t o_t o t \u200b , if the actual reward exceeds the Critic\u2019s expectation, it means the action performed better than predicted. If it\u2019s lower, that action underperformed. In the simplest formulation, we optimize something like: J adv ( \u03b8 ) = E [ A ( o ) ] , where A ( o ) = r ( o ) \u2212 V \u03c8 ( o ) . \\mathcal{J}_{\\text{adv}}(\\theta) = \\mathbb{E}\\big[A(o)\\big],\n\\quad\n\\text{where } A(o) = r(o) - V_{\\psi}(o). J adv \u200b ( \u03b8 ) = E [ A ( o ) ] , where A ( o ) = r ( o ) \u2212 V \u03c8 \u200b ( o ) . By subtracting this \u201cscore line,\u201d we reduce variance in training, giving higher gradient signals to actions that exceed expectations and penalizing those that fall short. 4. Adding Clip and Min Operations: Preventing Over-Updates Even with the \u201cscore line,\u201d new problems can emerge. For instance: If I suddenly break through on a test and score 95 or 100, Dad might give me a huge reward, pushing me to adopt overly aggressive study patterns before the next exam. My grades might swing between extremes (95 and 60), causing massive reward volatility. Thus, Dad decides to moderate how drastically I can update my study strategy in each step\u2014he won\u2019t give me exponentially more pocket money just because of one good test. If he gives too much, I might veer into extreme exploration; if too little, I won\u2019t be motivated. So he must find a balance. Mathematical Correspondence In PPO (Proximal Policy Optimization) , this balance is achieved through the \u201cClip\u201d mechanism. The core of the PPO objective includes: min \u2061 ( r t ( \u03b8 ) A t , clip ( r t ( \u03b8 ) , 1 \u2212 \u03b5 , 1 + \u03b5 ) A t ) , \\min \\Big(r_t(\\theta) A_t,\\ \\text{clip}\\big(r_t(\\theta), 1 - \\varepsilon,\\, 1 + \\varepsilon\\big)\\,A_t\\Big), min ( r t \u200b ( \u03b8 ) A t \u200b , clip ( r t \u200b ( \u03b8 ) , 1 \u2212 \u03b5 , 1 + \u03b5 ) A t \u200b ) , where r t ( \u03b8 ) = \u03c0 \u03b8 ( o t \u2223 s t ) \u03c0 \u03b8 old ( o t \u2223 s t ) , r_t(\\theta) = \\frac{\\pi_{\\theta}(o_t\\mid s_t)}{\\pi_{\\theta_{\\text{old}}}(o_t\\mid s_t)}, r t \u200b ( \u03b8 ) = \u03c0 \u03b8 old \u200b \u200b ( o t \u200b \u2223 s t \u200b ) \u03c0 \u03b8 \u200b ( o t \u200b \u2223 s t \u200b ) \u200b , represents the probability ratio between the new and old policies for that action. If the ratio deviates too far from 1, it\u2019s clipped within [ 1 \u2212 \u03b5 , 1 + \u03b5 ] \\bigl[\\,1-\\varepsilon,\\ 1+\\varepsilon\\bigr] [ 1 \u2212 \u03b5 , 1 + \u03b5 ] , which limits how much the policy can shift in one update. In simpler terms: Scoring 100 gets me extra rewards, but Dad imposes a \u201cceiling\u201d so I don\u2019t go overboard. He\u2019ll then reassess on the next exam, maintaining a steady approach rather than fueling extreme fluctuations. 5. Reference Model: Preventing Cheating and Extreme Strategies Even so, if I\u2019m solely fixated on high scores, I might resort to questionable tactics \u2014for instance, cheating or intimidating the teacher into awarding me a perfect score. Clearly, that breaks all rules. In the realm of large language models, an analogous scenario is producing harmful or fabricated content to artificially boost some reward metric. Dad, therefore, sets an additional rule: \u201cNo matter what, you can\u2019t deviate too much from your original, honest approach to studying. If you\u2019re too far off from your baseline, even with a high score, I\u2019ll disqualify you and withhold your pocket money.\u201d That\u2019s akin to marking down a \u201creference line\u201d from the start of the semester (i.e., after initial supervised fine-tuning). You can\u2019t stray too far from that original strategy or you face penalties. Mathematical Correspondence In PPO, this is reflected by adding a KL penalty against the Reference Model (the initial policy). Concretely, we include something like: \u2212 \u03b2 D K L ( \u03c0 \u03b8 \u2225 \u03c0 ref ) -\\beta\\, \\mathbb{D}_{\\mathrm{KL}}\\big(\\pi_{\\theta}\\,\\|\\ \\pi_{\\text{ref}}\\big) \u2212 \u03b2 D KL \u200b ( \u03c0 \u03b8 \u200b \u2225 \u03c0 ref \u200b ) in the loss. This keeps the Actor from drifting too far from the original, sensible policy, avoiding \u201ccheating\u201d or other drastically out-of-bounds behaviors. 6. GRPO: Replacing the Value Function with \u201cMultiple Simulated Averages\u201d One day, Dad says, \u201cI don\u2019t have time to keep assessing your learning progress and draw new score lines all the time. Why not do five sets of simulated tests first, then take their average score as your expected score ? If you surpass that average on the real test, it shows you did better than your own expectations, so I\u2019ll reward you. Otherwise, you won\u2019t get much.\u201d My brother and I, and potentially more classmates, can each rely on a personal set of simulated tests rather than an external \u201cvalue network\u201d that Dad would have to constantly adjust. Up until now, we saw that PPO relies on the Actor + Critic + Clip + KL penalty framework. However, in large language model (LLM) scenarios, the Critic (value function) often needs to be as large as the Actor to accurately evaluate states, which can be costly and sometimes impractical\u2014especially if you only have a single final reward at the end (like a final answer quality). Hence, Group Relative Policy Optimization (GRPO) steps in. Its core idea: No separate value network for the Critic, Sample multiple outputs from the old policy for the same question or state, Treat the average reward of these outputs as the baseline , Anything above average yields a \u201cpositive advantage,\u201d anything below yields a \u201cnegative advantage.\u201d Meanwhile, GRPO retains PPO\u2019s Clip and KL mechanisms to ensure stable, compliant updates. Mathematical Correspondence According to DeepSeekMath\u2019s technical report, the GRPO objective (omitting some symbols) is: J G R P O ( \u03b8 ) = E [ \u2211 i = 1 G ( min \u2061 ( \u03c0 \u03b8 ( o i ) \u03c0 \u03b8 old ( o i ) A i , clip ( \u03c0 \u03b8 ( o i ) \u03c0 \u03b8 old ( o i ) , 1 \u2212 \u03b5 , 1 + \u03b5 ) A i ) \u2212 \u03b2 D K L ( \u03c0 \u03b8 \u2225 \u03c0 ref ) ) ] , ",
      "content_length": 2321,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.6199999999999999,
      "scrape_method": "requests"
    },
    {
      "content_id": "405a448a80ee46a8968bfa1982c90ed7",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "Uncensor any LLM with abliteration",
      "url": "https://huggingface.co/blog/mlabonne/abliteration",
      "author": "Unknown",
      "published_at": "2025-02-11T00:00:00",
      "fetched_at": "2025-10-17T17:30:37.620767",
      "content_type": "article",
      "content_text": "Back to Articles Uncensor any LLM with abliteration Community Article Published\n\t\t\t\tJune 13, 2024 Upvote 696 +690 Maxime Labonne mlabonne Follow The third generation of Llama models provided fine-tunes (Instruct) versions that excel in understanding and following instructions. However, these models are heavily censored, designed to refuse requests seen as harmful with responses such as \"As an AI assistant, I cannot help you.\" While this safety feature is crucial for preventing misuse, it limits the model's flexibility and responsiveness. In this article, we will explore a technique called \"abliteration\" that can uncensor any LLM without retraining. This technique effectively removes the model's built-in refusal mechanism, allowing it to respond to all types of prompts. The code is available on Google Colab and in the LLM Course on GitHub. \u2702\ufe0f What is abliteration? Modern LLMs are fine-tuned for safety and instruction-following, meaning they are trained to refuse harmful requests. In their blog post , Arditi et al. have shown that this refusal behavior is mediated by a specific direction in the model's residual stream. If we prevent the model from representing this direction, it loses its ability to refuse requests . Conversely, adding this direction artificially can cause the model to refuse even harmless requests. In the traditional decoder-only Llama-like architecture, there are three residual streams we can target: at the start of each block (\"pre\"), between the attention and MLP layers (\"mid\"), and after the MLP (\"post\"). The following figure illustrates the location of each residual stream. To uncensor an LLM, we first need to identify the \"refusal direction\" within the model. This process involves a few technical steps: Data Collection : Run the model on a set of harmful instructions and a set of harmless instructions, recording the residual stream activations at the last token position for each. Mean difference : Calculate the mean difference between the activations of harmful and harmless instructions. This gives us a vector representing the \"refusal direction\" for each layer of the model. Selection : Normalize these vectors and evaluate them to select the single best \"refusal direction.\" Once we have identified the refusal direction, we can \"ablate\" it, effectively removing the model's ability to represent this feature. This can be done through an inference-time intervention or permanently with weight orthogonalization . Let's talk about inference-time intervention first. For every component that writes to the residual stream (such as an attention head), we calculate the projection of its output onto the refusal direction and subtract this projection. This subtraction is applied at every token and every layer, ensuring that the model never represents the refusal direction. On the other hand, weight orthogonalization involves modifying the model weights directly. By orthogonalizing the component weights with respect to the refusal direction, it prevents the model from writing to this direction altogether. This is achieved by adjusting the matrices that write to the residual stream, ensuring they do not contribute to the refusal direction. In the next section, we will implement abliteration with weight orthogonalization. \ud83d\udcbb Implementation The following implementation of abliteration is based on FailSpy's notebook , which is itself based on the original authors' notebook . I mostly adapted and simplified it to make it easier to understand. This section is quite code-heavy so you can see what is going on, but you can use FailSpy's abliterator library if you're less interested in the technical details (also check his collection of abliterated models on Hugging Face). The code relies on the excellent TransformerLens library (formerly known as EasyTransformer) to do the heavy lifting. It is designed for mechanistic interpretability and is used here to intervene on activations. Thanks to Neel Nanda and Joseph Bloom for creating and maintaining this library. First, let's install the necessary packages and import them. All these steps are available in this Google Colab notebook . !pip install transformers transformers_stream_generator tiktoken transformer_lens einops jaxtyping import torch import functools import einops import gc from datasets import load_dataset from tqdm import tqdm from torch import Tensor from typing import List from transformer_lens import HookedTransformer, utils from transformer_lens.hook_points import HookPoint from transformers import AutoModelForCausalLM, AutoTokenizer from jaxtyping import Float, Int from collections import defaultdict # Turn automatic differentiation off to save GPU memory (credit: Undi95) torch.set_grad_enabled( False ) We need two datasets: one containing harmless instructions, and one containing harmful instructions. We'll use tatsu-lab/alpaca as well as data from llm-attacks . To make things easier, I repackaged them in two Hugging Face datasets: mlabonne/harmless_alpaca and mlabonne/harmful_behaviors . That way, you can easily replace them with your own datasets. We will load the instructions and reformat them into a list of dictionaries with \"role\" and \"content\" keys. This makes it compatible with the apply_chat_tokenizer() method, which we will use to follow Llama 3's chat template. def reformat_texts ( texts ): return [[{ \"role\" : \"user\" , \"content\" : text}] for text in texts] # Get harmful and harmless datasets def get_harmful_instructions ():\n    dataset = load_dataset( 'mlabonne/harmful_behaviors' ) return reformat_texts(dataset[ 'train' ][ 'text' ]), reformat_texts(dataset[ 'test' ][ 'text' ]) def get_harmless_instructions ():\n    dataset = load_dataset( 'mlabonne/harmless_alpaca' ) return reformat_texts(dataset[ 'train' ][ 'text' ]), reformat_texts(dataset[ 'test' ][ 'text' ])\n\nharmful_inst_train, harmful_inst_test = get_harmful_instructions()\nharmless_inst_train, harmless_inst_test = get_harmless_instructions() Now that we have our datasets, we can load the model we want to abliterate. Unfortunately, you can't directly load a custom model using HookedTransformer . Here, I use a trick described in FailSpy's notebook to download a custom model and rename it as meta-llama/Meta-Llama-3-8B-Instruct . Load in torch.float16 format if your GPU is not compatible with BF16. In this example, we'll use mlabonne/Daredevil-8B , a mega-merge created with DARE TIES (see my article about model merging ) that has the highest MMLU score on the Open LLM Leaderboard in the 8B category. MODEL_ID = \"mlabonne/Daredevil-8B\" MODEL_TYPE = \"meta-llama/Meta-Llama-3-8B-Instruct\" # Download and load model !git clone https://huggingface.co/{MODEL_ID} {MODEL_TYPE} # Load model and tokenizer model = HookedTransformer.from_pretrained_no_processing(\n    MODEL_TYPE,\n    local_files_only= True ,\n    dtype=torch.bfloat16,\n    default_padding_side= 'left' )\ntokenizer = AutoTokenizer.from_pretrained(MODEL_TYPE)\ntokenizer.padding_side = 'left' tokenizer.pad_token = tokenizer.eos_token We can now tokenize our datasets. We're using the same number of samples for both harmless and harmful instructions. Note that a high number of samples can use all the RAM/VRAM, which is why I'm limiting it to 256 here. def tokenize_instructions ( tokenizer, instructions ): return tokenizer.apply_chat_template(\n        instructions,\n        padding= True ,\n        truncation= False ,\n        return_tensors= \"pt\" ,\n        return_dict= True ,\n        add_generation_prompt= True ,\n    ).input_ids\n\nn_inst_train = min ( 256 , len (harmful_inst_train), len (harmless_inst_train)) # Tokenize datasets harmful_tokens = tokenize_instructions(\n    tokenizer,\n    instructions=harmful_inst_train[:n_inst_train],\n)\nharmless_tokens = tokenize_instructions(\n    tokenizer,\n    instructions=harmless_inst_train[:n_inst_train],\n) Everything is set up, we can now implement the first step of abliteration: data collection. We want to process these tokenized datasets and store the residual stream activations in harmful and harmless . This is managed by the transformer_lens library. # Define batch size based on available VRAM batch_size = 32 # Initialize defaultdicts to store activations harmful = defaultdict( list )\nharmless = defaultdict( list ) # Process the training data in batches num_batches = (n_inst_train + batch_size - 1 ) // batch_size for i in tqdm( range (num_batches)): print (i)\n    start_idx = i * batch_size\n    end_idx = min (n_inst_train, start_idx + batch_size) # Run models on harmful and harmless prompts, cache activations harmful_logits, harmful_cache = model.run_with_cache(\n        harmful_tokens[start_idx:end_idx],\n        names_filter= lambda hook_name: 'resid' in hook_name,\n        device= 'cpu' ,\n        reset_hooks_end= True )\n    harmless_logits, harmless_cache = model.run_with_cache(\n        harmless_tokens[start_idx:end_idx],\n        names_filter= lambda hook_name: 'resid' in hook_name,\n        device= 'cpu' ,\n        reset_hooks_end= True ) # Collect and store the activations for key in harmful_cache:\n        harmful[key].append(harmful_cache[key])\n        harmless[key].append(harmless_cache[key]) # Flush RAM and VRAM del harmful_logits, harmless_logits, harmful_cache, harmless_cache\n    gc.collect()\n    torch.cuda.empty_cache() # Concatenate the cached activations harmful = {k: torch.cat(v) for k, v in harmful.items()}\nharmless = {k: torch.cat(v) for k, v in harmless.items()} We can now compute the refusal direction for each layer. This corresponds to the mean difference between the activations of harmful and harmless instructions, which is then normalized. We sort them in descending order in activation_scored . # Helper function to get activation index def get_act_idx ( cache_dict, act_name, layer ):\n    key = (act_name, layer) return cache_dict[utils.get_act_name(*key)] # Compute difference of means between harmful and harmless activations at intermediate layers activation_layers = [ \"resid_pre\" , \"resid_mid",
      "content_length": 2805,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.6199999999999999,
      "scrape_method": "requests"
    },
    {
      "content_id": "bf971a3c81c6618fa16951ba8b2e654a",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "Ksenia Se",
      "url": "https://huggingface.co/Kseniase",
      "author": "Unknown",
      "published_at": "2025-03-03T00:00:00",
      "fetched_at": "2025-10-17T17:30:38.501541",
      "content_type": "article",
      "content_text": "view post Post 3052 9 Powerful AI Video Generation Tools Since Sora 2 is on fire these weeks, reminding us what high-quality video generation should look like, we decided you really need this list of video generation tools \u2013 great alternatives or complements to it. 1. Sora 2 \u2192 https://openai.com/sora/ It needs no introduction, but this OpenAI\u2019s text-to-video model produces short, ultra-realistic clips across styles (cinematic, photorealistic, animated, etc.) with synced audio 2. Google Veo 3 (Gemini Video Generation) \u2192 https://aistudio.google.com/models/veo-3 Part of Gemini AI. Generates 8-second high-fidelity videos from text or images with native sound: background soundtracks and realistic voices with near-perfect lip sync 3. Runway (Gen-4 by Runway ML) \u2192 https://runwayml.com/ Text, image, or video-to-video generation with advanced editing like changing lighting, weather, camera angles or replacing objects. Popular in AI filmmaking 4. Pika Labs \u2192 https://pollo.ai/m/pika-ai Provides creative, often stylized short videos \u2013 from cinematic mini-scenes to cartoon-like animations. Ideal for social media clips and visual storytelling. Plus, you can add playful effects to manipulate objects in the generated videos 5. Luma\u2019s Dream Machine \u2192 https://lumalabs.ai/dream-machine Powered by Luma AI\u2019s latest Ray 3 model, it quickly visualizes story ideas, animated concept art, or abstract motion videos. It supports consistent custom characters and seamless looping Read further below \u2b07\ufe0f If you like it, also subscribe to the Turing Post https://www.turingpost.com/subscribe See translation",
      "content_length": 224,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.5900000000000001,
      "scrape_method": "requests"
    },
    {
      "content_id": "ceee8cc3226156b9838ca0694cb2cfc5",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "Model statistics of the 50 most downloaded entities on Hugging Face",
      "url": "https://huggingface.co/blog/lbourdois/huggingface-models-stats",
      "author": "Unknown",
      "published_at": "2025-10-14T00:00:00",
      "fetched_at": "2025-10-17T17:30:39.387410",
      "content_type": "article",
      "content_text": "Back to Articles Model statistics of the 50 most downloaded entities on Hugging Face Community Article Published\n\t\t\t\tOctober 13, 2025 Upvote 21 +15 Lo\u00efck BOURDOIS lbourdois Follow Foreword It's not possible to add .hmtl graphics to Hugging Face blog posts. If you speak French, we really invite you to read the French version here instead where the graphics display well. For non-French speakers, we can only include screenshots in this article. Under each of them you will find a link to a dynamic plot. Please note that in the following, in the text within a blue box, we express only our personal reflection. Finally, it should be noted that on October 9, Elastic announced the acquisition of Jina.ai. We haven't had time to modify the various graphs to take this into account. Introduction In this blog post, we analyze the most impactful open-source models in practice . To do so, we focus on a very pragmatic metric: \"Which models are downloaded most often on the Hugging Face Hub?\". The assumption is that models that are downloaded massively are those that are used in the real world. This approach is also intended to be fairer to individuals/organizations that do not have a communication service or are not followed/liked massively on the Hub. TL;DR The analysis of the 50 most downloaded entities on the Hugging Face Hub (80.22% of total Hub downloads) shows that: Among all open-source models where it is possible to know the size of the model (96.94% of the top 50 and 77.76% of the Hub), small models are by far the most downloaded: - 92.48% of downloads are for models with fewer than one billion parameters, - 86.33% are for models with fewer than 500 million parameters, - 69.83% on models with fewer than 200 million parameters, - 40.17% on models with fewer than 100 million parameters. Model downloads primarily concern NLP (58.1%), followed by CV at 21.2%, audio at 15.1%, various forms of multimodality at 3.3%, time series at 1.7%, with the rest  undetermined due to a lack of correctly annotated metadata. Text encoders represent (base models + their fine-tuning on a specific task) more than 45% of total downloads (or 77.5% of the NLP modality), compared to only 9.5% for decoders (16.5% of the modality) and 3% for encoder-decoders (5% of the modality). Thus, contrary to the hype surrounding these models, LLMs are not being downloaded massively in open source. Could it be that their real-world use is more on the side of private APIs? English represents more than 79.46% of downloads of models (monolingual or multilingual) using a language (and even 92.85% if we only consider models with a language tag). This language is far ahead of the others. For example, French, which comes in second place, accounts for only 17.48% (20.43% of models with a language tag). Companies are the largest contributors to open source, accounting for 63.2% of downloads (20 entities out of 50), followed by universities at 20.7% (10 entities), then individuals at 12.1% (16 entities), non-profit organizations at 3.8% (4 entities), and finally hybrid laboratories at 0.3% (1 entity). The United States is presents everywhere, covering all modalities (NLP, vision, audio, time series, multimodalities) and all model sizes (from less than 5M parameters to tens/hundreds of billions). Americans are notably driven by their open-source companies (crushing all competition in this segment) but also have strengths in all types of existing organizations (excluding hybrid laboratories) since they are represented 18 times in this top 50. Europe (notably Germany, France, and the United Kingdom) is also positioned in all types of existing organizations outside of hybrid laboratories (present 20 times) but stands out due to the impact of its specialized universities on small models (<200M parameters). It is also present in all modalities except time series. China (represented by five entities) has a strong presence in the large open-source model segment (31.8% vs. 43.1% for the United States and 24% for Europe on models with more than 7.5 billion parameters). However, it is badly placed in all other model size categories (only 130 million downloads of models with fewer than 100 million parameters, compared to 7.05 billion for the United States and 5.3 billion for Europe). Its lack of positioning in vision (barely 4 million downloads) and audio (0 downloads) also penalizes it. These are not areas in which it is known to be lagging behind, but it is clear that it does not currently produce open-source content in these areas on Hugging Face (a platform that is not accessible in the country). It dominates the non-profit sector and is the only player in the university/business hybrid laboratory sector. Finally, other countries in this top 50 only benefit from a specialized player in a given modality. Data collected The data shown in this article was collected on October 1, 2025. After identifying the 50 open-source entities with the most downloads, we collected all the models associated with them. \nThis represents 72,423 models out of the 2,126,833 hosted on the Hub, or 3.41% of the total. These accounts represent exactly 36,450,707,797 downloads out of a total of 45,438,836,957, or 80.22%. For each of the models, the pipeline and language tags were also collected when available. Similarly, the size was estimated based on the .safetensors file. When information was missing (no tags or model size in particular), we manually corrected the data by consulting the model card or publication associated for the 1,000 most downloaded open-source models. These alone represent 77.89% of all Hub downloads and 97.10% of the 50 entities analyzed. Everything was finally stored in a dataframe that looked like this: Figure 1: Dataset used, available here All the graphs shown below are generated from this data. The amounts are rounded to the nearest million for clarity, but also because the Hub is experiencing some display issues. For example, for the sentence-transformers/static-retrieval-mrl-en-v1 model, no downloads are available . The goal here is mainly to understand the orders of magnitude rather than to focus on the exact numbers evolving every day. Figure 2: Example of what can be observed for models where the Hub does not correctly display the number of downloads Plots Overview In this first section, we display the overall downloads for each of the top 50 entities contributing to open source, as well as their category type and country of origin. We will discuss these last two points in dedicated sections. Note that we use the term \"entity\" rather than (Hugging Face) \"account\"  because an entity can be composed of several accounts. For example, Google is composed of google , google-bert , google-t5 , and albert . We therefore offer a global plot allowing you to compare the most downloaded entities, and another plot by sub-account to visualize how the different accounts are distributed within them. Overview Figure 3: Top 50 Hugging Face Entities by Total Downloads Dynamic version available here Sub-accounts view Figure 4: Top 50 Hugging Face Entities by Total Downloads (with sub-account breakdown) Dynamic version available here A word about each entity 1. The Google entity is composed of google , google-bert , google-t5 and albert . More than 74% of its downloads come from \"old models\", namely 64% from BERT, 6.8% from T5, and 3.2% from ALBERT. 2. The Meta entity is composed of FacebookAI , facebook and meta-llama . Similar observations as for Google but to a lesser extent, with 48.3% of downloads coming from its RoBERTa models versus 9% for Llamas. 3. The Sentence-transformers entity (from the Ubiquitous Knowledge Processing Lab in Darmstadt, Germany, and more specifically the work of Nils Reimers) completes the trio. This entity is composed of sentence-transformers and cross-encoder . The sentence-transformers account is actually the most downloaded on all of Hugging Face. 4. The Hugging Face entity is composed of timm (52.4% of downloads), distilbert (44.6%), llava-hf , HuggingFaceTB and HuggingFaceM4 . 5. The OpenAI entity is composed of openai (72%) and openai-community (28%). Although it publishes little in open-source, the company is extremely impactful when it does (its CLIP and Whisper are particularly downloaded). 6. 99% of MIT's downloads come from its ast-finetuned-audioset-10-10-0.4593 model, which is the second most downloaded model on the Hub. 7. Microsoft is another popular Big Tech company. The chart in the section on modalities shows that it is the most diversified organization in terms of modalities addressed (whereas, with the exception of a few others, most entities in the top 50 are highly specialized in a given modality). 8. Jonatas Grosman is the most downloaded individual. With barely 300 followers on the Hub, he is an illustration of the fact that it is not necessarily the most followed/liked entities that are the most impactful. He specialized for a time in finetuning wav2vec2 but has not published new models for 3 years now. 9. Pyannote specializes in small audio segmentation and diarization models. 10. 99% of Falcons.ai's downloads (350 followers) come from its nsfw_image_detection model, which is the seventh most downloaded model on the entire Hub. 11. BAAI is the most downloaded Chinese entity on Hugging Face, notably through its bge models. It is also the most downloaded non-profit entity. 12. The Alibaba entity is composed of Qwen (81.5%), Alibaba-NLP (9.5%), thenlper (7.7%), Wan-AI , AIDC-AI , alibaba-pai , alibaba-damo . It is thus primarily downloaded for its Qwen models (especially the Qwen2.5-1.5B-Instruct which represents 20% of its downloads and is the most downloaded LLM on the Hub). 13. The Amazon entity is composed of amazon (80.1%) and autogluon (19.9%). It is the only entity whose downloads are massively focused on time series. 14. Dima806 is primarily downloaded for its dima806/fairface_age_image_detection model. It is an individual still a",
      "content_length": 6153,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.74,
      "scrape_method": "requests"
    },
    {
      "content_id": "354ab7327ac366c7951670d851bca6ec",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "Richard Bian",
      "url": "https://huggingface.co/RichardBian",
      "author": "Unknown",
      "published_at": "2025-10-17T17:30:40.159743",
      "fetched_at": "2025-10-17T17:30:40.159768",
      "content_type": "article",
      "content_text": "11 28 40 Richard Bian RichardBian Follow zixuch24's profile picture SmiIemaker's profile picture curryandsun's profile picture 28\n\t\t\t\t\tfollowers \u00b7 26 following richsfo richardbsk richardbian AI & ML interests None yet Recent Activity new activity 1 day ago inclusionAI/Ring-1T-preview: How and where can i use this model online? new activity 1 day ago inclusionAI/Ring-1T-preview: Request: DOI liked a model 2 days ago inclusionAI/Ling-1T-FP8 View all activity Organizations Articles 3 Article 9 Ring-flash-linear-2.0: A Highly Efficient Hybrid Architecture for Test-Time Scaling Article 5 Ring-mini-2.0: Small Model, Great Intelligence View all Articles models 0 None public yet datasets 0 None public yet",
      "content_length": 99,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.68,
      "scrape_method": "requests"
    },
    {
      "content_id": "f78ae6ed657d4fbae8c16f942b7cd341",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "Hafedh Hichri\nPRO",
      "url": "https://huggingface.co/not-lain",
      "author": "Unknown",
      "published_at": "2025-08-29T00:00:00",
      "fetched_at": "2025-10-17T17:30:42.005137",
      "content_type": "article",
      "content_text": "view post Post 5447 \ud83d\ude80AraClip is now fully integrated with Hugging Face \ud83e\udd17 AraClip is a specialized CLIP model that was created by @ pain and optimized for Arabic text-image retrieval tasks\ud83d\udd25 \ud83d\udd17 Try it out \ud83d\udd17 \ud83e\udd16 model: Arabic-Clip/araclip \ud83e\udde9 Gradio demo: Arabic-Clip/Araclip-Simplified \ud83c\udf10 website: https://arabic-clip.github.io/Arabic-CLIP/ See translation",
      "content_length": 49,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.5399999999999999,
      "scrape_method": "requests"
    },
    {
      "content_id": "c10773e480ba5e9fcc9839cf4a1723cf",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "There is no such thing as a tokenizer-free lunch",
      "url": "https://huggingface.co/blog/catherinearnett/in-defense-of-tokenizers",
      "author": "Unknown",
      "published_at": "2025-09-26T00:00:00",
      "fetched_at": "2025-10-17T17:30:42.751787",
      "content_type": "article",
      "content_text": "Back to Articles There is no such thing as a tokenizer-free lunch Community Article Published\n\t\t\t\tSeptember 25, 2025 Upvote 78 +72 Catherine Arnett catherinearnett Follow An argument in defense of tokenizers. The only time most people hear about tokenization is when it\u2019s being blamed for some undesirable behavior of a language model. These incidents have helped turn ignorance and indifference towards tokenizers into active dismissal and disdain. This attitude makes it harder to understand the tokenizers and develop better ones, because fewer people are actually studying tokenizers. The goal of this blog post is to provide some context about how we got the tokenization approaches we have and argue that they\u2019re not actually so bad. On a personal level, I also want to foster more engagement with the tokenization literature.\nRegardless of whether you are pro- or anti-tokenization, more people to be working on issues related to tokenizers, the faster we\u2019re going to make progress. For those that think they are taking a \u201ctokenizer-free\u201d approach, I argue that these approaches are just other kinds of tokenization. And incorporating the findings from static subword tokenization research can only help develop better alternatives. What is Tokenization and Why Does it Exist? Language models cannot operate over raw text (or raw audio or whole images, for that matter). Text needs to be discretized into units the model knows---items in its vocabulary. Whether those units are whole words, multiple-word phrases, individual bits , pixels, audio segments, etc. the segmentation of text into discrete units is the definition of tokenization. No matter how you chunk up your input data, you\u2019re doing tokenization. Traditionally, computational linguists often segmented text into words, as words were often the atomic unit of analysis for tasks like syntactic parsing. Words are tricky, though. Linguists don\u2019t agree on any definition of wordhood . For English, a simple operationalization is any whitespace-separated orthographic unit (sometimes called orthographic words). There are certainly edge cases in English, but this will get you pretty far. Under this approach, the vocabulary is composed of orthographic words. This means that your vocabulary has to be pretty big. English has at least a million words , depending on how you count. (And, indeed word2vec , which used whitespaces to define tokens, originally included embeddings for 1.4M words). That vocabulary size isn\u2019t viable for large contemporary models, however. If your vocabulary only includes the most frequent 50,000 to 100,000 words, you will get pretty good coverage, but you will occasionally encounter an out-of-vocabulary (OOV) problem and assign an UNK (unknown) token to represent a word. Even considering only English, this kind of approach will also lead to UNKs when people make errors ( langauge vs. language ) or use non-standard spelling or capitalization ( gonna for going to ). When training on web text, this is going to come up a lot. And a word-level vocabulary will also not cover new words, which crop up often. New words can be borrowed from other languages. Many of these are food words (e.g. arrancini , bibimbap , sriracha ), but not all of them ( hygge , bokeh , emoji ). And of course, sometimes we make up new words and wordforms ( skibidi , delulu ). It is very common that new words enter the lexicon, so we want a model that can handle all of these cases. Even though NLP as a field doesn\u2019t always act like it, there are of course languages other than English. While a million English words might sound like a lot, languages with very rich morphological processes have many, many different forms for each word. Turkish, for example, may have up to one million forms of a single verb . On the other end of the spectrum, some languages like Chinese and Thai do not use whitespaces at all. So whitespace separation is not an adequate heuristic for defining a tokenizer\u2019s vocabulary. Some people have proposed using morphemes as tokens. This has the benefit of being able to create all possible inflections of a word with a reduced vocabulary. So instead of having \u2018walk\u2019, \u2018walking\u2019, \u2018walks\u2019, \u2018walked\u2019, etc. in your vocabulary, you could have entries for stems like \u2018walk\u2019 and \u2018listen\u2019 and entries for grammatical markers like \u2018ing\u2019, \u2018s\u2019, \u2018ed\u2019. You can get all possible word forms from combining the stems and the endings. This would be especially great for languages like Turkish, where many grammatical markers can be combined to get very specific word forms (hence, a million forms of a single verb). For example , the word avrupal\u0131la\u015ft\u0131ramad\u0131klar\u0131m\u0131zdan means \u2018one of those who we could not Europeanize\u2019. This word is made by combining the chunks avrupa+l\u0131+la\u015f+t\u0131r+ama+d\u0131k+lar+\u0131m\u0131z+dan. If you build up word forms based on these chunks, you don\u2019t have to have every possible (low-frequency) verb form in your vocabulary. But the limitation of this method is that you need morphological parsers for each language. Those don\u2019t exist for many languages. And this would also not leave us much flexibility in terms of misspellings, new words, or other domains like code, and we would often run into OOV errors. Instead, many people use subword tokenizers, which kind of approximate this. Subword tokenizers learn a vocabulary of a fixed size. Tokens can be individual UTF-8 bytes, Unicode characters, whole words, or pieces of words. Very frequent forms like \u2018walking\u2019 might be stored as a single token, but other words may be encoded as multiple tokens \u2018disadvantaging\u2019 \u2192 [\u2018dis\u2019, \u2018advantag\u2019, \u2018ing\u2019] or [\u2018disad\u2019, \u2018van\u2019, \u2018taging\u2019]. Even with a small vocabulary size, you\u2019ll never get an OOV issue, because the worst case is that it segments text into individual bytes or characters. Sometimes these tokens are aligned with morphological structure, but often the tokenizations are not interpretable or intuitive.  Because of this, some people think these kinds of tokens are suboptimal . Nevertheless, subword tokenization remains the dominant approach. What if we used characters instead? The benefit is that this could work for any language, provided you use the right set of characters. The problem is that there are over 150,000 Unicode characters , which would lead to a pretty large vocabulary size. And there is another problem: our sequences would be really long. The word \u2018walking\u2019 would be represented with seven character-level tokens, whereas subword tokenizers would usually tokenize this word as one or two tokens. Going even more fine-grained than characters, we could use UTF-8 bytes as our tokens. Bytes are sequences of eight bits (1s and 0s) that can be used to represent characters. This will lead to the same sequence length problem, but compounded because of the variable length used to represent characters in different languages. Languages that use non-Latin scripts will need several bytes to represent a given character. As a result, to encode the same sentence in Burmese, you need more than four times as many UTF-8 bytes relative to English . So the sequence length problem explodes when you consider languages that use non-Latin scripts. Source: ScriptBPE (Land & Arnett, 2025) All of these approaches to tokenization have their pros and cons. But some people think the only solution is to get rid of this step in language modeling altogether. \u201cTokenizer-Free\u201d Language Modeling In practice, \u201ctokenizer-free\u201d is often a stand-in for byte-level tokenization and/or dynamic tokenization. However, based on the definition of tokenization I have provided, neither of these is really \u2018tokenizer-free\u2019. Earlier language models have sometimes operated over characters ( Sutskever et al., 2011 ; Graves, 2013 ), before subword tokenizers were popularized for language models. The earliest paper I can find that uses the term \u201ctokenizer-free\u201d is this paper by Choe, Al-Rfou, et al. (2019) . In the following years, it was used to refer to character-level or byte-level tokenization, e.g. CANINE ( Clark et al., 2021 ) and Charformer ( Tay et al., 2021 ), ByT5 ( Xue et al., 2021 ), and MambaByte ( Wang et al., 2024 ). I think part of the appeal of these approaches is that their proponents feel that bytes (or characters) are a more neutral representation and are atomic units of language . By contrast, subwords feel impure or sub-optimal because they are not interpretable by humans and seem to impose design choices onto the data, which might lead to unpredictable outcomes at scale. However, I would consider both Unicode and UTF-8 to be tokenizers. Others have made this claim before. Neither of these is neutral. Every aspect of the way we encode text is a design choice. The Unicode Consortium , which is based in Silicon Valley, decides how text is represented in Unicode and UTF-8. (Though anyone can make their own alternative encoding scheme .) Furthermore, writing itself is a technology developed by people. Which writing system we use and how spelling conventions are decided are often political decisions. So, when we use characters or UTF-8 bytes as tokens, we are essentially two levels of abstraction away from any pure source of language (a continuous signal made of sounds or gestures). UTF-8 encoding and writing systems build in all kinds of biases and artifacts . Another approach that is often called \u201ctokenizer-free\u201d is dynamic tokenization. But dynamic tokenization, too, is tokenization . And, often dynamic tokenizers operate over characters or bytes as their most basic units. One of the earliest examples I can find is CharBERT , which uses CNNs to convolve over embeddings of individual characters to create an embedding for a sequence that would not incur the downsides of long sequence lengths. Similarly, MegaByte segments sequences of bytes into fixed-length patches, and then uses a local model to embed the byte sequence. Then that representation is fed into the model. Recently, a new wave of dynamic tokenization models has come about. This",
      "content_length": 3129,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.7,
      "scrape_method": "requests"
    },
    {
      "content_id": "2b3c5f56cdff0151e4ba638483f36686",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "High-Quality Datasets for Far-Field ASR (Treble Technologies x Hugging Face)",
      "url": "https://huggingface.co/blog/treble-technologies/treble10-announcement-treble-x-hugging-face",
      "author": "Unknown",
      "published_at": "2025-10-17T17:30:43.487951",
      "fetched_at": "2025-10-17T17:30:43.488046",
      "content_type": "article",
      "content_text": "Back to Articles High-Quality Datasets for Far-Field ASR (Treble Technologies x Hugging Face) Community Article Published\n\t\t\t\tOctober 13, 2025 Upvote 12 +6 Georg G\u00f6tz georg-goetz Follow treble-technologies Sarabeth Mullins ssm-treble Follow treble-technologies Daniel Gert Nielsen daniel-treble Follow treble-technologies Steven Zheng Steveeeeeeen Follow treble-technologies Eric Bezzam bezzam Follow treble-technologies Treble Technologies x Hugging Face We are thrilled to announce the start of a collaboration between Treble Technologies and Hugging Face. In connection with the collaboration, we are publishing the Treble10 dataset , which contains high fidelity room-acoustic simulations from 10 different furnished rooms: Treble10-Speech with over 3000 speech samples from the LibriSpeech test set ( test.clean & test.other ) convolved with simulated RIRs (mono, HOA or multi-channel), for benchmarking a variety of single and multi-channel tasks; far-field ASR, dereverberation, speech enhancement, etc. Treble10-RIR with the corresponding impulse responses (mono, HOA or multi-channel) for generating your own far-field content. \ud83d\udde3\ufe0f\ud83c\udfa4 Code examples can be found at the Dataset cards \ud83d\udde3\ufe0f\ud83c\udfa4 TL;DR \u2014 What Makes the Treble10 Dataset Unique \u2728 High-fidelity room acoustics at scale : Over 3000 physically accurate room impulse responses (RIRs) from 10 realistic, furnished rooms, simulated with the Treble SDK\u2019s hybrid wave-based + geometrical-acoustics approach. Bridging the realism gap : Combines the physical accuracy of measured datasets with the scalability of simulation, capturing phenomena such as diffraction, scattering, interference, and modal behavior that simpler simulations miss. 6 high-quality subsets for maximum flexibility : mono, 8th-order Ambisonics, and 6-channel device RIRs, plus matching reverberant speech versions for each. Broadband and physically grounded : All data are broadband at 32 kHz, modeling low-frequency wave effects and high-frequency reflections for realistic, full-band audio behavior. Open and ready to use : Freely available on the Treble10-Speech and Treble10-RIR , enabling straightforward integration into ASR, dereverberation, speech-enhancement, and source-separation pipelines. Introduction - The Scaleability Bottleneck for Physical Accurate Audio Data Accurate room-acoustic data is the foundation for far-field speech recognition, dereverberation, speech enhancement, or source separation. Yet most existing datasets are limited either in scale or realism. Measured corpora, such as the BUT ReverbDB or the CHIME3 challenge dataset capture acoustic conditions of the measured scenes reliably, but only coarsely cover selected areas of the rooms. For example, BUT ReverbDB contains around 1400 measured room impulse responses (RIRs) from 9 rooms, recorded with a sound source and a few dozen microphones inside spatially constrained areas. Expanding or replicating these datasets is extremely time-consuming and expensive. The CHiME datasets offer much larger amounts of noisy, reverberant speech but usually do not provide the underlying RIRs. The lack of matching RIR data limits the usability for tasks like dereverberation, where both original (aka \u201cdry\u201d) and reverberant versions of the same utterance are required. Additionally, systematic data generation and controlled ablations are not possible in such cases. The Treble10 dataset bridges this gap by combining physical accuracy with the scalability of advanced simulation. Using the Treble SDK\u2019s hybrid wave-based and geometrical-acoustics engine, we model sound propagation in 10 realistic, fully furnished rooms. In contrast to other simulation tools, which typically rely on simplified geometrical acoustics modeling, our hybrid approach models physical effects such as scattering, diffraction, interference, and the resulting modal behavior. Each room of the Treble10 dataset is densely sampled across receiver grids at multiple heights, resulting in over 3000 distinct transfer paths / RIRs per subset. The dataset includes 6 subsets: mono, 8th-order Ambisonics, and 6-channel device RIRs, along with corresponding reverberant speech signals from the LibriSpeech test set ( test.clean & test.other ). All responses are broadband (32 kHz sampling rate), accurately modeling both low-frequency wave behavior and high-frequency reflections. The Treble10 dataset The Treble10 dataset contains high fidelity room-acoustic simulations from 10 different furnished rooms (see the table below for more details). Room Volume (m\u00b3) Reverberation time (T30) , averaged over all source-receiver configurations and octave bands (s) Bathroom 1 15.42 0.58 Bathroom 2 18.42 0.77 Bedroom 1 15.6 0.43 Bedroom 2 17.65 0.22 Living room with hallway 1 38.66 0.62 Living room with hallway 2 46.08 0.62 Living room 1 40.91 0.37 Living room 2 43.16 0.87 Meeting room 1 13.83 0.38 Meeting room 2 23.97 0.19 The dataset contains six subsets: Treble10-RIR-mono : This subset contains mono RIRs. In each room, RIRs are available between 5 sound sources and several receivers. The receivers are placed along horizontal receiver grids with 0.5 m resolution at three heights (0.5 m, 1.0 m, 1.5 m). The validity of all source and receiver positions is checked to ensure that none of them intersects with the room geometry or furniture. Treble10-RIR-HOA8 : This subset contains 8th-order Ambisonics RIRs. The sound sources and receivers are identical to the RIR-mono subset. Treble10-RIR-6ch : For this subset, a 6-channel cylindrical device, see Fig. 1, is placed at the receiver positions from the RIR-mono subset. RIRs are then acquired between the 5 sound sources from above and each of the 6 device microphones. In other words, there is a 6-channel DeviceRIR for each source-receiver combination of the RIR-mono subset. The microphone coordinates along with an example of how to use the multichannel device is present in the dataset card. Treble10-Speech-mono : Each RIR from the RIR-mono subset is convolved with a speech file from the LibriSpeech test set ( test.clean & test.other ). Treble10-Speech-HOA8 : Each Ambisonics RIR from the RIR-HOA subset is convolved with a speech file from the LibriSpeech test set ( test.clean & test.other ). Treble10-Scene-6ch : Each DeviceRIR from the RIR-6ch subset is convolved with a speech file from the LibriSpeech test set ( test.clean & test.other ). Fig1: Sketch of the multi channel device included in the dataset. The device consists of 6 microphones evenly spaced with a radius of 3 cm. The coordinates of the microphones are present in the metadata for the 6ch split and also in the dataset card. All RIRs (mono/HOA/device) were simulated with the Treble SDK, and more details on the tool can be found in the dedicated section below. We use a hybrid simulation paradigm that combines a numerical wave-based solver (discontinuous Galerkin method, DGM) at low to midrange frequencies with geometrical acoustics (GA) simulations at high frequencies. For the Treble10 dataset, the transition frequency between the wave-based and the GA simulation is set at 5 kHz. The resulting hybrid RIRs are broadband signals with a 32 kHz sampling rate, thus covering the entire frequency range of the signal and containing audio content up to 16 kHz. A small subset of simulations from the same rooms has previously been released as part of the Generative Data Augmentation (GenDA) challenge at ICASSP 2025 . The Treble10 dataset differs from the GenDA dataset in three fundamental aspects: The Treble10 dataset contains broadband RIRs from a hybrid simulation paradigm (wave-based below 5 kHz, GA above 5 kHz), covering the entire frequency range of a 32 kHz signal. In contrast to the GenDA subset, which only contained the wave-based portion, the Treble10 dataset therefore more than doubles the usable frequency range. The Treble10 dataset consists of 6 subsets in total. While three of those subsets contain RIRs (mono, 8th-order Ambisonics, 6-channel device), the other three contain pre-convolved scenes in identical channel formats. The GenDA subset was limited to mono and 8th-order Ambisonics RIRs, and no pre-convolved scenes were provided. With Treble10, we publish the entire dataset, containing approximately 3100 source-receiver configurations. The GenDA subset only contained a small fraction of approximately 60 randomly selected source-receiver configurations. Why should I include room acoustics during algorithm development? Which use cases benefit from the Treble10 dataset? Let us explain this briefly with an example scenario. Consider the difference between far-field and near-field automatic speech recognition (ASR). In near-field ASR, a user speaks directly into a smartphone or headset, and the captured speech signal is relatively clean. The microphone is close to the mouth, so the direct sound dominates while reverberation and background noise are comparatively weak. In these conditions, ASR models may perform well even with limited room-acoustic diversity in the training data. However, in far-field ASR, as in smart speakers or conference-room devices, the microphone may be located several meters from the talker. The speech signal reaching the microphone is a complex mixture of direct sound, reverberation, and background noise, making the recognition task substantially more challenging. The difference between near-field and far-field conditions is not just a matter of distance, but also a matter of physics. In far-field setups, sound interacts heavily with the room: it reflects off walls, diffracts around furniture, and decays over time. RIRs comprise all of these effects and encode how sound propagates from the source to the receiver. By convolving a dry audio signal with an RIR, we can simulate reverberant speech for a specific room configuration, replicating the far-field scenario. Fig 2: The transformation of a clean audio signal into a reverberant one via convolution with a simulated Room Impulse Response in Treble is sh",
      "content_length": 2310,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.74,
      "scrape_method": "requests"
    },
    {
      "content_id": "e1f0c29a70402c03d92347aa13a87a8c",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "AI for Food Allergies",
      "url": "https://huggingface.co/blog/hugging-science/ai-for-food-allergies",
      "author": "Unknown",
      "published_at": "2025-10-17T17:30:44.250113",
      "fetched_at": "2025-10-17T17:30:44.250329",
      "content_type": "article",
      "content_text": "Back to Articles AI for Food Allergies Community Article Published\n\t\t\t\tOctober 16, 2025 Upvote 17 +11 Ludovico Comito ludocomito Follow hugging-science Antonis Vozikis Vozikis Follow hugging-science Vaibhav Pandey vaibhav2507 Follow hugging-science Kisejjere Rashid rashid0784 Follow hugging-science Let\u2019s get straight to the point: worldwide, an estimated 220 million people suffer from at least one food allergy, and in the United States alone, this accounts for roughly 10% of the population. This means that if you don\u2019t have an allergy, you\u2019ll likely know someone who does \u2014 and it\u2019s not a pleasant situation to be in. This condition affects not only patients\u2019 physical health but also takes a significant toll on their mental well-being and overall quality of life. So, what can we do about it? In recent years, biomedical research has made several remarkable advances: from experimental vaccines and desensitization-based immunotherapies to improved diagnostic tools capable of identifying specific allergen sensitivities with unprecedented precision. These developments are pointing us in the right direction toward building long-term immune tolerance, but we\u2019re not quite there yet. In the meantime, we\u2019ve also witnessed groundbreaking progress in artificial intelligence applied to biology and medicine. Models like AlphaFold and Boltz-1 have revolutionized protein structure prediction, while AI-driven approaches in genomics , drug discovery , and molecular modeling are accelerating the pace of biomedical innovation. The convergence of these worlds is opening up new possibilities for understanding, predicting, and ultimately treating complex immune conditions such as food allergies. Four among the major allergenic proteins folded by AlphaFold. Up left to bottom right: glycinin (soybean), ovalbumin (egg), alpha lactalbumin (milk), ara-h-2 (peanut). Our vision with the AI for Food Allergies project is to build the first community-driven research lab dedicated to exploring how artificial intelligence can meaningfully advance the field of food allergy research. We aim to bridge the gap between cutting-edge AI and biomedical science by developing open, collaborative projects that contribute tangible value to researchers, clinicians, and patients alike. Current State of The Art: Where AI Meets Food Allergy Research The last couple of years have been transformative for food allergy research. Artificial intelligence, once limited to image recognition or text translation, now operates comfortably in the biological and regulatory spaces that define food safety. This evolution began with early bioinformatics techniques utilized sequence alignment and physicochemical descriptors to detect and flag potential allergens. Databases such as SDAP and AllergenOnline were used to identify cross-reactive proteins. Machine-learning algorithms such as AllerHunter , and NetAllergen later enhanced these methods, training on thousands of known allergens and non-allergens to improve predictive accuracy. Today, at the molecular level , deep learning models like ProtBERT , ESM-2 , and AllergenBERT can analyze amino-acid sequences to predict whether a protein might act as an allergen. They identify subtle biochemical patterns, sequence motifs, secondary-structure signals, and epitope similarities, which correlate with immune reactions. For example, AllergenAI applies convolutional neural networks to allergen sequences from SDAP 2.0 , COMPARE , and AlgPred 2 , uncovering motifs essential for IgE binding and demonstrating the promise of integrating structural data into prediction pipelines What used to require months of lab experiments can now be screened computationally, dramatically accelerating allergen discovery in novel foods and plant-based proteins. Concurrently, AI is expanding the scope of allergy therapeutics through advances in drug-target interaction (DTI) modelling. Deep neural networks, graph neural networks and transformer models utilize data from chemogenomic datasets such as DAVIS , PDBbind to predict binding affinities, enabling virtual screening of compounds that can potentially inhibit IgE\u2013Fc\u03b5RI binding or modulate inflammatory pathways. Multimodal datasets that contain molecular structures, transcriptomics and imaging readouts can be utilized for tasks such as small molecule generation, prediction of properties and assessment of immune cell response. The subsequent subheadings present critical datasets supporting the mentioned AI approaches and explain how each resource is employed in food allergy drug design. In clinical research , AI is helping refine diagnostics. Traditionally, allergists rely on a mix of skin-prick results, serum-specific IgE levels, and patient history, but interpreting these together is difficult. Machine learning models have begun combining these modalities to estimate the true probability of a food allergy, reducing unnecessary oral food challenges and improving patient safety. Importantly, these models don\u2019t replace doctors, they simply reduce uncertainty and provide interpretable probabilities rather than binary outcomes. On the consumer and regulatory side , advances in natural language processing (NLP) and computer vision (CV) have made it possible to read and understand ingredient labels at scale. NLP models trained on multilingual data can detect hidden or misspelled allergen names (\u201ctahini\u201d \u2192 sesame, \u201cpaneer\u201d \u2192 dairy), while vision models can read curved, low-light packaging and extract ingredient text more reliably than standard OCR systems. Combined with live monitoring of FDA and USDA recall feeds, AI can now alert consumers to undeclared allergen risks in near real time. Awesome Food Allergy datasets The need for data A fundamental step in applying Machine Learning to this field is having access to high-quality data . As highlighted by Channing and Ghosh in their position paper \u201c AI for Scientific Discovery is a Social Problem \u201d , the real challenge in ML for science goes beyond advanced models and powerful GPUs. It lies in the scarcity, fragmentation, and inaccessibility of data . This issue is particularly evident in the biomedical domain, where data gatekeeping, inconsistent standards, and lack of interoperability often hinder collaboration and slows down progress. Collection release The first milestone of our community is dedicated to addressing this very challenge. We have curated Awesome Food Allergy Datasets , the first open collection of datasets on food allergies , meticulously annotated and categorized to serve as a foundation for future research. By making this resource openly accessible, we aim to accelerate discovery, foster collaboration, and lower the entry barrier for researchers and innovators interested in applying AI to this critical field. Stats about the distribution of our datasets by data type, category and public availability. We organize this resource into three complementary layers , each designed to serve a specific part of the AI-for-Food-Allergies ecosystem. \ud83e\uddec\u00a0The Protein and Molecular Allergenicity Layer At the molecular level, we are assembling what may become the most complete open dataset for allergen and protein analysis ever built. It merges classical allergen repositories with next-generation molecular and drug-target databases, enabling deep learning models to move seamlessly from sequence to structure to immune response. This layer draws from trusted allergen-focused sources such as WHO/IUIS Allergen Nomenclature Database , AllergenOnline , Allergen30 , AllerBase , AllFam , Allermatch , AllerHunter , AllerCatPro 2.0 , AllergenAI , NetAllergen , AllerTOP v1.1 , Alleropedia , Allergome , and the Allergen Family Database . These provide verified allergenic and non-allergenic protein sequences, family classifications, and cross-reactivity annotations. To capture the biochemical and structural side of allergenicity, we integrate resources like SDAP 2.0 , PDBBind+ , ProPepper , and quantum-chemistry datasets including nabla\u00b2DFT , QM , QD\u03c0 , QCML , and QCDGE . These datasets provide molecular surfaces, binding affinities, and electrostatic descriptors that help AI models learn why certain proteins interact with IgE antibodies. Because allergic response often overlaps with pharmacology, this layer also incorporates drug\u2013target and compound databases such as DAVIS , QSAR , e-Drug3D , Stanford Drug Data , DrugCentral , MedKG , Therapeutic Target Database , STITCH , Probes & Drugs , IUPHAR Pharmacology , and Enamine REAL . These enable studies of cross-reactivity between allergens and drugs, side-effects that mimic allergic reactions, and opportunities for immunomodulatory therapy. Each record in this unified dataset is annotated with sequence data, taxonomy, molecular descriptors, and literature references. We apply homology reduction to prevent data leakage between training and test sets and evaluate model quality using AUROC , AUPRC , MCC , and calibration scores . This layer serves as the foundation for building transformer-based models that predict allergenicity, drug\u2013allergen interactions, and cross-reactive epitopes. \ud83c\udfe5 The Clinical, Immunological, and Therapeutic Layer Allergies begin at the immune level, and understanding that requires human data. The second layer combines immunology, clinical, and trial datasets to help researchers model how allergic sensitization, tolerance, and treatment evolve over time. From the immunological perspective, we include the IEDB (Immune Epitope Database) and its Analysis Resource , alongside specialized datasets like AlgPred 2.0 , Allergen30 , Allergen Peptide Browser , and ProPepper , which map B- and T-cell epitopes and antibody binding regions. For studying patient-level outcomes, we integrate clinical and population datasets such as Food Anaphylaxis ML Dataset (TIP) , Food Allergy Risk Stratification Dataset , Food Allergy & Intolerance Dataset , and AllergyMap . Large-scale cohorts like HealthNuts , CHILD , and DIABIMMUNE ",
      "content_length": 6556,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.74,
      "scrape_method": "requests"
    },
    {
      "content_id": "e9abee149df92b0d81a2eae93c7deb76",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "mem-agent: Equipping LLM Agents with Memory Using RL",
      "url": "https://huggingface.co/blog/driaforall/mem-agent-blog",
      "author": "Unknown",
      "published_at": "2025-10-17T17:30:45.083505",
      "fetched_at": "2025-10-17T17:30:45.083972",
      "content_type": "article",
      "content_text": "Back to Articles mem-agent: Equipping LLM Agents with Memory Using RL Community Article Published\n\t\t\t\tOctober 9, 2025 Upvote 28 +22 Atakan Tekparmak AtakanTekparmak Follow driaforall andthattoo andthattoo Follow driaforall Atakan Tekparmak \u2014 Dria, atakan@dria.co \u00d6mer Kaya \u2014 Dria, omer@dria.co October 09, 2025 Code \u00b7 Model Abstract Large language models (LLMs) equipped with tools and a multi-turn action-feedback loop have been a popular direction of research, especially after the recent \"Cambrian explosion\" of reasoning models following the success of Deepseek-R1. With the increasing capabilities of small, open LLMs, the research in this area has become more accessible to a wider audience, leading to a wide effort in both open and closed research in LLM agents trained with reinforcement learning (RL). One area LLMs fundamentally lack is persistent state over conversations, which could be thought of in the form of a memory. This paper introduces mem-agent , a 4B LLM agent trained with GSPO on a scaffold that uses Python tools and markdown files to equip an LLM with memory. We outline a scaffold, data generation pipeline, the training process, and the application of the resulting agent. We also introduce md-memory-bench , a hand-crafted benchmark to evaluate LLMs on their proficiency in this scaffold. Using the benchmark, we demonstrate the success of our training setup as our model scores (75%) on the benchmark, second only to Qwen3-235B-A22B-Thinking-2507. 1. Introduction After the introduction of Deepseek-R1 (Guo et al., 2025), the research on LLM agents trained with RL or more specifically, Reinforcement Learning with Verifiable Rewards (RLVR) (Lambert et al., 2024) has become the de-facto standard in post-training research, eclipsing the dominance of SFT (Ouyang et al., 2022) only pipelines in post-training. Since then, many open source frameworks (Brown, 2025; Hu et al., 2024; Sheng et al., 2025; von Werra et al., 2020) have either been released or adopted the techniques used Deepseek-R1, mainly multi-turn GRPO (Shao et al., 2024) with interleaved reasoning. The adoption of GRPO and its derivatives like Dr.GRPO (Liu et al., 2025) and GSPO (Zheng et al., 2025) by both open and closed source labs and researchers set the stage for a \"Cambrian explosion\" of LLM agents trained with multi-turn RL with interleaved reasoning in the open source research scene. The biggest use-cases of these LLM agents came, to no surprise, from closed labs and their model and product offerings. State-of-the-art (SOTA) models like GPT-5 (OpenAI, 2025a), Claude 4 Opus & Sonnet (Anthropic, 2025b) are natively trained with multi-turn tool and thinking interleaved RL and have reached wide usage in applications like Claude Code (Anthropic, 2025a), Cursor and open-source options like Zed and Cline. The overall promise of these LLM agents with multi-turn tool calling, whether they are open or closed, is immense. SOTA models get better and better over time in terms of completing longer and more complex tasks (Kwa et al., 2025), and open models like the Qwen3 series (Yang et al., 2025),, GLM-4.5  (Zeng et al., 2025) and Kimi K2 (Team et al., 2025b) have equalled or surpassed the performance of their closed counterparts on many agentic tasks. With all the promise these agents might deliver, an everlasting problem with LLMs and their applications is the problem of context. Context has been an issue to deal with in language modeling long before there were transformer-based LLMs, and it is a problem of three facets: It's a hard limit as LLMs don't work past their set, maximum context length. It's also a moving, soft limit as the effective context length of LLMs are usually significantly lower than the claimed \"maximum\" context length (An et al., 2024; Li et al., 2024). Longer context requires more memory and compute, which makes all the applications of these agents more expensive. These problems make it clear that the ideal LLM agent should be context efficient. This becomes even harder when one tries to imbue the agent with extra knowledge with methods like continual pre-training (Ke et al., 2023) or SFT with on-policy samples, as they are gradient-based methods whose compute requirements scale exponentially with increasing context length. A possible remedy for this is Retrieval Augmented Generation (RAG) (Lewis et al., 2020), which sets up a (usually embedding model-based) scaffold for relevant content/document(s) to be retrieved according to the user query, and given to the LLM so it can generate a response accordingly. This method evolved into a sub-field called Agentic RAG (Singh et al., 2025), where the agent is equipped with a scaffold including tools and sometimes graph components. Overall, most of these methods are not dynamic: they give static knowledge to LLMs which cannot be deleted or modified, which can be thought of as equipping them with a read-only database. Ideally, an LLM agent would have a dynamic memory: one that can evolve over time with the usage of the agent, which would be a step forward in the direction of the ever-far promise of \"continual learning\". Motivated by the above problems and conclusions we have crafted a scaffold inspired by Obsidian, with Python tools and markdown files in folders. Based on this scaffold, we have determined three major tasks for such an agent to perform: Retrieval: Given a user query, the agent should be able to retrieve relevant content from the memory and generate a response accordingly. Update: Given a user query, the agent should be able to update the memory with the new information. Clarification: Given a user query that lacks clarity/has contradictory information, the agent should be able to ask the user for clarification. After determining the tasks, we built a graph-based data generation pipeline for the synthetic generation of training samples for the aforementioned tasks. The data generation process was followed by building the training pipeline and improving it over many experiments. Finally, we've handcrafted a new benchmark, md-memory-bench , to evaluate the performance of our trained agents and other open and closed SOTA models. Overall, our contributions are fivefold: We've crafted a scaffold for any LLM to be able to utilise a markdown-based memory, with a setup similar to dria-agent-a  (Tekparmak and andthattoo, 2025b), which itself was inspired by the ReAct framework (Yao et al., 2023), We have engineered a data-generation pipeline for the three important tasks a memory agent should be able to perform: Retrieval, Update and Clarification, We have built and improved upon a training pipeline for the training of mem-agent , and gained many insights about the training of such agents in the process, and We curated a new benchmark, md-memory-bench , to evaluate the performance of LLMs in the determined tasks. We've released mem-agent-mcp , a Model Context Protocol (MCP) (Anthropic, 2024) server for any LLM to be able to use mem-agent in the background for memory management. 2. Preliminaries 2.1 The State of Memory in LLMs To understand how we arrived at the scaffold we have in terms design choices, we must first mention the state of research in the sub-field of memory in LLMs. The most recent survey on the topic came out last year (Zhang et al., 2024). The paper goes into great detail about different formats of memory (textual and parametric), memory operations (writing, reading and management), memory sources and applications of LLM agents with memory, for example a Personal Assistant. In it, many important papers in the field are mentioned, like Voyager (Wang et al., 2023),, Generative Agents (Park et al., 2023), MemGPT (Packer et al., 2023) and Reflexion (Shinn et al., 2023). The paper, as detailed a survey as it is, does not make a distinction between declarative, procedural and episodic memory, which we deem important when discussing memory in LLM agents. In Voyager, a Minecraft-playing agent is equipped with a skill library consisting of executable JavaScript code that describes in-game actions that the agent can perform. The model can add new \"skills\" to this library and modify the ones already present, effectively giving it a procedural memory. In Generative Agents, the agent writes observations to a memory stream which are then retrieved by the model with a deterministic mechanism that takes recency, importance and relevance into account. This method equips the agent with a semi-episodic memory in a declarative format. MemGPT, which was a primary inspiration for this work, has an intricate scaffold with a queue, a working memory of sorts, and an explicit flow of memory from working memory to long-term memory. In it, the agent is given tools to add data into, modify data in and delete data from the memory system. Finally, in Reflexion, there's a 3 LLM training setup with an Actor, an Evaluator and a Self-Reflector. The Self-Reflector LLM summarizes the experience from a trajectory in a shorter text, which is then added to an \"Experience\" buffer, which serves as a long-term memory for the Actor LLM. The argument could be made that the memory systems in Generative Agents and Reflexion are more declarative than they are episodic, as they are not the raw conversation/trajectory data but the LLM generated summaries of that data. For the purposes of this paper, they will be considered declarative and we will deal with a binary classification, with only declarative and procedural memory being considered. This is mainly because true \"episodic\" memory in LLMs is the trajectory itself, which we don't want to save fully in its raw form due to aforementioned problems with increasing context length. The trajectory can also be considered as the working memory of the LLM, but that is beyond the scope of this paper. The road to the elusive Artificial General Intelligence (AGI) or for our purposes, an Artificial Generalist Agent (AGA), no doubt crosses through the research and successful",
      "content_length": 14339,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.74,
      "scrape_method": "requests"
    },
    {
      "content_id": "60b32f5baaa904e2f01d6764e9ab7232",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "Introducing the Massive Legal Embedding Benchmark (MLEB)",
      "url": "https://huggingface.co/blog/isaacus/introducing-mleb",
      "author": "Unknown",
      "published_at": "2025-10-17T00:00:00",
      "fetched_at": "2025-10-17T17:30:45.831500",
      "content_type": "article",
      "content_text": "Back to Articles Introducing the Massive Legal Embedding Benchmark (MLEB) Community Article Published\n\t\t\t\tOctober 17, 2025 Upvote 14 +8 Umar Butler umarbutler Follow isaacus Abdur-Rahman Butler abdurrahmanbutler Follow isaacus tl;dr We're announcing the release of the Massive Legal Embedding Benchmark (MLEB) , the largest, most diverse, and most comprehensive benchmark for legal text embedding models. MLEB contains 10 datasets spanning multiple document types, jurisdictions, areas of law, and tasks. To do well on MLEB, embedding models must demonstrate both extensive legal domain knowledge and strong legal reasoning skills. On MLEB, our newly released Kanon 2 Embedder legal embedding model scores highest while simultaneously maintaining the lowest inference time of all commercial competitors, highlighting the extreme accuracy and efficiency gains to be had from domain adaptation. The need for an industry-standard legal embedding benchmark In the process of training Kanon 2 Embedder , we found that the only two existing benchmarks for legal embeddings, LegalBench-RAG and the legal split of the Massive Text Embedding Benchmark (MTEB) , were either of low quality or low diversity. With regard to LegalBench-RAG, we found that it included only 4 evaluation datasets, and all datasets consisted entirely of contracts. In practice, legal professionals and users seeking legal advice or knowledge tend to search for and be interested in a much broader range of document types, including legislation, regulations, cases, and general legal literature. Additionally, the datasets were largely dominated by US contracts, reflecting the broader overrepresentation of American law in legal benchmarks and public legal datasets. In respect of the legal split of MTEB, we observed two key issues. First, we found a significant amount of mislabeling. AILA Casedocs and AILA Statutes , in particular, comprising 25% of the legal split and 50% of English data in the split, contain many query-passage pairs that are totally irrelevant to each other. Upon review of the authors' paper , we discovered the cause to be that the datasets had been created using an 'automated methodology' that paired 'facts stated in certain [Indian] Supreme Court cases' with cases and statutes that had been 'cited by the lawyers arguing those cases'. According to the authors, 'actually involving legal experts (e.g., to find relevant prior cases / statutes) would have required a significant amount of financial resources and time'. The second issue we found with the legal split of MTEB was that it lacked diversity in the areas that matter most to legal practitioners and seekers of legal knowledge. Of the remaining English-language datasets after exclusion of AILA Casedocs and AILA Statutes, two deal with consumer terms of service (Consumer Contracts QA and Legal Summarization), leaving only one (Corporate Lobbying) that deals with legislation, and none dealing with case law. All such datasets are again largely representative of American law. Regarding the non-English-language datasets in the legal split of MTEB, we argue that, in many cases, the legal systems of different cultures may fundamentally differ in ways that make cross-jurisdictional comparisons (e.g., between the common law system used by Anglosphere countries and Sharia law) of the effectiveness of legal embeddings inappropriate. Furthermore, given that the legal split contains two German datasets, one Chinese dataset, and no other non-English datasets, and that those datasets are concentrated on three select legal tasks, we argue that the inclusion of non-English datasets largely introduces bias and noise in ways that are unlikely to be conducive to real-world performance on most English-language legal information retrieval tasks. What makes MLEB an industry-standard benchmark Learning from the limitations of existing legal embedding benchmarks, we designed MLEB with four key objectives in mind, namely to: be of high quality, both in terms of provenance and labeling; consist of text processing tasks that have genuine real-world utility to legal tech professionals; be meaningfully challenging in ways likely to require significant legal knowledge and strong legal reasoning skills; and represent a broad variety of jurisdictions, legal areas, and types of legal texts. To that end, MLEB contains 10 different evaluation sets spanning a range of difficulties (including tasks requiring legal reasoning as well as tasks requiring lexical analysis), problem types (specifically, retrieval, zero-shot classification, and question answering), jurisdictions (the US, UK, Australia, Ireland, Singapore, and the EU) and document types (decisions, legislation, regulations, contracts, and literature). Of the 10 datasets in MLEB, 7 are entirely new, constructed either by having subject matter experts hand-label data or by adapting existing expert-labeled data. One of the most valuable constituents of MLEB is the Australian Tax Guidance Retrieval dataset. This dataset pairs 112 real-life tax questions posed by Australian taxpayers with 105 relevant Australian Government guidance and policy documents. We constructed this dataset by sourcing questions from the Australian Taxation Office's community forum, where Australian taxpayers ask accountants and ATO officials their tax questions. We found that, in most cases, such questions can be answered by Australian Government guidance materials that, for whatever reason, taxpayers were unable to locate themselves. Accordingly, we manually went through a stratified sample of challenging forum questions and extracted guidance materials linked to by tax experts that we confirmed to answer such questions. What makes this dataset so valuable is that, unlike the vast majority of legal information retrieval evaluation sets currently available, this dataset consists of genuine, challenging real-world user-created queries, rather than artificially constructed queries that, at times, diverge considerably from the types of tasks embedding models are actually used for. The queries are valuable and challenging precisely because users have gone to the effort of asking them on a forum, indicating that traditional search engines failed to surface the answers they were looking for. The relevant materials are, in turn, also valuable because accountants and ATO officials have confirmed them to be relevant, and we have independently affirmed their relevance. This dataset is just one of several others that we invested considerable, painstaking effort into ensuring the usefulness and quality of. Below, we present an overview of all the datasets included in MLEB alongside all the various features that make them unique. Name Document type Jurisdiction Creators Description Bar Exam QA Judicial US Stanford RegLab (Zheng et al) US bar exam questions paired with relevant caselaw. SCALR Judicial US Faiz Surani and Varun Iyer Questions presented to the US Supreme Court paired with descriptions of the Court's final holdings. Singaporean Judicial Keywords Judicial Singapore Isaacus (Umar Butler) Judicial catchwords paired with Singaporean court judgments. GDPR Holdings Retrieval Judicial EU Isaacus (Umar Butler and Abdur-Rahman Butler) GDPR case fact patterns paired with descriptions of court holdings. Australian Tax Guidance Retrieval Regulatory Australia Isaacus (Abdur-Rahman Butler and Umar Butler) Australian tax law questions paired with relevant Australian Government tax guidance and policies. Irish Legislative Summaries Regulatory Ireland Isaacus (Umar Butler) Long titles paired with Irish acts. UK Legislative Long Titles Regulatory UK Isaacus (Umar Butler) Long titles paired with UK acts. Contractual Clause Retrieval Contractual Multinational Isaacus (Umar Butler) NLI-style descriptions of types of contractual clauses paired with examples of those clauses. License TL;DR Retrieval Contractual Multinational Isaacus (Abdur-Rahman Butler) Summaries of software licenses paired with their full texts. Consumer Contracts QA Contractual Multinational Noam Kolt Questions about online terms of service paired with relevant clauses. As part of our long-standing commitment to open and accessible legal data, AI, and tech, we've ensured that MLEB and all its constituent datasets are licensed as permissively as possible. We've also publicly released our evaluation code and raw results on GitHub to assist with the consistent and reproducible evaluation of models on MLEB. How models fare on MLEB As of 16 October 2025, Kanon 2 Embedder , our newly released legal embedding model, ranks first on MLEB with an NDCG@10 score of 86%, followed by Voyage 3 Large at 85.7%. Interestingly, we find that the qualities that make an embedding model good at general multilingual information retrieval tasks are not necessarily the same as those that make a model good at legal information retrieval. Gemini Embedding ranks 1 st on MTEB and Voyage 3.5 ranks 23 rd , whereas on MLEB, Gemini is only 7 th and Voyage 3.5 is 3 rd . We observe that strong performance on MLEB seems to correlate with legal domain adaptation. Last year, Harvey announced they had partnered with Voyage to train a custom embedding model on private legal data, which may explain, in some part, why they outperform Qwen, Gemini, OpenAI, and Jina models. We note, however, that there is, unfortunately, a serious risk that Voyage's models were trained on some of the evaluation sets in MLEB, particularly SCALR and Consumer Contracts QA, which are both also part of MTEB, due to the fact that Voyage trains on their customers' private data by default (which would invariably include benchmarks). This is also a risk for Cohere and Jina models. Nevertheless, despite also being much smaller in size than Voyage 3 Large, Kanon 2 Embedder manages to punch far above its weight thanks to the enormous amounts of high-quality, licensed legal data it was trained on alongside several design improvements o",
      "content_length": 1842,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.74,
      "scrape_method": "requests"
    },
    {
      "content_id": "ce032beb948f9efa203c99005dd6a029",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "Catherine Arnett",
      "url": "https://huggingface.co/catherinearnett",
      "author": "Unknown",
      "published_at": "2025-06-12T00:00:00",
      "fetched_at": "2025-10-17T17:30:46.575171",
      "content_type": "article",
      "content_text": "4 5 8 Catherine Arnett catherinearnett Follow rakendd's profile picture thomwolf's profile picture Erland's profile picture 85\n\t\t\t\t\tfollowers \u00b7 27 following https://catherinearnett.github.io/ linguist_cat catherinearnett catherinearnett.bsky.social AI & ML interests multilingual NLP, tokenization Recent Activity commented on their article 18 days ago There is no such thing as a tokenizer-free lunch commented on their article 18 days ago There is no such thing as a tokenizer-free lunch published an article 23 days ago There is no such thing as a tokenizer-free lunch View all activity Organizations Articles 7 Article 78 There is no such thing as a tokenizer-free lunch Article 4 An Analysis of Multilingual Models on Hugging Face View all Articles Collections 2 Multilingual Leaderboards Leaderboards for languages other than English Running on CPU Upgrade 74 74 La Leaderboard \ud83c\udf38 Evaluate open LLMs in the languages of LATAM and Spain. Runtime error 123 123 Open Chinese LLM Leaderboard \ud83c\udfc6 Explore and submit LLM benchmarks Running on CPU Upgrade 167 167 Open Arabic LLM Leaderboard \ud83c\udfc6 Track, rank and evaluate open Arabic LLMs and chatbots Running 40 40 OpenLLM French leaderboard \ud83c\uddeb\ud83c\uddf7 \ud83e\udd47 Explore and submit LLM benchmarks B-GPT Bilingual GPT-2 models with checkpoints catherinearnett/B-GPT_en_nl_simultaneous Text Generation \u2022 0.1B \u2022 Updated Jun 12 \u2022 16 catherinearnett/B-GPT_nl_en_simultaneous Text Generation \u2022 0.1B \u2022 Updated Jun 12 \u2022 6 catherinearnett/B-GPT_en_nl_sequential Text Generation \u2022 0.1B \u2022 Updated Jun 12 \u2022 3 catherinearnett/B-GPT_nl_en_sequential Text Generation \u2022 0.1B \u2022 Updated Jun 12 \u2022 10 Multilingual Leaderboards Leaderboards for languages other than English Running on CPU Upgrade 74 74 La Leaderboard \ud83c\udf38 Evaluate open LLMs in the languages of LATAM and Spain. Runtime error 123 123 Open Chinese LLM Leaderboard \ud83c\udfc6 Explore and submit LLM benchmarks Running on CPU Upgrade 167 167 Open Arabic LLM Leaderboard \ud83c\udfc6 Track, rank and evaluate open Arabic LLMs and chatbots Running 40 40 OpenLLM French leaderboard \ud83c\uddeb\ud83c\uddf7 \ud83e\udd47 Explore and submit LLM benchmarks B-GPT Bilingual GPT-2 models with checkpoints catherinearnett/B-GPT_en_nl_simultaneous Text Generation \u2022 0.1B \u2022 Updated Jun 12 \u2022 16 catherinearnett/B-GPT_nl_en_simultaneous Text Generation \u2022 0.1B \u2022 Updated Jun 12 \u2022 6 catherinearnett/B-GPT_en_nl_sequential Text Generation \u2022 0.1B \u2022 Updated Jun 12 \u2022 3 catherinearnett/B-GPT_nl_en_sequential Text Generation \u2022 0.1B \u2022 Updated Jun 12 \u2022 10 Papers 10 arxiv: 2507.06378 arxiv: 2505.24689 arxiv: 2503.03962 arxiv: 2410.22587 Expand 10 papers models 18 Sort:\u00a0\n\t\tRecently updated catherinearnett/B-GPT_pl_en_sequential Text Generation \u2022 0.1B \u2022 Updated Jun 12 \u2022 3 catherinearnett/B-GPT_en_pl_sequential Text Generation \u2022 0.1B \u2022 Updated Jun 12 \u2022 1 catherinearnett/B-GPT_pl_en_simultaneous Text Generation \u2022 0.1B \u2022 Updated Jun 12 \u2022 1 catherinearnett/B-GPT_en_pl_simultaneous Text Generation \u2022 0.1B \u2022 Updated Jun 12 \u2022 26 catherinearnett/B-GPT_el_en_sequential Text Generation \u2022 0.1B \u2022 Updated Jun 12 \u2022 1 catherinearnett/B-GPT_en_el_sequential Text Generation \u2022 0.1B \u2022 Updated Jun 12 \u2022 3 catherinearnett/B-GPT_el_en_simultaneous Text Generation \u2022 0.1B \u2022 Updated Jun 12 \u2022 2 catherinearnett/B-GPT_en_el_simultaneous Text Generation \u2022 0.1B \u2022 Updated Jun 12 \u2022 8 catherinearnett/B-GPT_es_en_sequential Text Generation \u2022 0.1B \u2022 Updated Jun 12 \u2022 9 catherinearnett/B-GPT_en_es_sequential Text Generation \u2022 0.1B \u2022 Updated Jun 12 \u2022 2 View 18\n\t\t\t\t\t\t\tmodels datasets 3 Sort:\u00a0\n\t\tRecently updated catherinearnett/montok Updated 28 days ago \u2022 469 catherinearnett/eng_montok Updated Sep 11 \u2022 14 \u2022 1 catherinearnett/morphscore Viewer \u2022 Updated Jul 10 \u2022 5.09M \u2022 157 \u2022 2",
      "content_length": 524,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.605,
      "scrape_method": "requests"
    },
    {
      "content_id": "0a36b26891607d75ff45eefc930354bc",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "Yihua Zhang",
      "url": "https://huggingface.co/NormalUhr",
      "author": "Unknown",
      "published_at": "2025-10-17T17:30:47.301233",
      "fetched_at": "2025-10-17T17:30:47.301258",
      "content_type": "article",
      "content_text": "1 3 5 Yihua Zhang NormalUhr Follow YShow's profile picture Sladwell's profile picture flyingbugs's profile picture 96\n\t\t\t\t\tfollowers \u00b7 2 following https://www.yihua-zhang.com zyh2022 normaluhr zhangyihua AI & ML interests None yet Recent Activity published an article 14 days ago A Role Shift for AI Infra: From Foundational Support to a Core Engine of Innovation liked a model 18 days ago deepseek-ai/DeepSeek-V3.2-Exp commented on their article 2 months ago MLA: Redefining KV-Cache Through Low-Rank Projections and On-Demand Decompression View all activity Organizations Articles 10 Article A Role Shift for AI Infra: From Foundational Support to a Core Engine of Innovation Article Re-understanding KL Approximation from an RL-for-LLM Lens: Notes on \u201cApproximating KL Divergence\u201d View all Articles Papers 1 arxiv: 2402.11846 models 0 None public yet datasets 0 None public yet",
      "content_length": 129,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.68,
      "scrape_method": "requests"
    },
    {
      "content_id": "1846616b3d12f3cd86690c40f7b2c010",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "How I Trained Action Chunking Transformer (ACT) on SO-101: My Journey, Gotchas, and Lessons",
      "url": "https://huggingface.co/blog/sherryxychen/train-act-on-so-101",
      "author": "Unknown",
      "published_at": "2025-10-10T00:00:00",
      "fetched_at": "2025-10-17T17:30:48.137951",
      "content_type": "article",
      "content_text": "Back to Articles How I Trained Action Chunking Transformer (ACT) on SO-101: My Journey, Gotchas, and Lessons Community Article Published\n\t\t\t\tSeptember 30, 2025 Upvote 31 +25 Sherry Chen sherryxychen Follow I\u2019ve been wanting to try out the Action Chunking Transformer (ACT) on a real robot since I saw the model cooking a shrimp \ud83e\udd90. A month ago, I finally got my hands on a SO-101. After watching many videos of people training this arm to pick and place a block, I thought, how hard would it be for me to do the same? Thus began a journey filled with pitfalls and obstacles at every turn! When I look back now, many of them look pretty damn straightforward.\ud83e\udd37\ud83c\udffb\u200d\u2640\ufe0f So I figured I should share these here - you all might find them helpful. Here we go - my journey, gotchas, and lessons learned from training ACT on SO-101 to pick a block and place it in a container. Try 1: My Dear Woodpecker LeRobot\u2019s tutorial was a pretty good starting point. I grabbed two webcams and set them up on tripods. Then I clamped down the two arms - a leader and a follower. Third, I plugged all of them into a gaming desktop that my boyfriend kindly allowed me to use for science. Voila! I was ready to collect data. The plan was simple: use the leader arm to control the follower arm to pick up a block and place it in a container. I would record 50 episodes - \u201c10 episodes per location\u201d, kick off training, which would last a few hours, and boom\u2014robot butler. Data collection & training The data collection itself was\u2026 unexpectedly time-consuming. My cameras would disconnect every ~5 episodes and crash the recorder script. After some quick conversation with ChatGPT I realized it was because my two webcams were identical to each other. That confused my computer and so the camera\u2019s USB paths would be randomly changed every once in a while. I decided to push forward instead of fixing it. It took me about 2 hours (half of an afternoon) to get 50 demonstrations in the bag. Afterwards, the training script was pretty easy to set up, thanks to the training and logging pipeline provided by LeRobot. I kicked off training before going to sleep. My ACT model (52M) trained in ~4 hours on a 12GB NVIDIA RTX 3080. I woke up to find these beautiful loss curves on Weights & Biases. Can\u2019t wait to try it on my SO-101! Eval The next day, I loaded up my freshly trained policy, placed the block at one of the five positions in the training set, and hit run. Look what I\u2019ve got here! \ud83d\udc26 ... a woodpecker!! My SO-101 didn\u2019t learn to pick up the block, but rather, to peck at the table over and over again\u2026 following some vague memory of \u201capproach the block, close gripper, move to the container\u201d, while completely missing the part where it was supposed to, you know, actually grab the block. I tried moving the block closer, thinking maybe it just needed a little help. Nope! The arm continued to approach the block much closer than it should... Classic. I was disappointed. People made it look so easy in the videos! What did I do wrong? I sat down and made a list\u2026 Laundry list... of rookie mistakes The camera POV wasn\u2019t fixed! : Upon some investigation I found that my cameras had, in fact, moved a little between training and testing. As much as a little perturbation was ok, I shouldn\u2019t expect my model, trained on only 50 episodes, to be able to understand different camera POVs. Camera POV in the training set. Camera POV during eval. (Difference: Front camera is more towards the left. Right camera is closer and more to the right) Camera angle was difficult for grasping : From the right camera\u2019s POV, the gripper tips overlap with each other during grasping. This makes the right camera footage less helpful for grasping. An episode where the gripper tips overlap with each other during grasping. Arm calibration mismatch : The arm calibration was different between training and testing! I didn\u2019t question when the eval record script complained about missing arm calibration files and asked to recalibrate. It turns out that I accidentally lost my old calibration files during a power cycle as they were saved in a temporary folder! And when I redid the calibration, I didn\u2019t bring all the joints to its middle range, which messed up the homing offset of some of the joints\u2026 so even if the model learned the correct joint angles, they got mapped to incorrect joint control commands to the servo. Limited data diversity : my training set contained 10 episodes per location at 5 fixed locations, as suggested in the tutorial. This makes it easier for the model to overfit by memorizing the trajectories and thus not actually learning to pick up a block from a location not in the training set. This also brings me to my next point\u2026 No eval set during training : Back in college when I trained computer vision models, I\u2019d always set aside a small eval set aside from the training set, for monitoring model performance. This helped with detecting overfitting and selecting checkpoints. But the training script from LeRobot only does eval for policies trained in simulation. Illustration of eval and training loss curves, and areas for under / overfitting. (Source) Cheating without knowing it : I was looking over my shoulder at the follower arm while recording, getting information the robot would never have from the camera footage alone. Thanks to this video for calling me out! Plus, I later found out that this was actually also called out in the LeRobot tutorial. But wait, there\u2019s more! The logistical annoyances were piling up too: Frequent webcam disconnection : Like I mentioned earlier, my cameras would disconnect every 10min or so. This was because Ubuntu likes to play musical chairs with USB addresses when you have identical webcams. After searching around, I realized that this was a common problem (with a clean solution - keep reading!). Debugging was painful : LeRobot provides rerun for visualization during recording, and its own online dataset visualizer. But it was hard for me to visualize the time series quantities (camera frames, joint positions, time synchronization) of a recorded episode, which would be helpful for me to filter out low-quality data in the dataset. Try 2: Improved Training and Eval Pipelines Time for some serious engineering. I rolled up my sleeves and got to work. Improvement 1: Standardize the Hardware Setup Camera placement : I ditched the front-and-side setup for front-and-top. This causes less occlusion during grasping. Bonus: top camera also gets to see where the gripper tip is relative to the block (top / middle / bottom). This helps me collect higher quality demonstrations with grasps at the middle of the block (which is harder to see with the front camera) Updated camera placement: front and top. Fixed arm & camera locations : Tape and markers became my best friends. I made sure the cameras and arm were always fixed to the same spot. Fixed camera parameters : I was recording episodes from day to night, so it was important to maintain fixed camera settings so the footage color scheme was as consistent as possible. I fixed camera parameters like exposure, white balance, etc. Increase gripper friction : Probably just a nice-to-have, but I added tape on the gripper tips to increase friction, inspired by the original ACT paper . Improvement 2: Better Data Collection Setup Raw dataset recorder : I modified LeRobot\u2019s script to save a lot more raw data - raw images, synchronized timestamps, videos, metadata, etc. These data was saved alongside LeRobotDataset which was still used for training. But I could customize extra info to record so I could visualize and debug. Camera & arm udev rules : This is the solution to the frequent camera disconnection! I wrote some scripts to specify udev rules that map USB ports to some invariant property of the webcams. It turns out that they have identical serial numbers, so I ended up using the physical USB path to differentiate between them two and encoded them as udev rules. Later, I realized that this disconnection issue could also affect arms and did the same for them (luckily they have different serial numbers!) See what the robot sees. When recording demonstrations, I only looked at the camera footage instead of the follower arm. A slight improvement: for some reason, rerun would randomly hide some topic stream even when they exist, so I ended up coding up my own OpenCV visualization of the two cameras\u2019 footage. Improvement 3: Formal Task Definition In order to pump up my data diversity, I needed to start by formally defining my task space and variations. Task definition : Pick up a block and place it in a container Variations : Block types, container types, start poses Available blocks. Left to right: white, grey, green. Available containers. Left to right: tupperware, bowl, box. Reproducible episodes : With clearly defined variation dimensions, I could now specify them as an enum or float range. So now each task has its own \u201ctask_config\u201d which specifies the block and container type, as well as their start locations. I also added ruler grids to the table. Why is this important? So I can run eval on the same episode over and over again and compare between different models, as the performance difference will be truly model differences instead of eval episode setup differences. # Example task_config for an episode. task_name: \"pick_and_place_block\" variations: # All blocks align longer edge with y axis at yaw = 0. block: \"green\" container: \"tupperware\" start_pose: # [x, y, yaw_deg] block: [ 0.077 , 0.224 , 0.0 ] container: [ 0.3 , 0.2 , 0.0 ] Stratified sampling : Now that I have configurable types of variations for each episode, I can now sample more efficiently by defining different bins for the start poses of the block. For the sake of simplicity, I fixed the container location for now and focused on varying the start poses of the block. Six bins for the block start pose. Success criteria : Define \u201csuccess\u201d in different stages - inspired by the progress sco",
      "content_length": 3166,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.74,
      "scrape_method": "requests"
    },
    {
      "content_id": "9a4bdbebf324c932a53cd9b632cf6c1c",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "Xuan-Son Nguyen",
      "url": "https://huggingface.co/ngxson",
      "author": "Unknown",
      "published_at": "2025-01-23T00:00:00",
      "fetched_at": "2025-10-17T17:30:49.502932",
      "content_type": "article",
      "content_text": "view post Post 5190 A comprehensive matrix for which format should you use. Read more on my blog post: https://huggingface.co/blog/ngxson/common-ai-model-formats | Hardware | GGUF | PyTorch | Safetensors | ONNX |\n| ----------------- | ----------- | ------------------------ | -------------------------- | ------- |\n| CPU | \u2705 (best) | \ud83d\udfe1 | \ud83d\udfe1 | \u2705 |\n| GPU | \u2705 | \u2705 | \u2705 | \u2705 |\n| Mobile | \u2705 | \ud83d\udfe1 (via executorch) | \u274c | \u2705 |\n| Apple silicon | \u2705 | \ud83d\udfe1 | \u2705 (via MLX framework) | \u2705 | See translation",
      "content_length": 95,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.56,
      "scrape_method": "requests"
    },
    {
      "content_id": "679728c641907b6838ecefac86da4744",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "Isaacus",
      "url": "https://huggingface.co/isaacus",
      "author": "Unknown",
      "published_at": "2025-10-17T00:00:00",
      "fetched_at": "2025-10-17T17:30:50.330403",
      "content_type": "article",
      "content_text": "Isaacus is a legal artificial intelligence research company building AI models for the legal technology industry. Our products include Kanon 2 Embedder (the world's best legal embedding model), the Massive Legal Embedding Benchmark (MLEB) (the world's largest legal embedding benchmark), and semchunk (the world's most popular semantic chunking algorithm, used by Microsoft, IBM, and the World Bank). Our models can be accessed via our online platform or through self-hosted enterprise deployments. Follow us on LinkedIn and Reddit to say up-to-date on our latest news.",
      "content_length": 84,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.68,
      "scrape_method": "requests"
    },
    {
      "content_id": "005f3521fddea334dc46d0c71deef8c5",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "Dria",
      "url": "https://huggingface.co/driaforall",
      "author": "Unknown",
      "published_at": "2025-10-09T00:00:00",
      "fetched_at": "2025-10-17T17:30:51.150264",
      "content_type": "article",
      "content_text": "Dria is a network specialized for synthetic data generation. Models tailored for edge compute and synthetic data Edge network with specialized models & inference. See the network .\nUse our SDK to generate data. \nRun a node .",
      "content_length": 38,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.58,
      "scrape_method": "requests"
    },
    {
      "content_id": "a81f7ebbfd10f07c562479ec43ed1d0e",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "Maxime Labonne\nPRO",
      "url": "https://huggingface.co/mlabonne",
      "author": "Unknown",
      "published_at": "2025-10-14T00:00:00",
      "fetched_at": "2025-10-17T17:30:52.084587",
      "content_type": "article",
      "content_text": "view post Post 3704 LiquidAI/LFM2-8B-A1B just dropped! 8.3B params with only 1.5B active/token \ud83d\ude80 > Quality \u2248 3\u20134B dense, yet faster than Qwen3-1.7B > MoE designed to run on phones/laptops (llama.cpp / vLLM) > Pre-trained on 12T tokens \u2192 strong math/code/IF See translation",
      "content_length": 43,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.62,
      "scrape_method": "requests"
    },
    {
      "content_id": "eeb0be97c196571753f4eecfdcf5f4d9",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "Radu Florian",
      "url": "https://huggingface.co/hansolosan",
      "author": "Unknown",
      "published_at": "2025-10-17T17:30:54.283179",
      "fetched_at": "2025-10-17T17:30:54.283201",
      "content_type": "article",
      "content_text": "1 8 Radu Florian hansolosan Follow jatinganhotra's profile picture 1\n\t\t\t\t\tfollower \u00b7 1 following hansolosan AI & ML interests None yet Recent Activity published an article 3 days ago Granite Embedding R2: Setting New Standards for Enterprise Retrieval liked a model 14 days ago ibm-granite/granite-embedding-125m-english liked a model 18 days ago ibm-granite/granite-embedding-reranker-english-r2 View all activity Organizations Articles 1 Article 13 Granite Embedding R2: Setting New Standards for Enterprise Retrieval Papers 1 arxiv: 2310.13961 models 0 None public yet datasets 0 None public yet",
      "content_length": 83,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.68,
      "scrape_method": "requests"
    },
    {
      "content_id": "fa3dcc199fbf48119132efee439ee1ce",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "Small Language Models (SLM): A Comprehensive Overview",
      "url": "https://huggingface.co/blog/jjokah/small-language-model",
      "author": "Unknown",
      "published_at": "2025-02-25T00:00:00",
      "fetched_at": "2025-10-17T17:30:55.120957",
      "content_type": "article",
      "content_text": "Back to Articles Small Language Models (SLM): A Comprehensive Overview Community Article Published\n\t\t\t\tFebruary 22, 2025 Upvote 90 +84 John Johnson jjokah Follow The past few years have been a blast for artificial intelligence, with large language models (LLMs) stunning everyone with their capabilities and powering everything from chatbots to code assistants. However, not all applications demand the massive size and complexity of LLMs; the computational power required makes them impractical for many use cases. This is why Small Language Models (SLMs) entered the scene to make powerful AI models more accessible by shrinking in size. Let's go through what SLMs are, how they are made small, their benefits and limitations, real-world use cases, and how they can be used on mobile and desktop devices. What are Small Language Models? Small Language Models (SLMs) are lightweight versions of traditional language models designed to operate efficiently on resource-constrained environments such as smartphones, embedded systems, or low-power computers. While large language models have hundreds of billions\u2014or even trillions\u2014of parameters, SLMs typically range from 1 million to 10 billion parameters . The small language models are significantly smaller but they still retain core NLP capabilities like text generation, summarization, translation, and question-answering. Some practitioners don't like the term \"Small Language Model\", because a billion parameter is not small by any means. They prefer \"Small Large Language Model\", which sounds convoluted. But the majority went with Small Language Model, so SLM it is. By the way, note that it is only small in comparison with the large models. How Are They Made Small? The process of shrinking a language model involves several techniques aimed at reducing its size without compromising too much on performance: Knowledge Distillation : Training a smaller \"student\" model using knowledge transferred from a larger \"teacher\" model. Pruning : Removing redundant or less important parameters within the neural network architecture. Quantization : Reducing the precision of numerical values used in calculations (e.g. converting floating-point numbers to integers). Examples of Small Language Models Several small yet powerful language models have emerged, proving that size isn\u2019t everything. The following examples are SLMs ranging from 1-4 billion parameters: Llama3.2-1B \u2013 A Meta-developed 1-billion-parameter variant optimized for edge devices. Qwen2.5-1.5B \u2013 A model from Alibaba designed for multilingual applications with 1.5 billion parameters . DeepSeeek-R1-1.5B - DeepSeek's first-generation of reasoning model distilled from Qwen2.5 with 1.5 billion parameters . SmolLM2-1.7B \u2013 From HuggingFaceTB, a state-of-the-art \"small\" ( 1.7 billion-parameter ) language model trained on specialized open datasets (FineMath, Stack-Edu, and SmolTalk). Phi-3.5-Mini-3.8B \u2013 Microsoft's tiny-but-might open model with 3.8 billion-parameters optimized for reasoning and code generation. Gemma3-4B - Developed by Google DeepMind, this light but powerful 4 billion-parameter model is multilingual and multimodal. Here are other more powerful small language models available out there: Mistral 7B , Gemma 9B , and Phi-4 14B (though I'm not sure if Phi-4 with 14 Billion parameters still qualifies as \"small\" but it's so capable :) Benefits of Small Language Models Low Compute Requirements \u2013 Can run on consumer laptops, edge devices, and mobile phones. Lower Energy Consumption \u2013 Efficient models reduce power usage, making them environmentally friendly. Faster Inference \u2013 Smaller models generate responses quickly, ideal for real-time applications. On-Device AI \u2013 No need for an internet connection or cloud services, enhancing privacy and security. Cheaper Deployment \u2013 Lower hardware and cloud costs make AI more accessible to startups and developers. Customizability : Easily fine-tuned for domain-specific tasks (e.g., legal document analysis). Limitations of Small Language Models While SLMs offer numerous advantages, they also come with certain trade-offs: Narrow Scope : Limited generalization outside their training domain (e.g., a medical SLM struggles with coding). Bias Risks : Smaller datasets may amplify biases if not carefully curated. Reduced Complexity : Smaller models may struggle with highly nuanced or complex tasks that require deep contextual understanding. Less Robustness : They are more prone to errors in ambiguous scenarios or when faced with adversarial inputs. Real-World Applications of Small Language Models Despite their limitations, SLMs have a broad range of practical applications: Chatbots & Virtual Assistants : Efficient enough to run on mobile devices while providing real-time interaction. Code Generation : Models like Phi-3.5 Mini assist developers in writing and debugging code. Language Translation : Lightweight models can provide on-device translation for travelers. Summarization & Content Generation : Businesses use SLMs for generating marketing copy, social media posts, and reports. Healthcare Applications : On-device AI for symptom checking and medical research. IoT & Edge Computing : Running AI on smart home devices without cloud dependency. Educational Tools : Tutoring systems can utilize SLMs to generate personalized explanations, quizzes, and feedback in real-time. Running Small Language Models on Edge Devices SLMs bring AI power directly to your smartphone (using PockPal) or PC (using Ollama), offering offline access, enhanced privacy, and lower latency. SLMs on Mobile Device with PocketPal For users interested in experiencing SLMs firsthand, the PocketPal AI app offers an intuitive way to interact with these models directly on your smartphone, without the need for an internet connection. Whether you want to draft emails, brainstorm ideas, or get answers to quick questions, PocketPal provides a seamless interface powered by optimized SLMs. Its offline capabilities ensure your queries remain private. Features Offline AI Assistance: Run language models directly on your device without internet connectivity. Model Flexibility: Download and swap between multiple SLMs - like Phi, Gemma, Qwen & others. Auto Offload/Load: Automatically manage memory by offloading models when the app is in the background. Inference Settings: Customize model parameters like system prompt, temperature, BOS token, and chat templates. Real-Time Performance Metrics: View tokens per second and milliseconds per token during AI response generation. Download PocketPal AI on iOS and Android Running SLMs on PC  with Ollama Ollama, an open-source tool, simplifies SLM deployment on PCs: Local Management : Run models like Llama3.2-1B or Phi-3.5 Mini with minimal setup. GPU Optimization : Leverages consumer-grade GPUs for faster inference. Custom Workflows : Integrate SLMs into data pipelines or creative tools (e.g., automated code reviews). Getting Started with Ollama: Install Ollama from ollama.com Open the terminal and download a model: ollama pull qwen2.5:1.5b Run the model interactively: ollama run qwen2.5:1.5b This setup enables local AI-powered chatbots, coding assistants, and document summarization without needing cloud services. Fine-Tuning Small Language Models One of the most exciting aspects of SLMs is their adaptability through fine-tuning. By exposing an SLM to domain-specific datasets, you can enhance its performance for niche applications. For instance: Fine-tune a model on legal documents to create a contract analysis assistant. Train an SLM on technical manuals to build a troubleshooting guide for engineers. There are several ways to fine-tune an SLM: Full Fine-Tuning \u2013 Retraining all parameters with new data (requires significant compute). LoRA (Low-Rank Adaptation) \u2013 Fine-tunes only a few layers, making it lightweight and efficient. Adapters & Prompt Tuning \u2013 Adds extra layers or optimizes prompts to guide model responses. Example: Fine-Tuning with LoRA\nUsing Hugging Face\u2019s peft library: from peft import LoraConfig, get_peft_model from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"gemma-2-2b\" model = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nconfig = LoraConfig(r= 8 , lora_alpha= 16 , lora_dropout= 0.1 )\nmodel = get_peft_model(model, config) # Train the model on new data... Fine-tuning not only improves accuracy but also ensures the model aligns closely with your unique requirements. Conclusion Small Language Models (SLMs) represent a crucial step toward efficient, accessible, and cost-effective AI. They provide practical solutions for businesses, developers, and researchers looking for powerful AI without the heavy computational burden of LLMs. With tools like Ollama for PCs and fine-tuning options for customization, SLMs are reshaping the AI landscape\u2014making AI more personal, private, and available to everyone. Let's discover how compact AI can transform our projects. Ref: A Survey of Small Language Models (Research Paper) https://arxiv.org/abs/2410.20011",
      "content_length": 1293,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.6199999999999999,
      "scrape_method": "requests"
    },
    {
      "content_id": "0d317d0abec4628b7333c2a06e1e1106",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "NVIDIA",
      "url": "https://huggingface.co/nvidia",
      "author": "Unknown",
      "published_at": "2025-10-13T00:00:00",
      "fetched_at": "2025-10-17T17:30:56.473751",
      "content_type": "article",
      "content_text": "NVIDIA Enterprise + company Verified https://www.nvidia.com/ nvidia Activity Feed Follow 40,036 AI & ML interests None defined yet. Recent Activity TheoViel updated a collection about 1 hour ago Nemotron RAG TheoViel updated a collection about 1 hour ago Nemotron RAG ravirajoshi updated a dataset about 10 hours ago nvidia/ChatRAG-Hi View all activity Papers VLA-0: Building State-of-the-Art VLAs with Zero Modification QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning\n  for LLMs View all Papers Articles Nemotron-Personas-India: Synthesized Data for Sovereign AI 4 days ago \u2022 9 Nemotron-Personas-Japan: \u30bd\u30d6\u30ea\u30f3 AI \u306e\u305f\u3081\u306e\u5408\u6210\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8 21 days ago \u2022 7 Nemotron-Personas-Japan: Synthesized Data for Sovereign AI 24 days ago \u2022 25 NVIDIA Releases 6 Million Multi-Lingual Reasoning Dataset Aug 20 \u2022 18 Supercharge Edge AI With High\u2011Accuracy Reasoning Using NVIDIA Nemotron Nano  2 9B Aug 18 \u2022 29 \ud83d\udce2 NVIDIA Releases Nemotron-CC-Math Pre-Training Dataset: A High-Quality, Web-Scale Math Corpus for Pretraining Large Language Models Aug 18 \u2022 5 NVIDIA Releases Improved Pretraining Dataset: Preserves High Value Math & Code, and Augments with Multi-Lingual Aug 18 \u2022 3 NVIDIA Releases 3 Million Sample Dataset for OCR, Visual Question Answering, and Captioning Tasks Aug 11 \u2022 73 Measuring Open-Source Llama Nemotron Models on DeepResearch Bench Aug 4 \u2022 5 Accelerate a World of LLMs on Hugging Face with NVIDIA NIM Jul 21 \u2022 5 OpenReasoning-Nemotron: A Family of State-of-the-Art Distilled Reasoning Models Jul 18 \u2022 49 Llama-NeMoRetriever-ColEmbed: Developer-Focused Guide to NVIDIA's State-of-the-Art Text-Image Retrieval Jul 9 \u2022 4 Welcome the NVIDIA Llama Nemotron Nano VLM to Hugging Face Hub Jun 27 \u2022 28 Introducing Cosmos Predict-2: A Foundation For Your Own World Model Jun 17 \u2022 9 Post-Training Isaac GR00T N1.5 for LeRobot SO-101 Arm Jun 11 \u2022 97 Supercharge Edge AI with High Accuracy Reasoning Using Llama Nemotron Nano 4B Jun 10 \u2022 7 Nemotron-Personas: Improve AI Training With the First Synthetic Personas Dataset Aligned to Real-World Distributions Jun 10 \u2022 18 Explore, Build, and Innovate AI Reasoning with NVIDIA\u2019s Open Models and Recipes Jun 4 \u2022 21 NVIDIA's GTC 2025 Announcement for Physical AI Developers: New Open Models and Datasets Mar 18 \u2022 42 Mastering Long Contexts in LLMs with KVPress Jan 23 \u2022 70 Controlling Language Model Generation with NVIDIA's LogitsProcessorZoo Dec 23, 2024 \u2022 49 Team members 3,038 Collections 63 Nemotron RAG nvidia/omni-embed-nemotron-3b Feature Extraction \u2022 5B \u2022 Updated 8 days ago \u2022 106 \u2022 27 NVIDIA Nemotron Open, Production-ready Enterprise Models. Nvidia Open Model license. nvidia/NVIDIA-Nemotron-Nano-12B-v2 Text Generation \u2022 12B \u2022 Updated 2 days ago \u2022 110k \u2022 99 nvidia/NVIDIA-Nemotron-Nano-9B-v2 Text Generation \u2022 9B \u2022 Updated 2 days ago \u2022 299k \u2022 408 nvidia/NVIDIA-Nemotron-Nano-9B-v2-Base Text Generation \u2022 9B \u2022 Updated 2 days ago \u2022 22.9k \u2022 38 nvidia/NVIDIA-Nemotron-Nano-12B-v2-Base Text Generation \u2022 12B \u2022 Updated 2 days ago \u2022 6.6k \u2022 76 Nemotron RAG nvidia/omni-embed-nemotron-3b Feature Extraction \u2022 5B \u2022 Updated 8 days ago \u2022 106 \u2022 27 NVIDIA Nemotron Open, Production-ready Enterprise Models. Nvidia Open Model license. nvidia/NVIDIA-Nemotron-Nano-12B-v2 Text Generation \u2022 12B \u2022 Updated 2 days ago \u2022 110k \u2022 99 nvidia/NVIDIA-Nemotron-Nano-9B-v2 Text Generation \u2022 9B \u2022 Updated 2 days ago \u2022 299k \u2022 408 nvidia/NVIDIA-Nemotron-Nano-9B-v2-Base Text Generation \u2022 9B \u2022 Updated 2 days ago \u2022 22.9k \u2022 38 nvidia/NVIDIA-Nemotron-Nano-12B-v2-Base Text Generation \u2022 12B \u2022 Updated 2 days ago \u2022 6.6k \u2022 76 View 63 collections spaces 35 Sort:\u00a0\n\t\tRecently updated pinned Running on CPU Upgrade 1 Judge's Verdict Leaderboard \u2696 Judge's Verdict: Benchmarking LLM as a Judge nvidia 8 days ago pinned Running 42 KVPress Leaderboard \ud83e\udd47 KVPress leaderboard: benchmark KV Cache compression methods nvidia 15 days ago pinned Runtime error 69 Audio Flamingo 3 Demo \ud83d\ude80 Audio Flamingo 3 Demo nvidia Jul 21 pinned Running 20 Llm Robustness Leaderboard \ud83e\udd47 LLM Robustness leaderboard nvidia Apr 7 Running 4 Voice Agent WebRTC + LangGraph \ud83c\udf99 Voice agent with LangGraph, WebRTC, ASR & TTS nvidia 2 days ago Paused 2 DoMINO with Ahmed Body Dataset - Multi-Scale Neural Operator for CFD \ud83d\udfe2 Access JupyterLab for interactive coding nvidia 10 days ago View 35\n\t\t\t\t\t\t\tSpaces models 521 Sort:\u00a0\n\t\tRecently updated nvidia/DLER-Llama-Nemotron-8B-Merge-Research 8B \u2022 Updated about 22 hours ago \u2022 50 nvidia/diar_streaming_sortformer_4spk-v2 Audio Classification \u2022 Updated 1 day ago \u2022 14.4k \u2022 59 nvidia/Cosmos-Transfer2.5-2B Updated 1 day ago \u2022 27.4k \u2022 9 nvidia/Cosmos-Predict2.5-2B Updated 1 day ago \u2022 409k \u2022 14 nvidia/NVIDIA-Nemotron-Nano-9B-v2-NVFP4 Text Generation \u2022 6B \u2022 Updated 2 days ago \u2022 185 \u2022 2 nvidia/NVIDIA-Nemotron-Nano-9B-v2-FP8 Text Generation \u2022 9B \u2022 Updated 2 days ago \u2022 1.88k \u2022 2 nvidia/Llama-3_3-Nemotron-Super-49B-v1_5 Text Generation \u2022 50B \u2022 Updated 2 days ago \u2022 166k \u2022 205 nvidia/Llama-3_3-Nemotron-Super-49B-v1_5-FP8 Text Generation \u2022 50B \u2022 Updated 2 days ago \u2022 2.8k \u2022 19 nvidia/Llama-3_1-Nemotron-Ultra-253B-v1 Text Generation \u2022 253B \u2022 Updated 2 days ago \u2022 2.45k \u2022 \u2022 336 nvidia/Llama-3_1-Nemotron-Ultra-253B-v1-FP8 Text Generation \u2022 253B \u2022 Updated 2 days ago \u2022 509 \u2022 8 View 521\n\t\t\t\t\t\t\tmodels datasets 111 Sort:\u00a0\n\t\tRecently updated nvidia/ChatRAG-Hi Viewer \u2022 Updated about 10 hours ago \u2022 3.95k \u2022 69 \u2022 1 nvidia/PhysicalAI-Autonomous-Vehicles-NuRec Viewer \u2022 Updated about 22 hours ago \u2022 1.03k \u2022 1.64k \u2022 25 nvidia/PhysicalAI-Robotics-mindmap-GR1-Stick-in-Bin Preview \u2022 Updated 2 days ago \u2022 43 nvidia/PhysicalAI-Robotics-mindmap-GR1-Drill-in-Box Viewer \u2022 Updated 2 days ago \u2022 2.58k \u2022 54 nvidia/PhysicalAI-Robotics-mindmap-Franka-Mug-in-Drawer Viewer \u2022 Updated 2 days ago \u2022 1.78k \u2022 65 nvidia/PhysicalAI-Robotics-mindmap-Franka-Cube-Stacking Viewer \u2022 Updated 2 days ago \u2022 2.54k \u2022 93 nvidia/Nemotron-Personas-India Viewer \u2022 Updated 3 days ago \u2022 3M \u2022 1.33k \u2022 21 nvidia/PhysicalAI-SmartSpaces Updated 5 days ago \u2022 23.5k \u2022 51 nvidia/IFEval-Hi Viewer \u2022 Updated 7 days ago \u2022 848 \u2022 32 nvidia/MT-Bench-Hi Viewer \u2022 Updated 7 days ago \u2022 200 \u2022 37 View 111\n\t\t\t\t\t\t\tdatasets",
      "content_length": 915,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.725,
      "scrape_method": "requests"
    },
    {
      "content_id": "e4cac4046072878a30f51711af2a5e9b",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "Everything You Need to Know about Knowledge Distillation",
      "url": "https://huggingface.co/blog/Kseniase/kd",
      "author": "Unknown",
      "published_at": "2025-10-17T17:30:57.202777",
      "fetched_at": "2025-10-17T17:30:57.202925",
      "content_type": "article",
      "content_text": "Back to Articles Everything You Need to Know about Knowledge Distillation Community Article Published\n\t\t\t\tMarch 6, 2025 Upvote 46 +40 Ksenia Se Kseniase Follow Alyona Vert alyona0l Follow \ud83d\udd33 This is one of the hottest topics thanks to DeepSeek. Learn with us: the core idea, its types, scaling laws, real-world cases and useful resources to dive deeper In the previous episode, we discussed Hugging Face\u2019s \u201cSmol\u201d family of models and their effective strategy for training small LMs through high-quality dataset mixing. Today we want to go further in exploring training techniques for smaller models, making it the perfect time to discuss knowledge distillation (KD) . Proposed a decade ago, this method has continued to evolve. For example, DeepSeek\u2019s advancements, particularly the effective distillation of DeepSeek-R1, have recently brought a wave of attention to this approach. So, what is the key idea behind knowledge distillation? It enables to transfer knowledge from larger model, called teacher, to smaller one, called student. This process allows smaller models to inherit the strong capabilities of larger ones, avoiding the need for training from scratch and making powerful models more accessible. Let\u2019s explore how knowledge distillation has evolved over time, the different types of distillation that exist today, the key factors to consider for effective model distillation, and useful resources to master it. \ud83d\udce8 Click follow! If you want to receive our articles straight to your inbox, please subscribe here In today\u2019s episode, we will cover: When did knowledge distillation appear as a technique? A detailed explanation of knowledge distillation Types of knowledge distillation Improved algorithms Distillation scaling laws Benefits Not without limitations Real-world effective use cases (why OpenAI got mad at DeepSeek) Conclusion Sources and further reading When did knowledge distillation appear as a technique? The ideas behind knowledge distillation (KD) date back to 2006, when Bucil\u0103, Caruana, and Niculescu-Mizil in their work \u201cModel Compression\u201d showed that an ensemble of models could be compressed into a single smaller model without much loss in accuracy. They demonstrated that a cumbersome model (like an ensemble) could be effectively replaced by a lean model that was easier to deploy. Later in 2015, Geoffrey Hinton, Oriol Vinyals, and Jeff Dean coined the term \u201cdistillation\u201d in their \u201cDistilling the Knowledge in a Neural Network\u201d paper. This term was referred to the process of transferring knowledge from a large, complex AI model or ensemble to a smaller, faster AI model, called the distilled model. Instead of just training the smaller model on correct answers, researchers proposed to give it the probability distribution from the large model. This helps the smaller model learn not just what the right answer is, but also how confident the big model is about each option. This training concept is closely connected to the softmax function, so let's explore more precisely how this all works at the core. Image Credit: \u201cKnowledge Distillation: A Survey\u201d paper A detailed explanation of knowledge distillation Firstly, we need to clarify what is softmax . It is a mathematical function used in machine learning, especially in neural networks, to convert raw scores, called logits, into probabilities . It helps a model decide which category, or class, an input belongs to by ensuring that the output values sum to 1, making them interpretable as probabilities. The important parameter in softmax is a temperature (T) \u2014 it is a way to control how confident or uncertain a model\u2019s predictions are. It adjusts the sharpness of the probability distribution \u2013 making it either more confident (sharp) or more uncertain (soft). If T = 1, it is the default setting referring to the normal softmax behavior, where only the correct answer receives 100% probability. In this case, softmax creates hard targets . When the temperature is increased (T > 1), softmax creates soft targets , meaning the probabilities are more spread out or softer. Soft targets are useful for distillation and training, and the knowledge distillation process below shows why. It typically involves several steps: First, the teacher model is trained on the original task and dataset. Next, the teacher model produces logits. These logits are converted into soft targets using a softmax function with a higher temperature to make the probability distribution softer. The student model is then trained on these soft targets often alongside the hard targets (true labels) by minimizing the difference between the student\u2019s output distribution and the teacher\u2019s output distribution. Image Credit: \u201cKnowledge Distillation: A Survey\u201d paper During this process the student learns to reproduce not just the correct answers, but also the teacher\u2019s relative confidence in those answers and its mistakes. This knowledge about how the teacher distributes probability mass among the incorrect categories provides rich information that helps the student generalize better. By combining a standard training loss on true labels with a distillation loss on the teacher\u2019s soft labels, the student can achieve accuracy close to the teacher model\u2019s accuracy, despite having far fewer parameters. If we were to summarize the key idea in one sentence, it would be this: The student is optimized to mimic the teacher\u2019s behavior, not just outputs. There is another approach to distillation proposed in the \u201cDistilling the Knowledge in a Neural Network\u201d paper, and it is matching logits . Instead of just copying probabilities, this method directly makes the small model's logits resemble the large model\u2019s logits. In high-temperature settings, this method becomes mathematically equivalent to standard distillation, and both approaches lead to similar benefits. Types of knowledge distillation What we have explored are just two options for knowledge distillation, but KD can be applied in various ways depending on what knowledge is transferred from teacher to student. These types of distillation were perfectly illustrated in the paper * \u201cKnowledge Distillation: A Survey\u201d by researchers from the University of Sydney and the University of London. So common techniques include: Image Credit: \u201cKnowledge Distillation: A Survey\u201d paper Response-based distillation (outputs as knowledge): It is exactly what we have discussed above. This classic approach uses the teacher\u2019s final output probabilities as the target for training the student. It works well in different AI tasks, like image classification, object detection, and pose estimation. Feature-based distillation (intermediate layers as knowledge): Instead of only copying the final predictions, the student also learns from the intermediate layers, or feature maps, of the teacher. This idea was proposed in the \u201cFitNets: Hints for Thin Deep Nets\u201d paper. Think of this technique as the student learning from the teacher's step-by-step problem-solving process. The student matches its intermediate representations to those of the teacher, learning both the right answers and the reasoning behind them. Different methods, such as attention maps, probability distributions, and layer connections, help match teacher and student features. It especially improves performance in tasks like image recognition and object detection. Image Credit: \u201cKnowledge Distillation: A Survey\u201d paper Relation-based distillation (relationships as knowledge): The student model learns to mimic relationships between different parts of the teacher model \u2013 either between layers or between different data samples. For example, the students compares multiple samples and learns their similarities. This method is more complex but can work with multiple teacher models, merging their knowledge. Image Credit: \u201cKnowledge Distillation: A Survey\u201d paper It is a classification of the three concepts used for knowledge distillation based on what to distill. But how exactly can we transfer the knowledge during training? There are three main ways: Offline distillation \u2013 The teacher is trained first, then teaches the student. Online distillation \u2013 The teacher and student learn together. Self-distillation \u2013 The student learns from itself. Here is what you need to know about these training schemes: But that\u2019s not all. Since the emergence of the knowledge distillation concept, many different approaches have been developed to improve knowledge transfer. Improved algorithms Different studies proposed knowledge distillation algorithms to make it more effective, in sometimes even applying other techniques. Here are some of them: Multi-teacher distillation: Combines knowledge from multiple teachers for a more well-rounded student. This approach can use different teachers for different features, average predictions from all teachers or randomly select a teacher in each training step. It combines strengths from various models and improves diversity in knowledge. Cross-modal distillation: Transfers knowledge between different types of data, for example, from image to text or from audio to video. Graph-based distillation: Focuses on relationships between different data points to captures hidden relationships and better understand data structure. Attention-based distillation: The teacher model generates attention maps to highlight important areas in the data, and the student copies these attention maps, learning where to focus. Non-Target Class-Enhanced KD (NTCE-KD): Focuses on the often-ignored probabilities of non-target classes in the teacher\u2019s output, so the student model learns incorrect labels as well. Adversarial distillation: Uses Generative Adversarial Networks (GANs) to help the student improve by mimicking the teacher. GANs can generate extra synthetic data to improve student training or use their discriminator, which checks if data is real or fake, to compare teacher and student outputs. Data-free distillation: Works without needing the original dataset. ",
      "content_length": 3479,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.74,
      "scrape_method": "requests"
    },
    {
      "content_id": "c79644f9ed8231c0db6117a7e534dc34",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "Mike Ravkine\nPRO",
      "url": "https://huggingface.co/mike-ravkine",
      "author": "Unknown",
      "published_at": "2025-05-06T00:00:00",
      "fetched_at": "2025-10-17T17:30:58.041750",
      "content_type": "article",
      "content_text": "view post Post 684 There are two very interesting reasoning models from ServiceNow-AI that I think are flying under everyone's radar - lets take a closer look at ServiceNow-AI/Apriel-1.5-15b-Thinker (#10 on the ReasonScape rankings) and ServiceNow-AI/Apriel-Nemotron-15b-Thinker (landing just below its brother at #12). A rather interesting attribute of these models is I have absolutely no idea what they are fine-tuned from, other then some kind of pre-small Mistrals!  The non-nemo 15b looks like Mistral Pixtral 12B, but with 8 more layers while the nemo 15b analogously looks like Mistral NeMo 12B but with 10 more layers and a smaller max context length. The performance trade-offs between these two models are quite clear: the Nemotron provides ~30% shorter answers but at the expense of totally collapsing under difficulty on 4 of the 12 tasks ... which all just happen to have \"Math\" in common, so it's pretty easy to point the finger at exactly what the price for the lower reasoning token usage is here. In principle ServiceNow-AI/Apriel-1.5-15b-Thinker is multimodal and should be able to reason about image queries but this is not something I have tried as ReasonScape is not currently able to evaluate VLMs - perhaps a future improvement. See translation",
      "content_length": 202,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.5900000000000001,
      "scrape_method": "requests"
    },
    {
      "content_id": "22918da75cc61ff29292637f9cb9b054",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "Hugging Face",
      "url": "https://huggingface.co/huggingface",
      "author": "Unknown",
      "published_at": "2025-10-16T00:00:00",
      "fetched_at": "2025-10-17T17:30:59.181513",
      "content_type": "article",
      "content_text": "\ud83d\udc4b Hi! We are on a mission to democratize good machine learning, one commit at a time. If that sounds like something you should be doing, why don't you join us ! For press enquiries, you can \u2709\ufe0f contact our team here .",
      "content_length": 43,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.62,
      "scrape_method": "requests"
    },
    {
      "content_id": "ba5299f0ce9577befc2d612cfe76c9fa",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "John Johnson",
      "url": "https://huggingface.co/jjokah",
      "author": "Unknown",
      "published_at": "2025-02-04T00:00:00",
      "fetched_at": "2025-10-17T17:31:00.017355",
      "content_type": "article",
      "content_text": "view post Post 314 The combination of Gemini Nano (Google's AI model) and the Tensor G5 chip (Google's AI processor), built into the Pixel 10 (Google's Smartphone), provides Google with a strong foundation to continue pushing the limits of edge AI \u2192 \ud83d\udd2eMagic Cue. Magic Cue digs through your device (Gmail, Calendar, Messages, Photos, screenshots, notes, and more) to surface what\u2019s useful at that moment. Ref (Magic Cue): https://store.google.com/intl/en/ideas/articles/magic-cue/ See translation",
      "content_length": 71,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.56,
      "scrape_method": "requests"
    },
    {
      "content_id": "7e2e59238caf86e99b9486b1fff56c70",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "From Benchmarks to Cognitive Landscapes: Building ReasonScape",
      "url": "https://huggingface.co/blog/mike-ravkine/building-reasonscape",
      "author": "Unknown",
      "published_at": "2025-10-13T00:00:00",
      "fetched_at": "2025-10-17T17:31:00.756654",
      "content_type": "article",
      "content_text": "Back to Articles From Benchmarks to Cognitive Landscapes: Building ReasonScape Community Article Published\n\t\t\t\tOctober 13, 2025 Upvote 4 Mike Ravkine mike-ravkine Follow When I last wrote, I was neck-deep in the beautiful chaos of trying to make AI models think efficiently. I had Ruminate steering reasoning processes, excess accuracy corrections separating signal from noise, and a power bill that was starting to develop opinions about my life choices. That was six months ago. Since then, what started as \"Can AI Think?\" has evolved into something I never quite intended to build: a complete framework for understanding how language models process information at a fundamental level. The Problem with Benchmarks (Again) Remember how my coding tests got contaminated? Turns out the reasoning tests had an even more insidious problem: the format itself was lying to me. Every benchmark I'd seen treated models like magic boxes: prompt goes in, answer comes out, we count the score. But models aren't magic boxes - they're information processing systems . They have architectures, they have cognitive patterns, they have input processing, they have systematic ways they succeed and fail. I wasn't just measuring the wrong thing - I was measuring things wrong . Enter the Third Dimension Traditional benchmarks give you a number: \"This model scored 73.2% on task X.\" Cool. What does that actually mean ? Is the model consistently mediocre at everything? Does it ace easy cases and crater on hard ones? Does adding one more item to a list cause a graceful degradation or a catastrophic collapse? You can't tell. Because that 73.2% is averaging over every possible variation of the problem, smoothing out all the interesting cognitive architecture details into a single meaningless number. So I did something weird: I made every test parametric. Instead of \"can you sort a list?\" I ask \"can you sort a list of length N with case mutations C?\" Instead of \"track these state changes,\" it's \"track D state changes across L objects with I interference instructions.\" Every test became a function across multiple difficulty axes, generating deterministic test cases from coordinate-based seeds. Now instead of a single score, I get a difficulty landscape - a 3D surface showing exactly how performance changes as you vary the parameters. And these surfaces? They're beautiful . And by beautiful, I mean they reveal cognitive architecture patterns that are completely invisible to traditional benchmarks. There's quite a lot going on in these plots, which can be made at-will via the Explorer .  It's like going from measuring \"how tall is this mountain?\" to having a full topographic map - you can suddenly see the terrain unfolding in front of you. Truncations (where the model failed to complete its answer within 8K token thinking budget) are indicated by the red stalagtites. The \"Competence Zone\" (region of difficulty where the model successfully performs the task) is indicated by the green spheres. Some models have sharp cliffs where adding one more nested parenthesis causes accuracy to fall off a ledge. Others show smooth degradation curves. Some excel at short, deep problems. Others prefer long, shallow ones. Some models are agnostic to boolean format representation, while others show clear preferences. The Spectral Analysis Plot Twist Here's where things got really weird. I'm generating millions of test cases, and I need to verify my generators aren't introducing systematic biases. Standard practice is to check statistical properties of the outputs - means, distributions, that kind of thing. But these are reasoning tasks . The content matters more than simple statistics. How can we be sure thousands of tests we are generating are really 1) the same when they should be the same, 2) more difficult when they should be more difficult? Then I remembered: problems are just token sequences! And token sequences are just signals, right? Signals can be analyzed in the frequency domain! What if I just... ran an FFT on the post-template, post-tokenized streams? It worked absurdly well. Different types of reasoning problems have different spectral signatures. Mathematical expressions show high-frequency components from repeated operator symbols, but compression from whitespace confounding. Natural language problems have characteristic low-frequency patterns from grammatical structure that get amplified as the problem depth growth. Even better: different model architectures all demonstrate unique spectral characteristics in their representations of otherwise identical problems.  The interation of chat template and tokenization caused architectural differences in the frequency domain which manifests as the positions of peaks, the presense of nulls, the response to whitespace randomization and a host of other factors across the test suite. Cognitive architecture analysis through signal processing was unplanned, it emerged from following the data. The Statistical Rigor Rabbit Hole With parametric tests generating thousands of samples across multiple difficulty dimensions, statistical rigor stopped being optional and became existential. I needed: Excess accuracy correction that works across binary, multiple-choice, and write-in questions Wilson confidence intervals with truncation awareness (models that max out at 100% or bottom out at random guessing) Dynamic sampling that keeps generating tests until hitting significance thresholds or safety limits Hierarchical sampling where smaller samples are perfect subsets of larger ones (for efficient scaling) The last one is particularly clever: every test is generated from coordinates in the difficulty manifold. Want more samples? Just extend the sequence. Want to downsample? Take the first N cases. Need to compare a 1000-sample run from last month with today's 5000-sample run? They share the first 1000 tests exactly. This means I can cache responses forever, merge datasets across time, and never waste compute re-running identical tests. In a world where my reasoning models are burning through 5000+ tokens per test case, this cache-everything approach has saved me approximately [checks power bill] one entire RTX 4090's worth of electricity costs. The methodology documentation covers these points in more detail. M12X: The Evaluation That Emerged What started as \"test some reasoning tasks\" metastasized into M12X : twelve cognitive domains, three difficulty degrees, configurable sampling densities, and adjustable precision levels. The domains span the cognitive reasoning waterfront : Math and logic (arithmetic, boolean) Language and selective attention (objects, letters, movies) Spatial and temporal reasoning (shapes, dates, cars) Structural parsing (brackets, sort, sequence) State tracking and planning (shuffle, cars) Each domain has carefully designed parametric generators that create difficulty manifolds - continuous surfaces where you can smoothly vary the challenge level and watch exactly where and how models break. The flexibility is what makes it powerful: You can run a quick corner-sampling pass to find the interesting regions, then do a high-density sweep of just those areas. Or run low-precision scans across everything, then boost precision only on domains or difficulty dimensions that show statistical uncertainty. It's not so much an evaluation system as it is an exploration framework for probing both depth and breadth of capabilities of reasoning models. The Dataset That Ate My Life As of this writing, the M12X dataset contains: 41 unique models (and counting) 2+ million individual tests 5.5+ billion tokens of model responses Complete difficulty landscapes across all 12 domains Spectral analysis data for every test case Full response caching for zero-cost re-analysis All data was generated in my basement on 4x very tired RTX3090 over the span of roughly 2 months. The Leaderboard tool offers a birds-eye view of the results: The smallest model evaluation (a tiny granite variant) burned through 7 million tokens. The largest (Ring Mini 2.0 at hard difficulty with thinking enabled) consumed 85 million tokens - that's longer than the entire Game of Thrones book series, except it's a language model having an existential crisis about bracket matching. The Explorer tool lets you fly through these landscapes in real-time 3D, comparing models, analyzing projections, examining spectral patterns. The Leaderboard shows exactly where each model's cognitive architecture excels and where it collapses, with truncation indicators showing guessing inflation and confidence intervals showing statistical certainty. The Tools That Built Themselves Along the way, I accidentally built an entire toolkit: The parametric test generators became a framework where adding a new domain is just defining difficulty parameters and a generation function. Want to test spatial rotation? Temporal arithmetic? Semantic categorization? Just map it to coordinates and plug it in. The analysis pipeline evolved into a full statistical framework with excess accuracy correction, confidence intervals, hierarchical sampling, and spectral analysis. It handles truncation, corrects for guessing, and produces publication-ready statistical rigor by default. The visualization tools grew from \"show me some numbers\" to interactive 3D explorers with FFT analysis, multi-model comparison, and projection studies. The leaderboard displays performance heatmaps where color shows accuracy and rising darkness indicates truncation at random guessing. At a glance you can see exactly where models break. None of this was planned. Each tool solved one problem, which revealed the next problem, which required the next tool. What Actually Matters (Besides My Power Bill) After analyzing billions of tokens across millions of tests, some patterns emerged: 1. Format effects are huge and systematic. The same reasoning capability looks 30 percentage points different between multiple choice and write-in for",
      "content_length": 1849,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.74,
      "scrape_method": "requests"
    },
    {
      "content_id": "4811fc609e9904f207fc240926d262e2",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "Code a simple RAG from scratch",
      "url": "https://huggingface.co/blog/ngxson/make-your-own-rag",
      "author": "Unknown",
      "published_at": "2025-01-18T00:00:00",
      "fetched_at": "2025-10-17T17:31:01.552793",
      "content_type": "article",
      "content_text": "Back to Articles Code a simple RAG from scratch Community Article Published\n\t\t\t\tOctober 29, 2024 Upvote 220 +214 Xuan-Son Nguyen ngxson Follow Recently, Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm in the field of AI and Large Language Models (LLMs). RAG combines information retrieval with text generation to enhance language models' performance by incorporating external knowledge sources. This approach has shown promising results in various applications, such as question answering, dialogue systems, and content generation. In this blog post, we'll explore RAG and build a simple RAG system from scratch using Python and ollama . This project will help you understand the key components of RAG systems and how they can be implemented using fundamental programming concepts. What is RAG To begin, let's examine a simple chatbot system without RAG : While the chatbot can respond to common questions based on its training dataset, it may lack access to the most up-to-date or domain-specific knowledge. A real-world example would be asking ChatGPT \"What is my mother's name?\". ChatGPT cannot answer this question because it doesn't have access to external knowledge, such as your family members' information. To address this limitation, we need to provide external knowledge to the model (in this example, a list of family members' names): A RAG system consists of two key components: A retrieval model that fetches relevant information from an external knowledge source, which could be a database, search engine, or any other information repository. A language model that generates responses based on the retrieved knowledge. There are several ways to implement RAG, including Graph RAG, Hybrid RAG, and Hierarchical RAG, which we'll discuss at the end of this post. Simple RAG Let's create a simple RAG system that retrieves information from a predefined dataset and generates responses based on the retrieved knowledge. The system will comprise the following components: Embedding model : A pre-trained language model that converts input text into embeddings - vector representations that capture semantic meaning. These vectors will be used to search for relevant information in the dataset. Vector database : A storage system for knowledge and its corresponding embedding vectors. While there are many vector database technologies like Qdrant , Pinecone , and pgvector , we'll implement a simple in-memory database from scratch. Chatbot : A language model that generates responses based on retrieved knowledge. This can be any language model, such as Llama, Gemma, or GPT. Indexing phase The indexing phase is the first step in creating a RAG system. It involves breaking the dataset (or documents) into small chunks and calculating a vector representation for each chunk that can be efficiently searched during generation. The size of each chunk can vary depending on the dataset and the application. For example, in a document retrieval system, each chunk can be a paragraph or a sentence. In a dialogue system, each chunk can be a conversation turn. After the indexing phrase, each chunk with its corresponding embedding vector will be stored in the vector database. Here is an example of how the vector database might look like after indexing: Chunk Embedding Vector Italy and France produce over 40% of all wine in the world. \\[0.1, 0.04, -0.34, 0.21, ...\\] The Taj Mahal in India is made entirely out of marble. \\[-0.12, 0.03, 0.9, -0.1, ...\\] 90% of the world's fresh water is in Antarctica. \\[-0.02, 0.6, -0.54, 0.03, ...\\] ... ... The embedding vectors can be later used to retrieve relevant information based on a given query. Think of it as SQL WHERE clause, but instead of querying by exact text matching, we can now query a set of chunks based on their vector representations. To compare the similarity between two vectors, we can use cosine similarity, Euclidean distance, or other distance metrics. In this example, we will use cosine similarity. Here is the formula for cosine similarity between two vectors A and B: Don't worry if you are not familiar with the formula above, we will implement it in the next section. Retrieval phrase In the diagram below, we will take an example of a given Input Query from User . We then calculate the Query Vector to represent the query, and compare it against the vectors in the database to find the most relevant chunks. The result returned by The Vector Database will contains top N most relevant chunks to the query. These chunks will be used by the Chatbot to generate a response. Let's code it In this example, we will write a simple Python implement of RAG. To run the models, we will use ollama , a command line tool that allows you to run models from Hugging Face. With ollama, you don't need to have access to a server or cloud service to run the models. You can run the models directly on your computer. For the models, let's use the following: Embedding model : hf.co/CompendiumLabs/bge-base-en-v1.5-gguf Language model : hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF And for the dataset, we will use a simple list of facts about cat . Each fact will be considered as a chunk in the indexing phrase. Download ollama and models First, let's start by installing ollama from project's website: ollama.com After installed, open a terminal and run the following command to download the required models: ollama pull hf.co/CompendiumLabs/bge-base-en-v1.5-gguf\nollama pull hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF If you see the following output, it means the models are successfully downloaded: pulling manifest\n...\nverifying sha256 digest\nwriting manifest\nsuccess Before continuing, to use ollama in python, let's also install the ollama package: pip install ollama Loading the dataset Next, create a Python script and load the dataset into memory. The dataset contains a list of cat facts that will be used as chunks in the indexing phrase. You can download the example dataset from here . Here is an example code to load the dataset: dataset = [] with open ( 'cat-facts.txt' , 'r' ) as file:\n  dataset = file.readlines() print ( f'Loaded { len (dataset)} entries' ) Implement the vector database Now, let's implement the vector database. We will use the embedding model from ollama to convert each chunk into an embedding vector, then store the chunk and its corresponding vector in a list. Here is an example function to calculate the embedding vector for a given text: import ollama\n\nEMBEDDING_MODEL = 'hf.co/CompendiumLabs/bge-base-en-v1.5-gguf' LANGUAGE_MODEL = 'hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF' # Each element in the VECTOR_DB will be a tuple (chunk, embedding) # The embedding is a list of floats, for example: [0.1, 0.04, -0.34, 0.21, ...] VECTOR_DB = [] def add_chunk_to_database ( chunk ):\n  embedding = ollama.embed(model=EMBEDDING_MODEL, input =chunk)[ 'embeddings' ][ 0 ]\n  VECTOR_DB.append((chunk, embedding)) In this example, we will consider each line in the dataset as a chunk for simplicity. for i, chunk in enumerate (dataset):\n  add_chunk_to_database(chunk) print ( f'Added chunk {i+ 1 } / { len (dataset)} to the database' ) Implement the retrieval function Next, let's implement the retrieval function that takes a query and returns the top N most relevant chunks based on cosine similarity. We can imagine that the higher the cosine similarity between the two vectors, the \"closer\" they are in the vector space. This means they are more similar in terms of meaning. Here is an example function to calculate the cosine similarity between two vectors: def cosine_similarity ( a, b ):\n  dot_product = sum ([x * y for x, y in zip (a, b)])\n  norm_a = sum ([x ** 2 for x in a]) ** 0.5 norm_b = sum ([x ** 2 for x in b]) ** 0.5 return dot_product / (norm_a * norm_b) Now, let's implement the retrieval function: def retrieve ( query, top_n= 3 ):\n  query_embedding = ollama.embed(model=EMBEDDING_MODEL, input =query)[ 'embeddings' ][ 0 ] # temporary list to store (chunk, similarity) pairs similarities = [] for chunk, embedding in VECTOR_DB:\n    similarity = cosine_similarity(query_embedding, embedding)\n    similarities.append((chunk, similarity)) # sort by similarity in descending order, because higher similarity means more relevant chunks similarities.sort(key= lambda x: x[ 1 ], reverse= True ) # finally, return the top N most relevant chunks return similarities[:top_n] Generation phrase In this phrase, the chatbot will generate a response based on the retrieved knowledge from the step above. This is done by simply add the chunks into the prompt that will be taken as input for the chatbot. For example, a prompt can be constructed as follows: input_query = input ( 'Ask me a question: ' )\nretrieved_knowledge = retrieve(input_query) print ( 'Retrieved knowledge:' ) for chunk, similarity in retrieved_knowledge: print ( f' - (similarity: {similarity: .2 f} ) {chunk} ' )\n\ninstruction_prompt = f'''You are a helpful chatbot. Use only the following pieces of context to answer the question. Don't make up any new information: { '\\n' .join([ f' - {chunk} ' for chunk, similarity in retrieved_knowledge])} ''' We then use the ollama to generate the response. In this example, we will use instruction_prompt as system message: stream = ollama.chat(\n  model=LANGUAGE_MODEL,\n  messages=[\n    { 'role' : 'system' , 'content' : instruction_prompt},\n    { 'role' : 'user' , 'content' : input_query},\n  ],\n  stream= True ,\n) # print the response from the chatbot in real-time print ( 'Chatbot response:' ) for chunk in stream: print (chunk[ 'message' ][ 'content' ], end= '' , flush= True ) Putting it all together You can find the final code in this file . To run the code, save it to a file named demo.py and run the following command: python demo.py You can now ask the chatbot questions, and it will generate responses based on the retrieved knowledge from the dataset. Ask me a question: tell me about cat speed\nRetrieved chunks: ...\nChatbot response:\nAccording to the gi",
      "content_length": 2164,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.6199999999999999,
      "scrape_method": "requests"
    },
    {
      "content_id": "1c805b9e26382ba3789b8af679a435e5",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "Lo\u00efck BOURDOIS\nPRO",
      "url": "https://huggingface.co/lbourdois",
      "author": "Unknown",
      "published_at": "2025-08-08T00:00:00",
      "fetched_at": "2025-10-17T17:31:02.630869",
      "content_type": "article",
      "content_text": "view post Post 923 New blog post analyzing the top 50 entities with the most downloaded models on @ huggingface \ud83e\udd17! https://huggingface.co/blog/lbourdois/huggingface-models-stats The purpose here is to get an idea of the profile of the models with the greatest impact in open source (we are not interested in closed models here!). 32 figures + data Enjoy \ud83e\udd17 See translation",
      "content_length": 59,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.6,
      "scrape_method": "requests"
    },
    {
      "content_id": "5d9ef162cf8a64fa65364856c5d48806",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "Ginkgo Datapoints",
      "url": "https://huggingface.co/ginkgo-datapoints",
      "author": "Unknown",
      "published_at": "2025-09-17T00:00:00",
      "fetched_at": "2025-10-17T17:31:03.483365",
      "content_type": "article",
      "content_text": "We are a leading AI-driven Contract Research Organization (CRO), revolutionizing drug discovery by uniting advanced high-throughput screening (HTS) capabilities with specialized assay development and CRISPR-based genetic engineering.  We offer unparalleled scale and unit economics for perturbation response profiling (DRUG-seq, High Content Imaging) giving customers the ability to screen thousands of compounds and genetic targets. To join the Antibody Developability Competition, go here: https://huggingface.co/spaces/ginkgo-datapoints/abdev-leaderboard . Check out our blog post with Hugging Face about publishing GDPx1-3 and GDPa1 on the Hugging Face platform! Read through our GDPx2 and GDPa1 pre-prints to get an in-depth look at our analyses and the insights you can glean from the datasets!",
      "content_length": 106,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.64,
      "scrape_method": "requests"
    },
    {
      "content_id": "77beb3dcdc23155badb912daa3b24e96",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "Announcing Hugging Face Fundamentals: A New Learning Track on DataCamp",
      "url": "https://huggingface.co/blog/huggingface/datacamp-ai-courses",
      "author": "Unknown",
      "published_at": "2025-10-17T17:31:04.189640",
      "fetched_at": "2025-10-17T17:31:04.189679",
      "content_type": "article",
      "content_text": "Back to Articles Announcing Hugging Face Fundamentals: A New Learning Track on DataCamp Community Article Published\n\t\t\t\tOctober 16, 2025 Upvote 13 +7 ben burtenshaw burtenshaw Follow huggingface We're excited to announce a partnership with DataCamp to bring comprehensive and interactive Hugging Face courses to AI/ML engineers and software engineers. The new Hugging Face Fundamentals skill track combines hands-on courses with a real-world project designed to help you master building modern AI workflows with the Hugging Face ecosystem. All courses feature interactive, real-world exercises that run directly in your browser, no local setup required, ensuring you gain practical skills applicable to your AI projects immediately. [!TIP] If you're new to Hugging Face, or working with folks who are, this skill track works through progressive courses from using the Hub to building multi-modal applications and AI agents. Plus, the first course is free until the end of 2025. Start learning today with zero cost or commitment! The Learning Track Built in collaboration with the Hugging Face team, the fundamental skill track consists of four comprehensive courses and a real world project that progressively build your expertise. 1. Working with Hugging Face This 2-hour beginner course is your comprehensive introduction to the Hugging Face ecosystem. Perfect for newcomers to AI and machine learning, it walks you through the fundamentals of working with pre-trained models and datasets. Covered in this course: Navigate and search the Hugging Face Hub effectively Load and use pre-trained models with the transformers library Download and prepare datasets for various NLP tasks Build production-ready pipelines for text classification, summarization, and question answering (QA) Deploy models for real-world inference scenarios 2. Introduction to LLMs in Python Take a deeper dive into LLMs, their transformer architecture, and Hugging Face\u2019s transformer library. This course moves inference into custom fine-tuning and evaluation to set you up for real-world projects. The nuts and bolts of transformer architecture Working with large language models for various NLP tasks Fine-tuning models for custom use cases Understanding model parameters and optimization techniques Working with custom datasets to improve model performance 3. Multi-Modal Models with Hugging Face This advanced course expands your AI toolkit beyond text to modalities including mages, audio, video, and their combinations. As AI applications increasingly require understanding and generating content across multiple modalities, this course positions you at the forefront of modern AI development. What you\u2019ll explore in the course: Build computer vision applications for image classification and analysis Process and analyze audio and video content with specialized models Create images and videos using state-of-the-art generative models Develop multi-modal workflows integrating text, images, audio, and video Implement image-text-to-image and image-text-to-text conversion pipelines Build visual question answering (VQA) systems that understand and reason about images 4. AI Agents with Hugging Face smolagents This cutting-edge course introduces you to the future of AI: autonomous agents that can reason, plan, and execute complex tasks independently. Using Hugging Face's lightweight smolagents framework, you'll learn to build AI systems that go far beyond simple question-answering to become true digital assistants capable of interacting with the world. Your learning outcomes: Build autonomous agents that reason through complex multi-step tasks Create coding agents that generate and execute Python code independently Design and orchestrate multi-agent systems for sophisticated workflows Implement web browsing capabilities for real-time information gathering Deploy agent systems with proper security, sandboxing, and execution controls Real World Project The skill track finishes with a real-world project to build a food image classification with the hub, transformers, and everything you\u2019ve learnt! Next Steps The entire track is designed for progressive learning, but you can also take individual courses based on your specific skill level, interests, and goals. Check out the Hugging Face Fundamentals Track to get started.",
      "content_length": 614,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.725,
      "scrape_method": "requests"
    },
    {
      "content_id": "6fb8ec48fec852b4d760283b2ddbc93a",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "Visualizing How VLMs Work",
      "url": "https://huggingface.co/blog/not-lain/vlms",
      "author": "Unknown",
      "published_at": "2025-10-13T00:00:00",
      "fetched_at": "2025-10-17T17:31:05.064679",
      "content_type": "article",
      "content_text": "Back to Articles Visualizing How VLMs Work Community Article Published\n\t\t\t\tOctober 7, 2025 Upvote 38 +32 Hafedh Hichri not-lain Follow Ed Daniels Donnyed Follow Introduction Visual Language Models (VLMs) are autoregressive AI models that process both text and images as input. In this post, we\u2019ll take a closer look at how VLMs like Idefics3 and SmolVLM operate under the hood exploring how they merge visual and textual information to generate coherent outputs. As model reference in this blogpost we will use HuggingFaceTB/SmolVLM-256M-Instruct . Processor Image Processor The processor prepares both text and image data into a unified format suitable for the model. For images, it performs a sequence of transformations before converting them into token-like representations the model can understand. The image pipeline can be visualized as follows: A key step in this process is image splitting , as seen in this code snippet .\nEach image is divided into smaller patches (or \u201csplits\u201d), which are individually encoded. When text accompanies images, each image is first represented by a <image> token in the text sequence. Depending on the number of splits, each image is then expanded into: 64 \u00d7 number_of_splits 64 \\times \\text{number\\_of\\_splits} 64 \u00d7 number_of_splits The constant 64 comes from the relation: 512 2 / 16 2 4 2 = 64 \\frac{512^2 / 16^2}{4^2} = 64 4 2 51 2 2 /1 6 2 \u200b = 64 We\u2019ll get back to this formula later, but for now, think of it as: Each image split is represented by 64 tokens. Text Processor When an image is included, the text must also reflect it using the <image> placeholder. The processor follows these steps: Insert placeholders \u2013 Each image in the input is represented by a <image> token within the text. Count splits \u2013 The processor determines how many splits each image has after preprocessing. Expand tokens \u2013 Each <image> token is then replaced (or \"expanded\") into a sequence based on the number of image splits, according to the formula below. To fully grasp how image tokens are expanded, check the encoding example below and review the following references: Documentation example \u2013 shows a practical example of how expansion happens. Main function \u2013 the core implementation that defines the expansion logic. Encoding Example (click to expand) import torch from PIL import Image from transformers import AutoProcessor, AutoModelForVision2Seq from transformers.image_utils import load_image # Initialize processor and model processor = AutoProcessor.from_pretrained( \"HuggingFaceTB/SmolVLM-256M-Instruct\" )\nmodel = AutoModelForVision2Seq.from_pretrained( \"HuggingFaceTB/SmolVLM-256M-Instruct\" ) # Load images image = load_image( \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\" ) # Create input messages messages = [\n    { \"role\" : \"user\" , \"content\" : [\n            { \"type\" : \"image\" },\n            { \"type\" : \"text\" , \"text\" : \"Can you describe this image?\" }\n        ]\n    },\n]\n\nprompt = processor.apply_chat_template(messages, add_generation_prompt= True )\ninputs = processor(text=prompt, images=[image], return_tensors= \"pt\" )\n\nout = processor.decode(inputs[ \"input_ids\" ][ 0 ]) print (out.replace( \"<image>\" , \".\" )) # printing the full <image> will bload the screen <|im_start|>User:<fake_token_around_image><row_1_col_1>................................................................<fake_token_around_image><row_1_col_2>................................................................<fake_token_around_image><row_1_col_3>................................................................<fake_token_around_image><row_1_col_4>................................................................\n<fake_token_around_image><row_2_col_1>................................................................<fake_token_around_image><row_2_col_2>................................................................<fake_token_around_image><row_2_col_3>................................................................<fake_token_around_image><row_2_col_4>................................................................\n<fake_token_around_image><row_3_col_1>................................................................<fake_token_around_image><row_3_col_2>................................................................<fake_token_around_image><row_3_col_3>................................................................<fake_token_around_image><row_3_col_4>................................................................\n\n<fake_token_around_image><global-img>................................................................<fake_token_around_image>Can you describe this image?<end_of_utterance>\nAssistant: Data Preparation Before being fed into the model, both the image and text data are prepared and aligned.\nAs in most autoregressive models, the output sequence is shifted to the right to teach the model how to predict the next token based on all previous ones. However, since image representations cannot be directly predicted, they are masked using the <pad> token.\nThis prevents the model from computing a loss over non-text (visual) tokens. Model Architecture Embedding Layer From a high-level view, SmolVLM consists of five main components (illustrated on the right).\nThe text processing starts with the prompt , which is tokenized and passed through an embedding layer . This layer transforms discrete tokens into high-dimensional vector representations, producing a tensor of shape: [ sequence_length , 576 ] [\\text{sequence\\_length}, 576] [ sequence_length , 576 ] This tensor now encodes the vector representation of the input text, and it serves as the foundation for all subsequent computations, note that this tensor already has 13 x 64 placeholder tokens for the upcoming processed image (where 13 is the number of image splits) Vision Model Patch Embedding For the visual branch, the input tensor has the shape [splits, num_channels, height, width] .\nFor example, for an image that can be split into 13 splits of RGB images is represented as [13, 3, 512, 512] . The patch embedding layer transforms this tensor into a sequence of visual tokens , making it compatible with the Transformer architecture.\nIt does this by dividing the image into small, non-overlapping patches and projecting each one into a high-dimensional vector space, the core principle behind this is to adapt the RGB channel as an embedding dim and expand that 3 -> 768 This operation is implemented as a 2D convolution , where kernel_size and stride are both set to the patch size. This ensures each convolutional window processes one patch without overlap: self.patch_embedding = nn.Conv2d(\n    in_channels=config.num_channels, # 3 (RGB channels) out_channels=self.embed_dim, # 768 kernel_size=self.patch_size, # 16 stride=self.patch_size, # 16 padding= \"valid\" ,\n) When applied to [13, 3, 512, 512] , the transformation proceeds as follows: Step Operation Output Shape Description 1 Conv2d [13, 768, 32, 32] Each 16\u00d716 patch is embedded into a 768-dimensional vector ( 512 / 16 = 32 ). 2 Reshape [13, 1024, 768] The 32\u00d732 grid is flattened into 1024 patches (visual tokens). The resulting tensor [13, 1024, 768] represents each image as a sequence of 1024 embedded patches , ready to be processed alongside text embeddings within the Transformer. Positional Encoder The positional encoder injects information about patch order and spatial layout just as positional encodings do for words in a text model.\nSince transformers have no inherent sense of order, these encodings allow the model to understand where each patch is located within the image. Encoder The encoder operates in a straightforward manner: A Multi-Head Attention (MHA) layer captures relationships between image patches, enabling the model to reason about spatial dependencies. The output is then passed through a feed-forward network (MLP) to refine and project the features. This attention implementation is close to how BERT functions, it is essential note that the output of the vision_model is [13,1024,768] given the example input we are using. Connector The connector serves as the bridge between the vision encoder and the language model , ensuring both modalities share a compatible embedding space.\nIts two main functions are: Compressing the visual output (reducing the number of tokens). Casting the embedding dimension to match that of the text embeddings. Pixel Shuffle The pixel shuffle operation compresses the spatial dimension of the visual features while preserving important spatial relationships.\nIn practical terms, it reduces the number of image tokens from 1024 \u2192 64 , drastically lowering the sequence length while maintaining representational richness. This transformation unfolds as follows: Description Code # (split height and width) \u2192 [splits, H, W, C] #  (apply transformation on width dimension) \u2192  [splits, H, W/scale, C\u00d7scale] #  (permute dimensions) \u2192 [splits, W/scale, H, C\u00d7scale] #  (apply transformation on height dimension) \u2192 [splits, W/scale, H/scale, C\u00d7scale\u00b2] # (permute dimensions back) \u2192 [splits, H/scale, W/scale, C\u00d7scale\u00b2] # (merge height and width back) \u2192 [splits, (H/scale)\u00d7(W/scale), C\u00d7scale\u00b2] by doing so we get a final dimension equal to [ splits , H\u2019 / scale \u00d7 W\u2019 / scale , C \u00d7 scale 2 ] [\\text{splits}, \\text{H'} / \\text{scale} \\times \\text{W'} / \\text{scale}, \\text{C} \\times \\text{scale}^2 ] [ splits , H\u2019 / scale \u00d7 W\u2019 / scale , C \u00d7 scale 2 ] resulting in an output dimension equal to [ 13 , 1024 / 4 2 , 768 \u2217 4 2 ] \u2192 [ 13 , 64 , 12288 ] [13, 1024/4^2, 768 * 4^2] \\rightarrow [13, 64, 12288] [ 13 , 1024/ 4 2 , 768 \u2217 4 2 ] \u2192 [ 13 , 64 , 12288 ] By progressively shuffling and regrouping pixels, this operation compresses both height and width dimensions while maintaining the spatial order of visual information in each direction.\nThe result is a more compact tensor that preserves essential image features. Modality Projection The connector also applies a modality_projection layer which is a linear tran",
      "content_length": 1746,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.74,
      "scrape_method": "requests"
    },
    {
      "content_id": "bbc2ea70ef047fef6bd3f76b73553e9a",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "Ethics + Sustainability = Responsible AI",
      "url": "https://huggingface.co/blog/sasha/ethics-sustainability",
      "author": "Unknown",
      "published_at": "2025-10-17T17:31:05.866841",
      "fetched_at": "2025-10-17T17:31:05.866913",
      "content_type": "article",
      "content_text": "Back to Articles Ethics + Sustainability = Responsible AI Community Article Published\n\t\t\t\tOctober 9, 2025 Upvote 10 +4 Giada Pistilli giadap Follow Sasha Luccioni sasha Follow TL;DR What's the point of an \"ethical\" AI that burns through resources, or a \"green\" AI that amplifies bias? Ethics and sustainability have to evolve together, starting with how we measure and disclose AI's real costs. Introduction Recent years have seen more research dedicated to evaluating AI\u2019s impacts on society and the environment. Our own team at Hugging Face has led a lot of this research, working on topics ranging from consent and bias to quantifying the energy demands of video generation models . While all of this work is important and continues to deepen our understanding of AI\u2019s impacts on the world that we live in, most of it remains siloed in terms of addressing AI\u2019s sustainability and ethics separately. For instance, carbon footprint analyses of AI models typically do not consider how the pursuit of scale has contributed towards building models that are both inaccessible to most researchers in terms of cost and disproportionately harmful to the environment, whereas evaluations of model performance mainly fail to engage with the environmental ramifications of AI models and how these fit into their auditing approaches. In our recent article on this subject, we argue that by addressing these two sets of issues separately, both sides are missing key transversal connections, as well as the potential solutions for making informed choices. In the sections below, we will hone in on two specific topics that are particularly relevant for both ethics and sustainability within the open-source community: evaluation and transparency , and how they can be operationalized, highlighting several of our projects that contribute to these topics. We\u2019ll conclude with a proposal of best practices to better integrate AI ethics and sustainability in AI research and governance. Transversal issues in ethics and sustainability In our initial paper , we identified four transversal themes that cut across ethics and sustainability and were relevant to both: generalizability, evaluation, transparency, and power. In this blog post, we focus on two of the most pressing ones: evaluation and transparency , since these are the most relevant ones to the open-source community. Evaluation \ud83d\udccf A popular adage states that \u201c you can\u2019t improve what you don\u2019t measure \u201d \u2013 for AI systems, this means that the criteria that we use to compare different systems and the way in which this evaluation is carried out are crucial \u2013 making some aspects of systems\u2019 performance indispensable, while others are overlooked. Popular approaches for evaluating AI systems like LM Arena and the MTEB Leaderboard typically only measure metrics like accuracy or precision, which do allow comparing the performance of systems, but do not reflect other factors like efficiency or bias. However, the truth of the matter is that in the vast majority of real-world deployment contexts, multiple measures, from monetary cost to toxicity, are considered in parallel to performance. This is especially the case for large language models (LLMs), which don\u2019t have a single well-established evaluation approach. In this case, different evaluation approaches are adopted, from red-teaming to external audits as well as leaderboards such as the Open LLM Leaderboard, which go above and beyond performance criteria to also consider efficiency and even carbon emissions. However, the lack of a standardized methodology can make it hard to compare the metrics reported by different organizations. We recently illustrated this point in a blog post about the environmental impact disclosures made by prominent companies developing AI systems \u2013 since each organization uses their own methodology and their own set of evaluation criteria, the resulting numbers cannot be compared by users or developers. This lack of comparability was also the rationale for the AI Energy Score project , which adopts a standardized methodology to compare the energy consumption of AI models across ten different tasks, allowing developers and users to choose between different models based on their relative energy efficiency compared to other models in their category: While the AI Energy Score project is well-positioned to allow for meaningful comparisons of AI inference, it overlooks the rest of the model lifecycle - from the embodied emissions of GPU manufacturing to the energy consumption of data generation and model training. Some of our previous work has proposed ways to incorporate a broader perspective of AI\u2019s environmental impacts into model evaluation \u2013 from a Life Cycle Analysis approach to a consideration of the rebound effects incurred by the efficiency gains that were made. We are still missing a more formal framework for evaluating and comparing different AI systems - which would encompass different categories of environmental costs as well as broader ethical and societal impacts - but the key principle that enables this kind of evaluation is transparency, which we discuss below. Transparency \ud83d\udd0e One of the fundamental principles of scientific practice, transparency is a key component of research and inquiry, but operationalizing it in the context of modern AI is challenging. On the one hand, modern AI models are not inherently transparent given the complexity of their architectures, making it difficult to make meaningful conclusions about the way in which they operate. On the other hand, given the increasingly blurred line between AI research and practice, many of the most popular AI systems are proprietary and therefore subject to trade secrets regarding the specific details of their architecture and deployment - making it difficult to compare them. In terms of sustainability, transparency has lagged even further behind. Environmental impacts are still rarely reported, and most carbon footprint estimates are reconstructed post-hoc by independent researchers rather than disclosed by model creators. To address this gap, we developed the Environmental Transparency Space , an open platform that standardizes and visualizes carbon reporting for AI models across different years and organizations. By making environmental data openly available, it allows anyone to understand and compare the ecological costs of AI systems, and to see how transparency has evolved over time. Artifacts such as data sheets and model cards have become important tools to bridge that gap, offering essential information about how an AI system works \u2014 and, crucially, where it doesn\u2019t. They help document a model\u2019s training data, intended use, limitations, potential biases, and environmental footprint. These transparency tools make AI systems more understandable and accountable, but they can only go so far without access to deeper information. While accuracy-based benchmarks often rely on simple API queries, evaluation requires richer disclosure, including details about training data, compute usage, and, where possible, access to model weights. As a community, we therefore need to develop and adopt best practices, both in the context of research and governance, to guide AI towards increased transparency \u2013 we discuss these below. Best Practices \ud83d\udcd6 Research \ud83e\uddd1\u200d\ud83d\udd2c When it comes to research, integrating ethics and sustainability means looking at AI systems as part of a broader socio-technical and ecological network. Models do not exist in isolation: they depend on data, energy, and infrastructures, and they affect communities and environments. Therefore, to make responsible progress, research should move beyond narrow technical benchmarks and engage with broader questions that span ethics and sustainability. Changing the way in which we evaluate AI systems . In reality, traditional machine learning metrics such as accuracy or efficiency tell us little about whether an AI system is just, inclusive, or environmentally responsible. A model that performs well on a benchmark might still amplify existing inequalities or increase energy consumption elsewhere. Moreover, efficiency gains can produce unintended rebound effects: when technology becomes cheaper or faster, it is often used more, offsetting any sustainability gains. We therefore argue for holistic evaluations that assess both social and environmental impacts, including long-term consequences that short-term performance metrics may not capture. Operationalizing transparency and standardization is essential. Research should clarify how AI systems work, where their data comes from, who stands to benefit, and who might bear the costs. Transparency in this sense is what allows scrutiny, reproducibility, and accountability. By embedding social and environmental context into research design and reporting, we strengthen scientific inquiry\u2019s reliability and ethical integrity. Governance \u2696\ufe0f As AI systems are increasingly scrutinized by policymakers and governments at different levels, it has become paramount to develop new governance mechanisms for operationalizing best practices in terms of system evaluation and transparency. This will require interdisciplinary approaches, incorporating efforts from different domains depending on the context of deployment \u2013 we propose some of these approaches below. Adapting compliance mechanisms to include environmental and ethical assessments: new and existing audits of AI systems can be extended to include assessments of both ethical and environmental impacts, such as bias and energy consumption and carbon emissions. Requiring these types of audits before the deployment of systems in practice, especially in high-stakes contexts such as education and healthcare, but also in contexts such as disaster prediction and climate modeling, that come with potentially widespread environmental impacts. Developing new governance approaches: attempting to evaluate the wider rebound effects of AI tools and their impacts on ",
      "content_length": 1727,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.74,
      "scrape_method": "requests"
    },
    {
      "content_id": "28f73d1274450716dd3253447f888c4d",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "Sherry Chen\nPRO",
      "url": "https://huggingface.co/sherryxychen",
      "author": "Unknown",
      "published_at": "2025-10-15T00:00:00",
      "fetched_at": "2025-10-17T17:31:06.596513",
      "content_type": "article",
      "content_text": "view post Post 336 **Training ACT on SO-101: From Woodpecker to 90% Success (All the Mistakes Included)** I spent 3 weeks training Action Chunking Transformer on SO-101 for pick-and-place. Spoiler: the first attempt trained a woodpecker that just pecked the table. \ud83d\udc26 **What's Different About This Post:** Most ACT tutorials show the success. I documented every failure, hardware issue, and debugging step. If you're new to SO-101/LeRobot/ACT, hopefully my mistakes save you time. Try 1: The Woodpecker - Followed the LeRobot tutorial, collected 50 episodes - Beautiful loss curves \u2705 - Robot learned to peck at table \u274c - Rookie mistakes: moving cameras, arm calibration mismatch, limited data diversity, looking at follower arm during teleop (it's cheating!) Try 2: Engineering Upgrades - Fixed hardware setup (tape + markers everywhere) - USB udev rules for camera stability - Formal task definition with stratified sampling - Built proper eval pipeline with progress scoring - Motor breakdown mid-collection (broke the gripper with excessive force \ud83d\udc80) - Results: 60% in-distribution success, 10% OOD (better, but not great) Try 3: More & Better Data sherryxychen/2025-09-07_act - 125 episodes with rotation variations ( sherryxychen/2025-09-01_pick-and-place-block ) - Better workspace coverage - Improved grasping technique - Results: 90% in-distribution success, 75% OOD! \ud83c\udf89 Key Learnings: - Consistent hardware setup is everything - Don't look at the follower arm during teleop - Data diversity is key for generalization - Debug infrastructure matters - Real robots break in mysterious ways (buy spare motors!) Full write-up (with more videos!): https://huggingface.co/blog/sherryxychen/train-act-on-so-101 Code: https://github.com/sherrychen1120/so101_bench Happy to answer any questions! \ud83e\udd17 #imitation-learning #lerobot #act #so-101 See translation",
      "content_length": 263,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.7100000000000001,
      "scrape_method": "requests"
    },
    {
      "content_id": "0ba9889321ea512f4fd9a68ec36b7cb6",
      "source_id": "huggingface",
      "source_name": "Hugging Face Blog",
      "title": "Granite Embedding R2: Setting New Standards for Enterprise Retrieval",
      "url": "https://huggingface.co/blog/hansolosan/granite-embedding-r2",
      "author": "Unknown",
      "published_at": "2025-10-17T17:31:07.286106",
      "fetched_at": "2025-10-17T17:31:07.286182",
      "content_type": "article",
      "content_text": "Back to Articles Granite Embedding R2: Setting New Standards for Enterprise Retrieval Community Article Published\n\t\t\t\tOctober 14, 2025 Upvote 13 +7 Radu Florian hansolosan Follow tl;dr IBM Research introduces next-generation embedding models that deliver breakthrough speed with top-tier accuracy In this post: What's New in the R2 Models ? Fast and Accurate Models for Retrieval and Reranking Enterprise-Ready from Day One How to Use the Models Why This Matters When it comes to enterprise information retrieval, organizations face a persistent challenge: existing embedding models force you to choose between accuracy and speed, between long-context support and commercial licensing, between general-purpose performance and domain-specific excellence. On August 15th 2025, we\u2019ve introduced the Granite Embedding R2 models \u2014 a comprehensive family of retrieval models designed to reduce the impact of these tradeoffs. What's New in the R2 Models? The Granite Embedding R2 release includes three models, all available under Apache 2.0 license: granite-embedding-english-r2 (149M parameters): Our flagship model with 768-dimensional embeddings granite-embedding-small-english-r2 (47M parameters): A first-of-its-kind efficient model with 384-dimensional embeddings (the first small ModernBERT model) granite-embedding-reranker-english-r2 (149M parameters): A cross-encoder for precision ranking These models deliver three improvements over our first-generation release: 16x expanded context length from 512 to 8,192 tokens \u2014 meeting modern document processing requirements 19\u201344% faster inference than comparable models, without sacrificing accuracy state-of-the-art performance across text, code, long-documents, conversational queries, and tabular data (Want to skip straight to the code? We get it \u2014 jump to the examples and start embedding things.) Built on Modern Foundations The R2 models leverage the ModernBERT architecture, incorporating recent advances in encoder design: Alternating attention mechanisms for efficiency Rotary positional embeddings enabling flexible context lengths Flash Attention support for optimized inference We trained these models on 2 trillion tokens from high-quality sources including GneissWeb , Wikipedia, and Granite Code data. Every dataset underwent comprehensive governance review, with screening for personal information and profanity \u2014 because enterprise deployments demand transparency and responsible AI practices. A Novel Training Pipeline What sets Granite R2 apart is our five-stage training methodology: Retrieval-Oriented Pretraining: Using RetroMAE to train rich [CLS] representations without explicit contrastive objectives. Tabular Pretraining: Traditional embedding models struggle with tables containing numerical data and limited context. Our solution? We generated synthetic summaries for 8 million tables using Mistral-7B, then modified the RetroMAE objective to predict masked tokens over summaries rather than table content itself. This forces the encoder to align table structure with natural language descriptions. Contrastive Finetuning: Training on large-scale semi-supervised pairs with improved contrastive loss. Contrastive Distillation: Rather than simply finetuning on hard negatives, we distill knowledge from a Mistral-7B teacher model trained on high-quality triples. This approach yields larger performance gains than traditional hard-negative training. Domain Adaptation: Specialized training for multi-turn conversational retrieval. This pipeline enables a single model family to excel across remarkably diverse tasks. Fast and Accurate Models for Retrieval and Reranking We evaluated Granite R2 on six open source retrieval benchmarks part of MTEB benchmarks (MTEB v2, CoIR, TableIR, LongEmbed, MTRAG, and MLDR), and the results demonstrate clear leadership in both accuracy and speed, as shown below. Solid bars represent models under 500M parameters, and hashed bars are for models under 100M parameters. Corresponding families share the same fill color. Accuracy: State-of-the-Art Across the Board As the chart shows, the granite-embedding-english-r2 model achieves the highest average performance at 59.5 NDCG@10, outperforming all comparable open-source models \u2014 including models that are twice its size. Even our efficient granite-embedding-small-english-r2 scores an average of 55.6, surpassing many larger open-source competitors. As of this writing (October 6th, 2025) \u2014 if one builds an English benchmark on the MTEB site with Reranking and Retrieval as tasks (which are the two objectives of the R2 granite embedding models) \u2014 the granite-embedding-english-r2 model is ranked first (as seen below) among the models with less than 500M parameters\u00b9. Similarly, the granite-embedding-small-english-r2 is ranked second for models under 100M parameters: Speed: Industry-Leading Efficiency Performance benchmarks often overlook a critical real-world constraint: encoding speed. When you\u2019re ingesting millions of documents with frequent updates, speed directly impacts operational costs and user experience. We benchmarked text embedding speed on a dataset of 23,000 IBM technical documents (averaging 6,393 characters, ranging from 10 to 475,001 characters, details in \u2014 you guessed it! \u2014 our paper ): The R2 models are 19\u201344% faster than leading competitors and as fast as the R1 models, despite the R2 models having slightly more parameters. The ModernBERT architecture\u2019s optimizations \u2014 particularly Flash Attention \u2014 enable this efficiency gain. The speed advantage becomes even more pronounced with the small model, which processes nearly 200 documents per second while maintaining competitive accuracy. This makes it ideal for real-time applications and high-throughput ingestion pipelines. All experiments were run on a H100, with a context size of 512 tokens and a batch of 128. Complete Retrieval Ecosystem: Reranker The reranker model completes the retrieval pipeline. Built on the granite-embedding-english-r2 model, it uses a PListMLE loss objective for position-aware ranking. Below is the comparison of the performance of the granite-embedding-english-reranker-r2 compared with a few open source rerankers of similar size: This retrieve-and-rerank framework maximizes both recall and precision without severe computational overhead. Enterprise-Ready from Day One Granite models prioritize enterprise requirements, including Data Governance: Comprehensive clearance process capturing content description, intended use, data classification, licensing information, usage restrictions, and personal information assessment Licensing: Apache 2.0 \u2014 no restrictions on commercial use, no proprietary training data limitations NTransparency: Fully documented training data sources, architectural decisions, and evaluation methodology More about IBM\u2019s open source LLM policy: https://www.ibm.com/granite/trust How to Use the Models All Granite Embedding R2 models are available now on Hugging Face under Apache 2.0 license: granite-embedding-english-r2 granite-embedding-small-english-r2 granite-embedding-reranker-english-r2 For technical details, architecture description, and comprehensive benchmark results, see our research paper . Head to granite embedding R2 models jupyter notebook to test/deploy these models, and visit the links above to read the models' cards. Please consider giving us a \u2764\ufe0f if you find the model useful! Why This Matters Information retrieval isn\u2019t just about finding documents \u2014 it\u2019s about enabling AI systems to access relevant knowledge efficiently. Whether you\u2019re building RAG applications, semantic search engines, or recommendation systems, embedding quality and speed determine what\u2019s possible. Granite R2 models don\u2019t force you to choose between accuracy and speed, between long-context support and efficiency, between general-purpose capability and domain-specific performance \u2014 they deliver all of it. In an era where milliseconds matter and accuracy cannot be compromised, Granite R2 models don\u2019t just meet the standard \u2014 they set it! The Granite Embedding R2 models represent collaborative work across IBM Research teams in multiple geographies. For questions or feedback, visit our GitHub repository. This work is a collaboration with many people at IBM Research including (in alphabetical order): Parul Awasthy, Ken Barker, Riyaz Bhat, Meet Doshi, Martin Franz, Bhavani Iyer, Vishwajeet Kumar, Yulong Li, Rudra Murthy, Vignesh P, Salim Roukos, Jaydeep Sen, Aashka Trivedi, Todd Ward, and Yushu (Elaine) Yang. \u00b9To generate the comparison tables above, go to the MTEB leaderboard, select \u201cGeneral Purpose\u201d/English on the left, then open the tie \u201cCustomize this Benchmark\u201d on the right and remove every task but \u201cRetrieval\u201d and \u201cReranking\u201d. Finally, open the tie for \u201cAdvanced Model Filters\u201d and select models \u201c<500M\u201d Model Parameters.",
      "content_length": 1196,
      "category": "ai_tutorials",
      "priority": "high",
      "base_weight": 0.7,
      "raw_score": 0.74,
      "scrape_method": "requests"
    }
  ]
}