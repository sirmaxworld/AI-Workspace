{
  "source_id": "moz",
  "source_name": "Moz Blog",
  "scrape_method": "requests",
  "collected_at": "2025-10-17T17:30:51.660984",
  "article_count": 12,
  "articles": [
    {
      "content_id": "7c8b0841ab37cc507f9624f8c5310f60",
      "source_id": "moz",
      "source_name": "Moz Blog",
      "title": "How to Be an Effective SEO Mentor \u2014 Whiteboard Friday",
      "url": "https://moz.com/blog/how-to-be-an-effective-mentor-whiteboard-friday",
      "author": "",
      "published_at": "2025-10-03T00:00:00",
      "fetched_at": "2025-10-17T17:30:33.037266",
      "content_type": "article",
      "content_text": "Table of Contents When you're set up for success, being a mentor can be a rewarding experience for you and your mentee. Miracle\u2019s in-depth framework presented in this week\u2019s Whiteboard Friday covers considerations like building trust, setting boundaries, and the qualities that make a great mentor. Click on the whiteboard image above to open a high-resolution version! Hello. My name is Miracle Inameti-Archibong. Welcome to Whiteboard Friday. And today, I'm going to be taking you through a topic that's very close to my heart \u2014 how to become an effective mentor. So today, I'm going to be taking you through how you can be an effective mentor and showing you that this is not something that is impossible for anyone to do because a mentor is just someone who you can sit down and share things with. A mentor is someone who motivates you, and a mentor is someone who equips you to grow. And I think one of the things people struggle with as mentors is they feel they have to have all the answers. I find out, when I'm mentoring people, a lot of time people already know what they want to do. People have agency. They just need someone to support them through that agency, and they just need someone to believe in them. And I think the goal of any mentoring session is to leave your mentee feeling like they can do things on their own. If you've ended a mentorship session and they still need you to solve the same problem, then it hasn't been successful. So it's not about you having all the answers. It's about you equipping somebody else to believe in their own solutions and keep doing it and just have that courage to grow. What are the key qualities of a mentor? Objectivity So what are the key qualities of a mentor? I think the first thing I always say to people is, \"Thou shall not collude.\" I think when you're supporting someone, you want to be nice, and so you want to encourage them, and sometimes you can be overly, overly supportive in a sense that you don't give them the right feedback. You find it difficult to have conversations that call them out. As a mentor, you have to challenge what you're hearing. You have to support, but with objectivity. Listening Another thing is listening. I love this definition of listening by Susan Howard, which is holding your self-expression in check. I really struggled with listening when I first started because I thought, \"Oh, I'm a really good listener. I'm just yes, yes, yes. I'm listening to the person. I know exactly what to respond.\" And when you realize that listening is not about responding, it's about actually hearing and understanding what the person is saying, it will give you a whole new meaning. And you'll realize that there's constantly a voice in your head analyzing what next to say when someone is talking, and so you have to practice listening, just keeping everything quiet, and then testing that understanding to say, because we all have bias, and we filter everything that we hear via the lens of what's happened to us in our past. And so when you listen to someone, always stop and be like, \"This is what I have understood, and I'm just checking that what exactly you were saying to me is exactly what I've understood, and that's why I'm saying it back to you.\" Self-awareness The next thing is self-awareness. Now I've talked about bias, and it's very easy for you to collude with somebody if you're not self-aware of your own bias. Like everyone feels that they don't have bias, but we all have biases. If you've been through your career and you've had a particularly bad boss that said exactly X, Y, Z to you, and you find yourself in that kind of mentoring situation where someone says, \"Oh, this person did this to me,\" and you've been in a similar situation, chances are your first instinct will be to agree with them. But you need to be self-aware enough to challenge that understanding. I think one of the rules of thumb for me is, like, I always say, \"Okay, let's look at the history. What happened before? What happened in your past job? Is this a pattern?\" And try to examine what I'm hearing. Am I being overly supportive? Is this true? So always make sure you're testing your understanding. And the key if you're not self-aware that these are blind spots that I have, and I'm not expecting everyone to go ahead and turn around and change their personality overnight. I don't believe that things like that happen. But the key to being self-aware is understanding that I have a blind spot in this. I have been given this feedback that I don't always see this. And so whenever I come into a new setting or a new conversation, I have to be aware that I have a tendency to do the X, Y, Z. So whenever I hear something, I need to test my understanding against all of these preset beliefs that I already have. Okay. How to cultivate a successful mentoring relationship Set boundaries How to cultivate a successful mentoring relationship. I tell a story about when I did my counseling course like everyone was grown. We decided to set up a WhatsApp group. And the tutor said to us, it's like, \"What are the rules for this group?\" And we're like, \"No, we're all grown-ups. It's going to be fine. Everyone knows how to behave.\" And at the end of each session, we have a group therapy. And then at the fifth group therapy session, she said, \"Do you realize that you all spend at least 30 minutes talking about this WhatsApp group and how it's making everyone feel?\" And we were like, \"Oh, wow.\" And she was like, \"This is because you set no boundaries.\" And so it's very easy because a lot of times, when we come into mentoring situations, it's someone we already know, someone we already like. And so you might just be like, \"Oh, just go loosey-goosey. We're all grown-ups. We'll do it as it comes.\" But it is really, really key, if you take anything from this, is boundaries. You need to set boundaries because if you don't set boundaries, you can't tell when they've been broken. If you start a mentoring relationship and you allow someone free access all the time, and then you go three months down the lane and you're like, \"Oh, I don't feel comfortable with this,\" that will break the trust because it will be like a betrayal of this relationship that we've built. So set your boundaries right from the beginning. And the first thing to think, \"What's the duration of this?\" A mentorship is not an unending relationship, because if you leave it as an unending relationship, it can feel like a burden. So the first thing to do is set the duration, set the time, set the mode of contact. I only want to be contacted via emails, Monday to Friday, X, Y, Z. I can only respond here. I don't want to be contacted via WhatsApp. You need to set those boundaries so that the mentorship that you are offering doesn't become a burden to you. Define outcomes Next thing is define outcomes. Because a mentorship is a time, it's a duration thing, it doesn't go on forever, you need to make sure that you're defining your outcomes so that you can monitor progress. If not, if you don't set an outcome, how would you know if you've reached it? So make sure you set your goals. Be vulnerable Next thing to do is vulnerability. Because it's a time-bound relationship, it's essential that you build trust. Being genuine is what brings out a vulnerability in us because we don't always want to show all the parts of ourselves. But if you're not doing that, you can't build trust because this is someone coming to you at a very vulnerable stage in their life. Vulnerability builds trust. We all seek to build connection. So if you give, you will get. Understand the ethics behind mentorship And finally, this is a lot of responsibility, so you need to understand that there are ethics behind this. Conflict of interest, confidentiality is key, because, again, you need to build that trust and keep that trust. And for other people to feel safe approaching you, they need to ensure and they need to see that you have kept the confidence of somebody else. Power dynamics is another big one. You're in a situation of knowledge, and someone is coming to you at a very vulnerable time in their lives. Check that the power dynamics are right. Sometimes, especially in a professional setting, it might be that you've gone too high for a mentor, and you don't feel free to, like, speak about the things that are happening at work because that person is in a position of authority, making decisions. So make sure that you check, oh, I really want to help somebody, but my contract says I have a non-compete, and I can't really mentor someone in the same industry. So make sure that you're checking and you're keeping all of those things, you are respecting the ethics of this relationship. I will close with this quote by Maya Angelou. I love it so much. \"When you get, give. When you learn, teach.\" Thank you. The author's views are entirely their own (excluding the unlikely event of hypnosis) and may not always reflect the views of Moz. Written by Miracle Inameti-Archibong As an SEO Strategist, I thrive on solving complex problems and delivering practical solutions that help my clients grow. What excites me most about SEO is the ever-changing SERP landscape and the new challenges it presents. I love sharing my learnings from a decade in the industry at conferences like MozCon, SMX, BrightonSEO, and Women in TechSEO. Scale revenue from SEO with Moz Pro Get started with the all-in-one SEO tool marketers trust Start your free trial Get the latest SEO tips and strategies in your inbox Back to Top",
      "content_length": 1715,
      "category": "seo",
      "priority": "medium",
      "base_weight": 0.6,
      "raw_score": 0.65,
      "scrape_method": "requests"
    },
    {
      "content_id": "288463f4963b942cb574af4fb047dbf0",
      "source_id": "moz",
      "source_name": "Moz Blog",
      "title": "How to Use Chrome to View a Website as Googlebot",
      "url": "https://moz.com/blog/how-to-view-website-as-googlebot-in-chrome",
      "author": "",
      "published_at": "2025-10-09T00:00:00",
      "fetched_at": "2025-10-17T17:30:35.090332",
      "content_type": "article",
      "content_text": "Table of Contents Struggling to ensure Googlebot properly crawls and indexes your website? For technical SEOs, rendering issues, especially on JavaScript-heavy sites, can lead to missed rankings and hidden content. That\u2019s where using Chrome (or Chrome Canary) to emulate Googlebot comes in. This method uncovers discrepancies between what users and search engines see, ensuring your site performs as expected. Whether spoofing Googlebot or not, with a specific testing browser, technical audits are more efficient and accurate. In this guide, I\u2019ll show you how to set up a Googlebot browser, troubleshoot rendering issues, and improve your SEO audits. Why should I view a website as Googlebot? In the past, technical SEO audits were simpler, with websites relying on HTML and CSS, and JavaScript limited to minor enhancements like animations. Today, entire websites are built with JavaScript, shifting the workload from servers to browsers. This means that search bots, including Googlebot, must render pages client-side, a process that\u2019s resource-intensive and prone to delays . Search bots often struggle with JavaScript. Googlebot, for example, processes the raw HTML first and may not fully render JavaScript content until days or weeks later, depending on the website. Some sites use dynamic rendering to bypass these challenges, serving server-side versions for bots and client-side versions for users. Mini rant Generally, this setup overcomplicates websites and creates more technical SEO issues than a server-side rendered or traditional HTML website. Thankfully, dynamically rendered websites are declining in use. While exceptions exist, I believe client-side rendered websites are a bad idea. Websites should be designed to work on the lowest common denominator of a device, with progressive enhancement (through JavaScript) used to improve the experience for people using devices that can handle extras. My anecdotal evidence suggests that client-side rendered websites are generally more difficult for people who rely on accessibility solutions such as screen readers. Various studies back this up, though the studies I\u2019ve seen are by companies and charities invested in accessibility (an example where I think any bias is perhaps justified for the good of all). However, there are instances where technical SEO and usability crossover . The good news Viewing a website as a Googlebot lets you detect discrepancies between what bots and users see. While these views don\u2019t need to be identical, critical elements\u2014like navigation and content must align. This approach helps identify indexing and ranking issues caused by rendering limitations and other search bot-speicific quirks. Never miss an issue impacting traffic on your site Find and fix technical SEO issues fast with Moz Pro. Run a free site audit Can we see what Googlebot sees? No, not entirely. Googlebot renders webpages with a headless version of the Chrome browser , but even with the techniques in this article, it\u2019s impossible to replicate its behavior perfectly. For example, Googlebot\u2019s handling of JavaScript can be unpredictable. A notable bug in September 2024 prevented Google from detecting meta noindex tags in client-side rendered code for many React-based websites. Issues like these highlight the limitations of emulating Googlebot, especially for important SEO elements like tags and main content. The goal, however, is to emulate Googlebot\u2019s\u00a0mobile-first indexing as closely as possible. For this, I use a combination of tools: A Googlebot browser for direct emulation. Screaming Frog SEO Spider to spoof and render as Googlebot. Google\u2019s tools like the URL Inspection tool in Search Console and Rich Results Test for screenshots and code analysis. It\u2019s worth noting that Google\u2019s tools, especially after they switched to the \u201cGoogle-InspectionTool\u201d user-agent in 2023, aren\u2019t entirely accurate representations of what Googlebot sees. However, when used alongside the Googlebot browser and SEO Spider, they\u2019re valuable for identifying potential issues and troubleshooting. Why use a separate browser to view websites as Googlebot? Using a dedicated Googlebot browser simplifies technical SEO audits and improves the accuracy of your results. Here's why: 1. Convenience A dedicated browser saves time and effort by allowing you to quickly emulate Googlebot without relying on multiple tools. Switching user agents in a standard browser extension can be inefficient, especially when auditing sites with inconsistent server responses or dynamic content. Additionally, some Googlebot-specific Chrome settings don\u2019t persist across tabs or sessions, and specific settings (e.g., disabling JavaScript) can interfere with other tabs you\u2019re working on. You can bypass these challenges and streamline your audit process with a separate browser. 2. Improved accuracy Browser extensions can unintentionally alter how websites look or behave. A dedicated Googlebot browser minimizes the number of extensions, reducing interference and ensuring a more accurate emulation of Googlebot\u2019s experience. 3. Avoiding mistakes It\u2019s easy to forget to turn off Googlebot spoofing in a standard browser, which can cause websites to malfunction or block your access. I\u2019ve even been blocked from websites for spoofing Googlebot and had to email them with my IP to remove the block. 4. Flexibility despite challenges For many years, my Googlebot browser worked without a hitch. However, with the rise of Cloudflare and its stricter security protocols on e-commerce websites , I\u2019ve often had to ask clients to add specific IPs to an allow list so I can test their sites while spoofing Googlebot. When whitelisting isn\u2019t an option, I switch to alternatives like the Bingbot or DuckDuckBot user-agent. It's a less reliable solution than mimicking Googlebot, but can still uncover valuable insights. Another fallback is checking rendered HTML in Google Search Console , which, despite its limitation of being a different user-agent to Google's crawler, remains a reliable way to emulate Googlebot behavior. If I\u2019m auditing a site that blocks non-Google Googlebots and can get my IPs allowed, the Googlebot browser is still my preferred tool. It\u2019s more than just a user-agent switcher and offers the most comprehensive way to understand what Googlebot sees. Which SEO audits are useful for a Googlebot browser? The most common use case for a Googlebot browser is auditing websites that rely on client-side or dynamic rendering. It\u2019s a straightforward way to compare what Googlebot sees to what a general visitor sees, highlighting discrepancies that could impact your site\u2019s performance in search results. Given I recommend limiting the number of browser extensions to an essential few, it\u2019s also a more accurate test than an extension-loaded browser of how actual Chrome users experience a website, especially when using Chrome\u2019s inbuilt DevTools and Lighthouse for speed audits, for example. Even for websites that don\u2019t use dynamic rendering, you never know what you might find by spoofing Googlebot. In over eight years of auditing e-commerce websites , I\u2019m still surprised by the unique problems I encounter. What should you investigate during a Googlebot audit? Navigation differences: Is the main navigation consistent across user and bot views? Content visibility: Is Googlebot able to see the content you want indexed? JavaScript indexing delays: If the site depends on JavaScript rendering, will new content be indexed quickly enough to matter (e.g., for events or product launches)? Server response issues: Are URLs returning correct server responses? For instance, an incorrect URL might show a 200 OK for Googlebot but a 404 Not Found for visitors. Page layout variations: I\u2019ve often seen links display as blue text on a black background when spoofing Googlebot. It\u2019s machine-readable but far from user-friendly. If Googlebot can\u2019t render your site properly, it won\u2019t know what to prioritize. Geolocation-based redirects: Many websites redirect based on location. Since Googlebot crawls primarily from US IPs, it\u2019s important to verify how your site handles such requests. How detailed you go depends on the audit, but Chrome offers many built-in tools for technical SEO audits . For example, I often compare Console and Network tab data to identify discrepancies between general visitor views and Googlebot. This process catches files blocked for Googlebot or missing content that could otherwise go unnoticed. Never miss an issue impacting traffic on your site Find and fix technical SEO issues fast with Moz Pro. Run a free site audit How to set up your Googlebot browser Setting up a Googlebot browser takes about 30 minutes and makes it much easier to view webpages as Googlebot. Here\u2019s how to get started: Step 1: Download and install Chrome or Canary If Chrome isn\u2019t\u00a0your default browser, you can use it as your Googlebot browser. If Chrome\u00a0is\u00a0your default browser, download and install Chrome Canary instead. Canary is a development version of Chrome where Google tests new features. It runs separately from the default Chrome installation and is easily identified by its yellow icon, a nod to the canaries once used in mines to detect poisonous gases. While Canary is labeled \u201cunstable,\u201d I haven\u2019t encountered any issues using it as my Googlebot browser. In fact, it offers beta features that are useful for audits. If these features make it to Chrome, you\u2019ll be ahead of the curve and can impress your non-Canary-using colleagues. Step 2: Install browser extensions To optimize your Googlebot browser, I recommend intalling five crucial extensions and a bookmarklet to optimize my Googlebot browser. These tools emulate Googlebot and improve technical SEO audits , with three especially useful for JavaScript-heavy websites. Here\u2019s the breakdown: Extensions for emulating Googlebot: User-Agent Switcher : Switches the browser\u2019s user-agent to mimic Googlebot\u2019s behavior. Web Developer : Allows you to turn JavaScript on",
      "content_length": 3212,
      "category": "seo",
      "priority": "medium",
      "base_weight": 0.6,
      "raw_score": 0.65,
      "scrape_method": "requests"
    },
    {
      "content_id": "1be6d229448471e89f260e5649d9fa4c",
      "source_id": "moz",
      "source_name": "Moz Blog",
      "title": "The Future of AI in Search | Whiteboard Friday Revisited With Britney Muller",
      "url": "https://moz.com/blog/the-future-of-ai-in-search-whiteboard-friday-revisited-with-britney-muller",
      "author": "",
      "published_at": "2025-09-19T00:00:00",
      "fetched_at": "2025-10-17T17:30:36.715086",
      "content_type": "article",
      "content_text": "Table of Contents Britney Muller is on a mission to empower marketers through AI education and to shed light on the misinformation and hype surrounding AI. In this Whiteboard Friday Revisited, we look back at her 2020 episode on \" Accessible Machine Learning for SEOs .\" Join us as we revisit the past to understand the present and speculate on the future. From skepticism to adoption, the LLM takeover Five years after this video was created, the idea that machine learning might be \u201cfar outside the scope of your SEO work\u201d seems outrageous. But you can understand why it was positioned this way. Britney\u2019s early work in LLMs , machine learning, and SEO was initially met with scepticism. \"I remember people telling me I was insane and actually, like, very seriously criticizing me for making these sorts of leaps when I have been at the hackathons. I've been at these AI conferences. I know the current capabilities exist,\" but at the time, the concept was \"such a foreign concept\" to many people. As we enter the second half of 2025, three years after the ChatGPT general release, there is no shortage of AI marketing workflows , including on our very own Moz Blog. But even Britney admits she couldn\u2019t have anticipated that LLMs would be the model that would \"explode in the way that it has\" with the wide release and adoption of tools like ChatGPT. Instead, she thought that predictive models or data models focused on \"analysis and surfacing statistical insights\" would take off first, as those applications had \"incredible capabilities through other types of machine learning\" that are now being overlooked. Perhaps this just proves that predicting anything in this crazy world is nearly impossible. The real lesson? Stay informed so you can react in the most sensible way possible. One great way to do that is through Britney's Maven Course, Actionable AI for Marketers. The enduring value of human expertise Britney Muller advises that adding a \"human in the loop\" is crucial when using AI tools for anything, but especially content. This human touch involves refining and adding a layer of expertise and personality to the AI's output. She emphasizes that a person's unique expertise, real-world experiences, and anecdotes are things AI can never replicate. For a good example, Britney references Ed Zitron's blog as content that is both \"highly informative and entertaining\" with a \"unique voice\" and \"humor throughout that is so uniquely human.\" While this piece was published after recording this interview, it is a good example in tone and topic. Does all this sound familiar? That's because you know this already, from our good, old friend EEAT . How to use AI efficiently It all starts with clarity of thought and a mindset shift. Britney emphasizes that to use AI effectively, you have to be willing to embrace failure because it's the \"yellow brick road of using AI.\" The process is inherently experimental and should be treated as such. She advises thinking like an engineer, which means breaking down problems into small, manageable chunks and being specific in your prompts. Avoid the \"magic engine\" mentality where you provide a vague prompt and expect a perfect result. You can begin by documenting the tasks that take up a lot of your time or are painful to do. This helps you identify which workflows could be most effectively automated with AI. Once you've honed in on a task, select the right model for it. While generative content is what people often think of, Britney suggests it's actually one of AI's \"worst capabilities.\" Instead, explore using AI for more efficient tasks, such as: Data analysis: Analyze large datasets to surface unique insights. Automating mundane tasks: Use AI to identify qualified leads on social media. Building small tools: Use models like Claude to help create simple Chrome extensions. Running local models: Explore models like Llama, which can run directly on your computer to query against your own documents and create a mini RAG model. Actionable steps for marketers and SEOs \ud83d\udee0\ufe0f Because this is, after all, a marketing blog, I wouldn\u2019t leave you without a framework, so here it is: Reframe your approach: Don't get discouraged by AI's experimental nature and the failures that come with it. Embrace the fact that failure is the \"yellow brick road of using AI\". Start small and be specific: To avoid being overwhelmed, begin with small, simple tasks. The goal is to find one clear workflow that solves a problem for you. Learn to think like an engineer: To use AI effectively, you need to break down problems into individual, actionable chunks, as you would when instructing a person. I also like to think of this as being incredibly meta, or thinking of how you\u2019d explain this to a junior employee (sorry, job-seeking graduates \ud83d\ude1e). Go beyond content generation: While generative AI is good for some tasks, it's not well-suited for creating high-quality content. Explore other applications, like using AI to analyze data sets, check for grammar in emails, or set up automations to identify qualified leads on social media. Use the right tools: Not all AI models are the same. For instance, a text-to-image generator like Midjourney is different from the computer vision used by the post office to read addresses. You can even run your own local AI models for specific tasks on your computer. Document your workflows: As you experiment, document your processes. Tracking time-consuming tasks can help you find opportunities for automation. Separating AI hype from reality: The real mission According to Britney, you're not to be blamed for getting stuck in a hype-induced fear, hopelessness, urgency cycle. \"There's so much misinformation happening right now in terms of AI and the AI hype that circulates at an incredible speed and I like to shed light and give context around why these things are occurring and also give marketers peace of mind that they're not going to be replaced by AI tomorrow.\" The real tragedy is the misuse of models clouding the real benefits of AI by orchestrated LLM hype and the muddying of the world's information with \"AI slop.\" As marketers, we're perpetual optimizers, and the temptation to let a \"mountain of AI slop\" flow to \"keep up\" is a powerful one, but the return on this is likely short-lived and will result in diminishing returns. This brings us back to the recurring SEO is dead narrative. As many of us know, touted new SEO/GEO/etc methods are often just a rehash of existing, timeless strategies. The key is to be informed and proactive. A constructive path forward is possible without succumbing to the fear of being left behind. Written by Jo Cameron Jo is the Director of Content Marketing at Moz. Scale revenue from SEO with Moz Pro Get started with the all-in-one SEO tool marketers trust Start your free trial Get the latest SEO tips and strategies in your inbox Back to Top",
      "content_length": 1156,
      "category": "seo",
      "priority": "medium",
      "base_weight": 0.6,
      "raw_score": 0.65,
      "scrape_method": "requests"
    },
    {
      "content_id": "634f461321bb70f50e278ceaaf4b1aae",
      "source_id": "moz",
      "source_name": "Moz Blog",
      "title": "How I Used Vibe Coding to Build Custom SEO Tools (Without Writing Code!)",
      "url": "https://moz.com/blog/vibe-coding-to-build-seo-tools",
      "author": "",
      "published_at": "2025-10-16T00:00:00",
      "fetched_at": "2025-10-17T17:30:38.101515",
      "content_type": "article",
      "content_text": "Table of Contents I\u2019ve spent most of my career developing marketing ideas faster than I could implement them. Waiting on engineers, wrestling with confusing API docs, or shelving concepts that never made it out of the backlog became the norm. Sound familiar? It\u2019s a frustrating loop. That\u2019s when I started experimenting with vibe coding. With AI copilots and APIs, you can skip the bottlenecks and spin up a working prototype faster than you can finish your airport burger. I'm sharing my three-phase journey from a single Python script to a full web app using the Moz API. You'll also get my framework for prompting LLMs to build your own custom solutions. What vibe coding really means I define vibe coding as building real, working tools with the help of AI copilots. You don\u2019t need to know how to code. You just need a clear goal, an API, and a willingness to experiment. Here\u2019s what that looks like in my world: I bring the idea. I share the API docs and my token with a copilot like ChatGPT or Gemini. I explain what I want the tool to do, then iterate until it works. It\u2019s that simple. I\u2019m not sitting in front of a blank code editor anymore. I\u2019m collaborating with an AI that writes, debugs, and improves in real time. Vibe coding lets me move from concept to prototype in hours, not weeks. Why building tools used to suck For years, building tools felt impossible. I\u2019d come up with an idea, then wait weeks for engineers who were preoccupied with higher-priority projects. When I did get help, progress crawled. Everything depended on someone else\u2019s time. And even when I tried to take control, confusing API docs made it hard to start. The long dev cycles drained momentum. You\u2019d lose the spark before anything shipped. That\u2019s why vibe coding hit so hard. It finally gave marketers like me a way to build fast, keep the ideas alive, and let my coding colleagues focus their time on other revenue-driving initiatives How I got on the vibe coding path This whole thing started with a simple request. Chima, one of our content strategists, needed Brand Authority data for fifteen hundred domains for a Moz blog post. I had a Python script that could pull a single API request at a time, but running it 1,500 times would have taken days. I was sitting in the Montreal airport, pint of Guinness on one side, burger on the other, when I decided to fix it. What if I could turn that single script into a bulk analysis tool? That\u2019s when I jumped on the vibe coding train. I opened Google Colab, enlisted ChatGPT and Claude as copilots, and started rewriting my script. A few prompts later, I had a working bulk checker using the Moz API . It wasn\u2019t pretty, but it worked. And once I saw that first CSV file populate, I realized something big: I didn\u2019t need to wait for developers anymore. I could build my own tools - fast. From prototype to web app: my vibe coding journey Phase 1: Python scripts in Google Colab Once I had the idea, I opened Google Colab and started small. My goal was to take the Moz API and build a script that could handle bulk search intent requests instead of one at a time. I began with a single API call to test the connection. When that worked, I asked my copilot to expand the code for multiple keywords. At first, it was a mess - errors everywhere. I expected ChatGPT to read the docs for me and magically build the entire thing in one shot. It didn\u2019t. That\u2019s when I realized prompting isn\u2019t about delegating. It\u2019s a collaboration. I needed to provide context, explain the errors, and specify what I wanted fixed. Once I did that, the code started working. Within a few iterations, I had a working prototype that took a list of keywords, ran them through the Moz Search Intent endpoint , and exported the results as a CSV. That small script saved hours of manual work and became the foundation for everything I built after. And of course, the data assisted Chima with her request for Brand Authority metrics, which was then published in a Moz Blog post, \u201c Ziff Davis's Study Reveals That LLMs Favor High DA Websites. \u201d Here\u2019s what stuck with me: Start simple. A rough prototype beats a perfect idea. Be specific when debugging, and tell your copilot exactly what\u2019s going wrong. Read the API documentation. It\u2019s not glamorous, but it saves hours. Don\u2019t expect one-shot builds. Iteration is the real secret. That was my first taste of vibe coding. A working script, built in an airport, powered by curiosity and Guinness. Phase 2: Building a Google Sheets add-on The Python script worked, but sharing it was painful. No one on my team wanted to mess with Colab or code. So I opened Gemini and said, \u201cLet\u2019s turn this into a Google Sheets add-on.\u201d I wanted something anyone could use - plug in an API key, choose an endpoint, and pull data right into a sheet. Gemini asked about the layout, buttons, and error handling. After a few rounds, it clicked. By the end of the weekend, I had a working add-on. It pulled keyword metrics, saved API keys, and handled errors cleanly. The content team can use it without needing to touch the code. That build changed how I thought about tools: Usability matters more than clever code. LLMs work best when you talk to them like collaborators. The clearer your prompts, the faster you get results. From there, I knew the next step. If I could build in Sheets, I could build on the web. Phase 3: Creating a web app After the Google Sheets add-on, I wanted more freedom. Sheets worked, but it wasn\u2019t scalable. Everyone had to run their own version, and publishing through Google\u2019s extension store took forever. I wanted a web app anyone could access instantly. And the Moz API made that possible. I took everything I\u2019d already built in Sheets, fed the code into Gemini, and entered, \u201cHow can I create a working app using the Moz API in Gemini?\u201d That\u2019s when things got interesting. Gemini tried to turn me into a backend developer overnight. It assumed I knew how to spin up servers and handle CORS policies. News flash: I didn\u2019t. But with a little patience and a lot of debugging, we got there. The Moz API was the anchor. Its documentation was clean, the endpoints were stable, and it gave me the flexibility to test different datasets (rankings, search intent, Brand Authority) all from one interface. Within a few hours, I had a working web app that pulled live Moz data through my API key and displayed it beautifully. Then I started having fun. I added dark mode and a retro theme! I updated the ranking keywords endpoint so users could pull up to 500 results at once. I added a live quota counter that used the Moz API to track monthly usage. That build changed everything. The Moz API gave me the reliability to experiment fast, and vibe coding gave me the freedom to build without waiting for engineers. Here\u2019s what I took from it: A good API is the foundation of every great build. The Moz API\u2019s documentation makes it easy to start, even if you\u2019ve never written code. LLMs are powerful copilots, but you still need to guide them. Security and simplicity always win. That project proved it! Any marketer can turn an idea into a real tool when the data and documentation are on their side. Best practices for prompting and building with LLMs If there\u2019s one thing I\u2019ve learned through all this, it\u2019s that prompting is a skill. The way you talk to your copilot determines how well it can build with you. Once I stopped treating LLMs like magic buttons and started working with them like collaborators, everything clicked. Here\u2019s what\u2019s worked for me: 1. Define the end goal clearly. Avoid vague prompts. I break the task into sequential steps right in the prompt. I never try to one-shot the entire build. 2. Work iteratively, not all at once. I build the foundation (API calls) first, then the UI, then accessibility. This keeps the project manageable. 3. Debugging should be a conversation. I never say, \"Code doesn't run.\" I state the error clearly, explain the expected result, and ask the LLM to analyze the script to find the error. For example: \"It only returns 25 keywords. Can you analyze the script to figure out why it is not accepting the custom value in the API call?\" 4. Assign roles to the LLM. I give the LLM a specific persona and context. For example: \"You are a Next.js developer with experience deploying on Vercel. Help me build a front-end where the user can add their Moz API key.\" 5. Always ask for explanations. I never just accept a fix. I ask the LLM to explain how it solved the error so I can learn the underlying programming fundamentals. This prevents me from making the same mistake twice. Here\u2019s a helpful workflow you can follow ( check out the downloadable PDF here ): Prompts are the new superpower. The more you refine them, the more you realize you\u2019re designing the logic behind how your AI builds with you. What I learned from building it all This whole journey started with one script and a bit of curiosity. I built a Python prototype, turned it into a Google Sheets add-on, and finished with a web app powered by the Moz API. Prompting became my real skill. The more I practiced, the better I got at steering LLMs toward exactly what I wanted. And the Moz API made it easy to turn those prompts into something real and measurable. Here\u2019s the truth: marketers don\u2019t have to wait for developers anymore. You can build your own tools, test your ideas, and create something that moves your business forward. Vibe coding gives you that freedom. Grab your copilot, plug into the Moz API, and build the thing you\u2019ve been thinking about. You\u2019ll learn more in one weekend of building than in a month of planning. Written by Jonathan Berthold Currently the VP of Revenue at Moz, Jonathan Berthold is a seasoned SEO and paid media performance marketer with 14 years of experience focused on driving ROI through conversion rate optimization, product development, and creative problem-solving. Scale revenue from SEO with Moz Pro Get started with the all-in-one SEO tool marketers trust Start your free ",
      "content_length": 1791,
      "category": "seo",
      "priority": "medium",
      "base_weight": 0.6,
      "raw_score": 0.6900000000000001,
      "scrape_method": "requests"
    },
    {
      "content_id": "8256782eab437fe199a23cfe5263bee2",
      "source_id": "moz",
      "source_name": "Moz Blog",
      "title": "Stop Losing SEO Traffic: AI-Powered Strategies to Detect, Fix, and Drive [MozCon 2025 Speaker Series]",
      "url": "https://moz.com/blog/stop-losing-seo-traffic-mozcon-2025",
      "author": "",
      "published_at": "2025-09-18T00:00:00",
      "fetched_at": "2025-10-17T17:30:39.881739",
      "content_type": "article",
      "content_text": "Table of Contents I\u2019ve spent the past year fielding the same panicked question from clients repeatedly: \u201cWhy is traffic down, and what can we do about it?\u201d While AI features cannibalize clicks, there are other reasons for declining traffic. Technical issues, poor content targeting, and outdated strategies are often part of the problem. At MozCon New York, I\u2019ll show you how to use segmentation and machine learning to detect SEO issues, fix them, and build predictive recovery models to future-proof your organic traffic. The problem isn\u2019t traffic, it\u2019s visibility into what\u2019s broken SEO teams are under pressure to fix declining clicks, but you often work with incomplete data, vague KPIs, and no clear visibility into what\u2019s working. Many assume it\u2019s AI stealing clicks, but often, that\u2019s just one part of the story. Issues like tech debt, thin content, and poor targeting are major contributors. However, those problems stay hidden without proper diagnostics. MozCon is sold out! Join the waitlist to know when tickets are released Join the waitlist Here's what I'll cover in my MozCon presentation How to use machine learning to diagnose traffic loss I\u2019ll share a proven framework for traffic diagnostics using advanced segmentation and machine learning to find where it hurts and why. Fixing what\u2019s broken with targeted SEO strategies Once you\u2019ve identified the issues, I\u2019ll walk you through how to address them, whether it\u2019s tech debt, content issues, or strategy gaps. Building predictive recovery models to get buy-in I\u2019ll show you how to use\u00a0 AI to forecast potential gains from your fixes and make the business case to get buy-in for SEO. Learn how to use machine learning to detect and fix traffic decline only at MozCon New York Join the waitlist Who is this talk for? This session is for senior SEOs, program leads, and digital heads who are expected to drive results but lack clear insight into what's affecting traffic decline. If you\u2019re under pressure to prove ROI or figure out what\u2019s going wrong, this talk is for you. \ud83c\udf9f\u00a0Don\u2019t miss it Join me at MozCon New York to learn how to use AI to fix SEO issues and forecast the impact of your work. MozCon is sold out! Join the waitlist to know when tickets are released Join the waitlist The author's views are entirely their own (excluding the unlikely event of hypnosis) and may not always reflect the views of Moz. Written by Sam Torres Sam is the Chief Digital Officer for The Gray Dot Company. Scale revenue from SEO with Moz Pro Get started with the all-in-one SEO tool marketers trust Start your free trial Get the latest SEO tips and strategies in your inbox Back to Top",
      "content_length": 448,
      "category": "seo",
      "priority": "medium",
      "base_weight": 0.6,
      "raw_score": 0.62,
      "scrape_method": "requests"
    },
    {
      "content_id": "41f8bc65f61d6abc36f668713934b9aa",
      "source_id": "moz",
      "source_name": "Moz Blog",
      "title": "How To Diversify Your Traffic Outside of Google SERPS",
      "url": "https://moz.com/blog/diversify-traffic-mozcon-2025",
      "author": "",
      "published_at": "2025-09-23T00:00:00",
      "fetched_at": "2025-10-17T17:30:41.617158",
      "content_type": "article",
      "content_text": "Table of Contents Too many marketers are overdependent on SERPs for traffic, creating a dangerous single point of failure. With shrinking SERPs and volatile rankings, organic traffic is no longer reliable, especially for smaller brands trying to grow. At MozCon New York, I\u2019ll share how to use digital PR and brand-led strategies to diversify your traffic sources, increase branded search demand, and create lasting visibility beyond Google. The SERPs aren\u2019t giving you the same returns they used to About two years ago, I started noticing a troubling pattern. We were working with smaller brands following SEO best practices to the letter, but despite doing everything right, they weren\u2019t gaining traction in SERPs. That\u2019s when we started thinking differently about digital PR, not as a way to build thought leadership or land backlinks but as a demand-gen channel. We revamped our digital PR strategy to drive demand for branded searches tied to solutions our ideal customer sought. This allowed us to diversify our traffic sources and increase conversion. MozCon is sold out! Join the waitlist to know when tickets are released Join the waitlist Here\u2019s what I\u2019ll cover in my MozCon New York presentation How to build a brand-led content strategy that drives demand outside search You\u2019ll learn to move beyond traditional pillar-based models and create problem-solution content for specific audience segments. I\u2019ll show you how to repurpose content across channels like TikTok, YouTube, newsletters, and communities to increase visibility outside SERPs. How to use digital PR to increase branded search I\u2019ll show you how to use digital PR as a growth lever to increase branded search, build trust signals, and drive traffic that algorithms reward, even when rankings fluctuate. How to build a distribution model that supports SEO and conversion I\u2019ll explain how to build a content distribution engine that aligns with your SEO and revenue goals, so your best work keeps delivering results long after it\u2019s published. Get actionable insights to fix declining traffic at MozCon New York Join the waitlist Who is this talk for? This talk is for Digital PR professionals and growth marketers who want to build traffic resilience. If you're tired of seeing diminishing returns from organic traffic, this session will help you create a diversified strategy. \ud83c\udf9f\u00a0Don\u2019t miss it Join me at MozCon New York to learn how to diversify your traffic sources and increase demand for your products and services with digital PR. MozCon is sold out! Join the waitlist to know when tickets are released Join the waitlist The author's views are entirely their own (excluding the unlikely event of hypnosis) and may not always reflect the views of Moz. Written by Misty Larkins Misty Larkins is a strategic communications professional with nearly 20 years of experience driving brand visibility and organizational trust through integrated public relations, internal communications, and thought leadership. Scale revenue from SEO with Moz Pro Get started with the all-in-one SEO tool marketers trust Start your free trial Get the latest SEO tips and strategies in your inbox Back to Top",
      "content_length": 504,
      "category": "seo",
      "priority": "medium",
      "base_weight": 0.6,
      "raw_score": 0.635,
      "scrape_method": "requests"
    },
    {
      "content_id": "d0635216ac3d34abcc7cee582a0148a1",
      "source_id": "moz",
      "source_name": "Moz Blog",
      "title": "How I Found Internal Linking Opportunities With Vector Embeddings",
      "url": "https://moz.com/blog/internal-linking-opportunities-with-vector-embeddings",
      "author": "",
      "published_at": "2025-09-30T00:00:00",
      "fetched_at": "2025-10-17T17:30:43.524050",
      "content_type": "article",
      "content_text": "Table of Contents I felt overwhelmed when I first read Mike King\u2019s article on vector embeddings . The concepts seemed complex, and implementing them for SEO was intimidating. But with Screaming Frog\u2019s new features and Gus Pelogia\u2019s excellent guide , I saw the potential to improve internal link building using this method. Based on the two resources above, I decided to create a detailed, step-by-step guide to make this process more approachable, even for people unfamiliar with Python or vector embeddings. In this article, I\u2019ll walk you through how I used vector embeddings to identify internal linking opportunities at scale so you can confidently apply these techniques to your SEO strategy. What you\u2019ll need to get started To carry out this process, I used the following: Screaming Frog OpenAI API Key Google Sheets or Excel By the end, I had a comprehensive spreadsheet that included: Every important URL from my site listed in column A (target URL) The URL of every page that links to the target URL (excluding navigation) URLs to the top 5 most closely related pages based on cosine similarity Opportunities where one or more of those 5 URLs are not linking to the target URL Example Spreadsheet This is the example I used in the screenshots below, and it will look something like this: Pink cells indicate where the related page doesn\u2019t link to the target page. Step 1: Get an OpenAI API key I started by heading over to OpenAI\u2019s website , clicked the button to create a new secret key and copied that API key to use in Screaming Frog. Step 2: Set up and Run Screaming Frog For convenience, I have saved the Screaming Frog custom configuration profile. Download it here . In Screaming Frog, open a custom configuration file by using: Go to File > Open and select the .seospider file you want to load. Here is the explainer video . After loading the configuration file, I opened the API tab, selected OpenAI, and pasted in my API key. Once connected, I switched over to the \u201cPrompt Configuration\u201d tab in the same window and clicked \u201c+ Add from Library\u201d to select the \u201cExtract embeddings from page content\u201d prompt. Note : The API Access feature above pulls embeddings into the API report. However, the script below requires the much smaller file generated by the Custom JavaScript function. Next, I opened Screaming Frog and followed these steps: Navigated to Configuration > Custom > Custom JavaScript. Clicked \u201cAdd from Library\u201d and selected \u201c(ChatGPT) Extract embeddings from page content.\u201d This allowed Screaming Frog to extract the data needed for the internal link audit. I edited the custom JavaScript code to include my OpenAI API Key. Then, I pasted the API Key I generated in Step 1 into the appropriate section of the code. NOTE : You will have to do this even when using the custom configuration profile linked above. I ran a quick test on a URL from my target site. When I saw numbers populate in the \u201cCustom Extraction\u201d tab, I knew the setup was working correctly. NOTE: Troubleshooting insufficient_quota errors for ChatGPT in your Screaming Frog results (provided by Tory Gray ) 1: Ensure you have a paid ChatGPT account This process won't work without a paid account! - Log in and manage your paid subscription here . 2: Ensure you have appropriate ChatGPT API budget available - Log in to your OpenAI account here . Set monthly API usage limits, as well as budget alerts here (scroll past Rate Limits section). How much do you need? A one-time budget of $10 would roughly cover a small site with 100 pages and blog posts averaging 3200 words. You can view existing API usage levels here (e.g. to ensure you aren't already over the limit). Step 3: Export vector embeddings and all inlinks Export All Internal Links from Screaming Frog I saved the crawl once it was completed. I exported the \u201cAll Inlinks\u201d data from Screaming Frog. This file contains every internal link on the site and can be quite large. For example, my file, all_inlinks.csv, was around 52 MB and represented 1,428 URLs. Export vector embeddings from Screaming Frog I exported the Custom Javascript results where the vector embeddings are located. Step 4: Run both files through the Cleanup & Formatting Script I opened this convenient Python script on Google Collab, which was created by Britney Muller , and made a copy to use. I pressed the \u201cPlay button\u201d Scrolled down to \u201cChoose Files\" Uploaded my first CSV file (all_inlinks.csv) and let it process. Uploaded my second file (custom_javascript_all.csv) when prompted. Accepted the Save File messages when they were done processing. You should get an XLSX version and a CSV version. Note : The original version of this guide was about three times as long, comprised mostly of formatting and data cleanup. Britney\u2019s script automates ALL of this, saving you several hours of tedious work. Troubleshooting errors Sometimes, an issue pops up that keeps the script from working. But don\u2019t worry, clicking \u201cExplain Error\u201d will typically guide you to the fix. The explanation of the error helped me figure out that I needed to open the CSV file and look for irregularities in the Embeddings column. It turned out there was a blank cell. Other examples of what might cause errors during this phase are: Extra columns The wrong file name The wrong column names For example, I encountered a blank cell in the \u201cEmbeddings\u201d column that caused an error. I simply deleted that row, exported the cleaned file as file.csv again, refreshed the Google Colab notebook, and retried. Step 5: Import or Open the Saved File I opened a new Google Sheet and imported the XLSX file. The script also outputs a CSV, but I prefer the XLSX version because it preserves conditional formatting. If you\u2019re working from Excel or another spreadsheet program on your computer, simply open the XLSX file. Validate the data I double-checked a few entries manually to ensure everything was accurate. Now, I have a complete list that shows each target URL in column A, the top 5 related URLs, and whether those URLs are linking back to the target URL. My final spreadsheet looked like this, with \u201cExists\u201d or \u201cNot Found\u201d indicating whether each related URL was linking back to the target URL: Step 6: Build internal links Now comes the final and most actionable part \u2014 building those internal links. Identify the opportunities: I used the pink cells as indicators of where internal links were missing. Each pink cell represented a related page that wasn\u2019t linking to the target URL, even though it should. Add the links: I went to each related page (from the pink cells) and edited the content to include a relevant internal link to the target URL. I made sure to use a descriptive anchor text that aligns with the content on the target page. Prioritize: I started with the highest-priority pages first, such as those with the most traffic . Concluding thoughts: Create a cohesive internal linking structure with vector embeddings Take the time to build, analyze, and refine your internal link structure. This step-by-step guide transformed my internal linking process into a data-driven strategy with the power of vector embeddings. The effort will pay off in improved rankings, better user experience, and ultimately, more organic traffic. It also improves SEO performance by ensuring your most valuable pages are connected in a way that search engines and your users understand. After running this process on a client\u2019s site, I was surprised. I thought we\u2019d done a great job at internal linking, but there were hundreds of opportunities we\u2019d missed. And I don\u2019t just mean that the keyword we want to link from appears on the page. I mean opportunities to link pages that search engines would see as highly relevant to each other. In doing so, I was able to disambiguate closely related concepts and fix a few unnoticed keyword cannibalization issues as well. Links to templates and resources Example Spreadsheet (This is the example used in all of the screenshots above). Screaming Frog Custom Configuration File (saves time from the Screaming Frog setup recommendations) Python Script on Google Colab to clean up Screaming Frog export by Britney Muller Python Script on Google Colab to process vector embedding output from Screaming Frog by Gus Pelogia Vector Embeddings is All You Need: SEO Use Cases for Vectorizing the Web with Screaming Frog by Mike King How to use Screaming Frog + ChatGPT to map related pages at scale by Gus Pelogia Internal Linking Guide for SEO with Google Colab (Python) by Anna P\u00e9rez Google Colab for SEO: How to get started by John McAlpin What You Should Know About LLMs (Whiteboard Friday) by Robin Lord Intro to Python (Whiteboard Friday) by Britney Muller Intro to Python for AI (Hands-On Workshop) by Britney Muller Actionable AI For Marketers Course on Maven by Britney Muller The author's views are entirely their own (excluding the unlikely event of hypnosis) and may not always reflect the views of Moz. Written by Everett Sizemore Everett is an SEO and content strategy consultant with over 16 years of experience optimizing websites for search. For the last 6 years, he has focused on helping startups build the teams and processes that form the foundation of highly effective search engine optimized content marketing machines. Scale revenue from SEO with Moz Pro Get started with the all-in-one SEO tool marketers trust Start your free trial Get the latest SEO tips and strategies in your inbox Back to Top",
      "content_length": 1597,
      "category": "seo",
      "priority": "medium",
      "base_weight": 0.6,
      "raw_score": 0.65,
      "scrape_method": "requests"
    },
    {
      "content_id": "581a8ccbfb919547ab223db44dc58bcc",
      "source_id": "moz",
      "source_name": "Moz Blog",
      "title": "How to Analyze Behavioral Data for Search \u2014 Whiteboard Friday",
      "url": "https://moz.com/blog/behavioral-data-for-search-whiteboard-friday",
      "author": "",
      "published_at": "2025-09-26T00:00:00",
      "fetched_at": "2025-10-17T17:30:45.521497",
      "content_type": "article",
      "content_text": "Table of Contents Learn how to analyze behavioral data to improve your search performance. Discover the three levels of diagnostic tools \u2014 from basic GSC data to advanced neuromarketing metrics \u2014 and how to use them to optimize the entire search journey. Click on the whiteboard image above to open a high-resolution version! Hi, my name is Giulia Panozzo, and I\u2019m a neuroscientist turned marketer, and today I want to talk about the behavioral data that matter for search. And why behavioral data? Because search has changed dramatically. Not only search as we know it, with the introduction of AI overviews, organic product carousels, and other features that have impacted both informational and transactional queries in the past 12 months, but\u00a0search behaviour has changed too. Users are already making searches that are more conversational and their search journey now spans across different channels , including socials and LLMs, and it is estimated that by 2026, traditional search engine volume will drop 25%, with search marketing losing market share to AI chatbots and other virtual agents according to Gartner.com. Search is not longer a linear journey SEO is now getting evaluated on journeys that are no longer linear and are driven by the user. And SEOs have always been shy talking about user behavior and relegated it to the UX teams because it\u2019s not an official ranking signal, however, some recent data coming from the Google doc leak and Mark Williams-Cook\u2019s research have highlighted the role of user signals in ranking and the importance of nailing user intent to benefit the overall evaluation of a website, so it\u2019s time that we acknowledge that optimizing for search now includes more than just getting a click to your website , but encompasses the entire journey, something that is now referred to as SXO , the intersection of SEO, UX and CRO (something that Sara Fernandez often writes about). What all these disciplines have in common is the user as the end beneficiary of our optimization efforts. So SEO as we know it might be dead, but the future of search is analysing and predicting user behaviour in order to optimise accordingly. And when we talk about users, we are talking about humans who make decisions all the time and are very often biased. Familiarizing with what these biases are is important for everyone working in marketing, but in order to understand and influence users\u2019 behavior, it really all comes down to understanding and mastering two main dimensions: Getting Attention (to stand out in a sea of potential options) Fostering a Connection (so that users keep coming back to you) Provided, of course, that what you have to offer is relevant to their search. That\u2019s why we need to include the study of other measures than just the traditional SEO ones. The new data we need to take into account spans across the entire search experience and multiple touchpoints and include behavioral data. The doctor analogy Looking at behavioral data to inform your search strategy is what I imagine a doctor needs to do when examining a patient: You listen to complaints and symptoms You analyze data to diagnose the root cause You prescribe a treatment Analyze the symptoms The symptoms are the easy ones to start with, because they are relatively easy to spot and are usually quite uncomfortable to deal with from a business perspective, so they\u2019re the ones that your stakeholders will care the most about and will bring to your attention. These might be: Loss in traffic/low clicks to a site, lower impressions, lower average order volume or conversions. These are generally just an outer manifestation of something that might be wrong on the inside, so you\u2019ll need to dig a little deeper. Diagnose the root cause When it comes to analyzing the root cause , we have several diagnostic tools we can use, and they reside on three different levels of data that we can get: the basic behavioral data, the next level data, and the predictive data. Let\u2019s get into each one of these. 1. Basic data The basic data comes from tools that you don\u2019t need buy-in or set up for. One of them is Google Search Console (GSC), which can reveal poor intent match when we look at CTR both from a branded and non-branded perspective. Most of the other data in this bucket is qualitative and allows us to identify common points of frustration both for a pre- and post-purchase journey, like surveys, CX logs, social mentions, and reviews, so make sure you collaborate cross-functionally to have access to what users are asking of you. There\u2019s also\u00a0live testing, which is the most time-consuming option, but also potentially one of the most rewarding since there\u2019s not much to infer. 2. Next level data Next level data is primarily quantitative and can be obtained with tools that need tracking set up, like web analytics and heatmaps, which record user behavior that might be less explicit because they are not actively communicating their frustrations, so there is a certain level of inference we have to apply to our findings. From web analytics, you can look for examples related to engagement time and engaged sessions or bounce rates and points of abandonment. Interaction heatmapping tools can integrate that information and everything that we've seen from web analytics, uncovering not only areas that might not be getting enough attention but also elements that don\u2019t actually work (via dead clicks, rage clicks or error clicks for example). In general, while we can infer why some journeys get cut short or don\u2019t end as we expect via these tracking tools, I always recommend to pair it with qualitative data to really understand what is going on. 3. Predictive data Finally, there\u2019s the predictive data . This is the hardest to get, because it relies on special equipment and training to properly interpret it, but this is something that uncovers preferences and behavior that not even the user might be aware they have. For example, eye-tracking goes a level beyond heatmapping data and can show us attentional patterns and areas that are missed. This can inform the design of pages, which is important as attention is a precious commodity in the land of 24/7 stimuli. On the other hand, electrodermal activity, EEG and fMRI measure neural activation in response to marketing stimuli can help us understand and predict preferences in content even before the user is aware of them. Prioritize and Treat So, now that we know all of the diagnostic and predictive tools available, it\u2019s time to plan your treatment. Depending on the size of the business, there might be an element of collaboration and prioritization needed, so to facilitate it, ask yourself these questions: What\u2019s the time/effort involved in this fix? How critical is this fix to the success of the business? This fix resolves a blocker to navigation or conversion = urgent This fix is a nice-to-have feature = not urgent What\u2019s the impact or ROI of my fixing this issue on the wider business? This will inform how to populate a prioritization matrix, where everything that has high impact will get done either now or in the near future, and everything that has low impact will either get postponed or discarded. Bonus tip: Document your fixes and what they solve for Anytime you implement a fix, make sure you always record what problem it solves at a deeper level, so you can identify cross-domain opportunities for improvement. For example, if we see lots of searches but poor CTRs for return queries, fixes can include making return policies more available both on-page and on the Merchant Centre. If we look at the user need and the underlying bias that this solves for, it\u2019s the need to avoid losses, which means we can proactively address this need on other areas, like for example in the pre-sales messaging (e.g. \u201cfree trial,\u201d \u201cno credit card required,\u201d which conveys to the user you save both time and money). To sum it up It is now our duty as search professionals to take into account behavioral data. You don\u2019t need to be a UX professional to investigate them, and your title shouldn\u2019t be an excuse not to deliver better content or products to your audience. As SEOs, our job doesn\u2019t end when we make them land on site, but it continues throughout the entire journey to make sure that user interaction is a positive one and doesn\u2019t end in abandonment. The author's views are entirely their own (excluding the unlikely event of hypnosis) and may not always reflect the views of Moz. Written by Giulia Panozzo Giulia Panozzo is a neuroscientist turned SEO (through a few detours into the world of professional ice skating). She loves delving into new ways to explore the link between the human mind and marketing, and talking about it all. Scale revenue from SEO with Moz Pro Get started with the all-in-one SEO tool marketers trust Start your free trial Get the latest SEO tips and strategies in your inbox Back to Top",
      "content_length": 1515,
      "category": "seo",
      "priority": "medium",
      "base_weight": 0.6,
      "raw_score": 0.65,
      "scrape_method": "requests"
    },
    {
      "content_id": "a5e3bd73dd69e6391af6e64dfce95c3f",
      "source_id": "moz",
      "source_name": "Moz Blog",
      "title": "Google &num= and Changing SERP lengths in Moz Pro",
      "url": "https://moz.com/blog/num-100-and-changing-serp-lengths-in-moz-pro",
      "author": "",
      "published_at": "2025-10-14T00:00:00",
      "fetched_at": "2025-10-17T17:30:46.740541",
      "content_type": "article",
      "content_text": "Table of Contents September 2025 saw some major disruptions in the SEO tools and data industry with the removal of a widely used Google parameter\u00a0&num= . We\u2019ve been able to deliver consistent and unaffected data in Moz Pro and the Moz API so far, thanks to prior preparation and the resilience of our systems. That said, we wanted to share our thoughts, and our plan going forward. What happened? Virtually every SEO tool is powered by scraped search results. In order to get more insight from each scrape\u2014and thus keep costs down for end users\u2014tools generally use this parameter to force extended search results on the first page, instead of the typically 10 regular organic results a user would see by default. Moz Pro, for example, has for a long time standardized on &num=50 \u2013 so 5 \u201cpages\u201d of results per scrape. This parameter had actually been deprecated for many years, but continued to be unofficially supported. In mid-September, it started to slowly stop working, forcing SEO tools to seek alternative methods. Some tools\u2013including Moz and STAT\u2013prepared an alternative we call \u201cstitching\u201d, where we piece together a series of paginated results, 10 at a time, into one longer set of results. There are various difficulties with this approach, many of which can be mitigated or avoided, but the main implication is cost, which ends up being significantly higher, to the point of being unsustainable in many cases. This should also be seen in the context of SERP data costs generally increasing in recent years, as tools are forced to mimic real browsers more and more closely in order to get accurate, representative rankings. Are lower rankings valuable? The rankings that SEOs should care most about are clearly those on the first 1-2 pages of results. Very few users are scrolling down to position 40. That said, deeper SEO analysis and metrics do benefit greatly from the information gained from these deeper results. This includes tools like Keyword Explorer and our link index. Plans for Moz products We want to strike a balance between providing a good value, accessible product, and providing the best data possible to our users. As such, for Moz Pro and Moz APIs, we are taking a hybrid approach, starting from 20th October, 2025: Campaign rank tracking and Rank Checker, will report on the first two pages of results only - typically 20 regular organic positions, or 25-30 positions including features. You can read more here about how this will show up in your dashboards and reports. Keyword Explorer, Link Explorer, being primarily analytical data rather than rank tracking, will remain on a 50-result SERP as now. For STAT, we are pursuing a more flexible approach, which will provide clients with a range of options to suit their diverse needs. Why has Google done this? Many have speculated that this might be an attempt by Google to sabotage ChatGPT, which is powered to a large degree by scraped Google search results. This explanation is appealing but has a couple of issues: Does restricting ChatGPT to the first 10 results meaningfully harm its responses? Won\u2019t ChatGPT now just hit Google harder to get more results, thus incurring higher costs for both parties? Perhaps Google is happy to accept a war of attrition on this front. Another likely explanation is that this long-deprecated parameter simply stopped working as part of another update. Perhaps the same update that seems to have changed how impressions are measured in Google Search Console in the last few weeks (we don\u2019t think bots can have been driving enough traffic per term to drive the impression changes that many sites are seeing). What next? We\u2019ll continue to strive to provide reliable, consistent, and accurate data to our customers, as we have done throughout this period. If you are a Moz Pro customer and have any questions, feel free to contact support . Catch us at the STAT Booth at Brighton SEO, UK Come chat with the Moz and STAT team at Brighton SEO this month. I\u2019ll also be speaking on Friday afternoon , swing by and say hi! The author's views are entirely their own (excluding the unlikely event of hypnosis) and may not always reflect the views of Moz. Written by Tom Capper I head up the Search Science team at Moz, working on Moz's next generation of tools, insights, and products. Scale revenue from SEO with Moz Pro Get started with the all-in-one SEO tool marketers trust Start your free trial Get the latest SEO tips and strategies in your inbox Back to Top",
      "content_length": 762,
      "category": "seo",
      "priority": "medium",
      "base_weight": 0.6,
      "raw_score": 0.675,
      "scrape_method": "requests"
    },
    {
      "content_id": "bf23612a82db494fddb460d8159223b0",
      "source_id": "moz",
      "source_name": "Moz Blog",
      "title": "How to Communicate Site Migration to Clients \u2014 Whiteboard Friday",
      "url": "https://moz.com/blog/how-to-communicate-site-migration-to-clients-whiteboard-friday",
      "author": "",
      "published_at": "2025-10-10T00:00:00",
      "fetched_at": "2025-10-17T17:30:47.919478",
      "content_type": "article",
      "content_text": "Table of Contents Site migrations can be tricky! Rebecca Yu shares essential strategies for SEO professionals to clearly explain the process and potential impacts to non-SEO clients. Learn how to bridge the communication gap, accurately identify when a migration is happening, and ensure a smoother transition for everyone. Click on the whiteboard image above to open a high-resolution version! Hi, everyone. I'm Rebecca Yu, an SEO manager from Jakala . Today, I'm going to walk you through how to communicate SEO challenges with your client. Often, website migration is one of the biggest SEO challenges an SEO can have. And frankly speaking, the technical part isn't the most difficult thing. It is communicating with a client about migration issues and how to plan around it. Often, SEOs are informed about the site migration way too late, sometimes just days before a launch. So, in order to avoid all this chaos and try to rescue our sanity, it is very important to understand site migration in non-SEO languages because here it comes, like site migration in SEO terms. What is a site migration in SEO terms? So we have all the domain migration, platform migration, and structure migration. We can talk the whole day about it, like what are the domain name changes, how to move to a new CMS, blah, blah, blah. But then, this is not the language that clients understand. They will be like, \"What is a CMS migration?\" Okay, you're moving home in the backend then. How do we translate site migration details into non-SEO terms? So here comes the site migration in non-SEO terms because this is what the clients understand . For example, rebranding to a new name. So we are rebranding. So when an SEO, you kind of catch these hints from migration, you need to understand what's going on behind the website. So, I am going to go through a couple of them. So rebranding to a new website. You're trying to add to a new country's website. That's obviously international SEO, and moving to a new website, that's obvious. And we have adding to a new CMS, another obvious one. What's not so obvious is upgrading to a new CMS version. I think this is the one that a lot of us will miss. A new CMS version might not have all the necessary fields as the old CMS. So that's why, as an SEO, you want to make sure all the data is correctly transferred from the old version to the new version. I think this is one of the most tricky ones. Identify and fix critical redirect issues with Moz Pro Site Crawl Fix my redirect issues Another one is changing the menu and navigation. You might start to question, \"It's just changing the menu and navigation. Why does it sound like it's moving home? You're moving from one site to another. No, I'm just trying to change the menu.\" Well, actually, because changing the menu structure might involve all sorts of link changes, from taking away the whole section to adding a whole new batch of links to the menu. So that's why these are all the major changes to the website. I would say when working through all these hints for SEO migration, what you can think of here is, \"What are the major website changes that can impact SEO?\" Because sometimes the client might not know it would be your website migration. So, the best thing you can do is, when communicating regularly in your client meetings, you try to catch these hints so that you're able to identify a potential migration early on. These are the SEO migrations in human languages, and these are the languages that your client may speak. Then you can try to educate them back, \"Hey, these are the kind of stuff that actually needs to be done.\" Questions to ask about the migration So, once you realize there is some migration going on, here are some questions to ask. So first, a timeline. When is the launch date? That's the most important thing. And how much time do you have to prepare for the site migration? You will need to ask the reason for migration. So is it for marketing? Is it for rebranding? Or sometimes, some clients might just want to do it for compliance reasons. They're separating two countries' entities, so the website has to separate as well. But that's not quite SEO oriented, I would say. And after that, we will have stakeholders. So, who is leading the site migration, and what teams are involved? Usually, you will have the tech team, of course, who carry out the migration steps, and then the UI/UX team, the marketing team, the brand communication team, and so on. So, know your stakeholders and then you will be able to understand what are the ways of working between these parties. Are you meeting regularly? Are there any weekly or biweekly meetings ? Are you communicating through emails? Is there any ticketing platform, let's say Jira or ClickUp, to document the changes? And then you are able to store the information and your requirements and communicate back to the tech team. Identify and fix critical crawler issues with Moz Pro Site Crawl Fix crawler issues Last but not least, do you have direct access to developers ? Because I would say that's one of the most important factors for the success of a site migration, because if SEO requirements , they try to travel through like marketing, branding, corporate communications team, and then finally arrive to the tech team, sometimes things will be lost in translation. The site migration process So after the questions to ask, we have the site migration process that involves planning, preparation, testing, launch, and monitoring. I would say in the early stage it will be the planning and preparation time. So if you're involved in the site migration in the early stage, you have all the time to do the site analysis and try to provide SEO enhancement. So you do what you need. Identify and fix critical status code errors with Moz Pro Site Crawl Fix HTTP errors But if you're involved in let's say the testing phase or nearly phase before launch, you still have a chance to do a good migration. But I think the focus here is to avoid major SEO mistakes in the website. If you're very unlucky to be involved in a site migration project after launch, all I can say is it's not your fault. It happens. What you can do is try to focus on the remedial actions. So what are the quick fixes? How are the top 20 pages performing, and try to do the fixes for these pages to avoid all the traffic loss. So yeah, that's it. So I hope it was helpful. I hope everyone has a smooth website migration. And you can follow me on my LinkedIn, Rebecca Yu . And have a good day. The author's views are entirely their own (excluding the unlikely event of hypnosis) and may not always reflect the views of Moz. Written by Rebecca Yu Curious about how people find, think, and choose. Scale revenue from SEO with Moz Pro Get started with the all-in-one SEO tool marketers trust Start your free trial Get the latest SEO tips and strategies in your inbox Back to Top",
      "content_length": 1237,
      "category": "seo",
      "priority": "medium",
      "base_weight": 0.6,
      "raw_score": 0.6900000000000001,
      "scrape_method": "requests"
    },
    {
      "content_id": "4758b5a71132946664b8fba86b5e9da5",
      "source_id": "moz",
      "source_name": "Moz Blog",
      "title": "What is Index Bloat? \u2014 Whiteboard Friday",
      "url": "https://moz.com/blog/what-is-index-bloat-whiteboard-friday",
      "author": "",
      "published_at": "2025-10-17T00:00:00",
      "fetched_at": "2025-10-17T17:30:49.183204",
      "content_type": "article",
      "content_text": "Table of Contents Deep dive into index bloat - a critical SEO challenge affecting medium to large websites. Learn how to identify URLs consuming your index quota without delivering traffic, understand the difference between crawl budget and index bloat, and discover practical solutions for cleanup. This Whiteboard Friday video helps you assess your site's index health and implement effective remediation steps, from content consolidation to proper URL handling. Click on the whiteboard image above to open a high-resolution version! Happy Friday, Moz fans. Today I want to talk about index bloat. So this is a pretty common problem affecting especially large, but also sometimes medium-sized sites. And I'd say this is definitely something that you should have looked into if you work for a medium or a larger site. It's definitely something you should have looked into at least once. It does affect a lot of sites. It's worth checking whether this might affect you. This is something that I and a lot of other SEOs have seen very good results with both for a long time and very recently. And despite that, it's something that I think is relatively poorly codified and talked about in the industry, there are some reasons for that which I'll come on to in a moment. Understanding index bloat But before we get into all of that, I just want to explain this. So I put this diagram in just to give a bit of context, so that what I say next makes sense. So this outer box, this diagram as a whole is all of the URLs on your site, all of the URLs that might exist, that could exist, including parameters that no one has tried before, this kind of thing, the maximum possible set of URLs that would return a 200 response code and a valid page. And then I've got smaller sets within that, sort of subsets of URLs. So the next one down is Google discovered URLs. So if Google has seen the URL \u2013 they might have not crawled it, they might have not indexed it, but they've seen the URL, they know it exists. That's sort of your next step down. And if you've got a big difference between the red box and the blue box, that probably indicates some kind of crawl budget problem. But that's not what we're talking about today. If you've got a URL that's discovered, it might not necessarily be indexed. So indexed URLs is another smaller set. If you've got a URL that's discovered but not indexed, again there might be some reasons for that. Google might suspect that the page is unimportant based on other signals. You might have said not to index it. You might have shown them a noindex tag or something like this. So that, again, is a smaller set. Again, we're not necessarily speaking about that gap today. And then you've got indexed versus pages with non-trivial traffic. Now what counts as non-trivial traffic to a page might vary from site to site. You might have your own idea of this. But a big gap between the number of URLs that are indexed and the number of URLs that are getting any kind of meaningful non-zero traffic, if that's a big gap, that would suggest an index bloat problem, and that's what I want to talk about today. Never miss an issue impacting traffic on your site Find and fix technical SEO issues fast with Moz Pro. Run a free site audit What index bloat is not So before I get into that, just to make it totally clear, I want to quickly disambiguate a couple of things I mentioned there. So I'm not talking about crawl budget . As I mentioned before, that's when you've got a lot of URLs that Google just isn't going to crawl at all. Perhaps you're producing them too quickly. You've got a very large number, a huge number of URLs on your site. This might affect news websites, for example, sometimes large forums. I'm also not talking about cannibalization . Now that is a related concept. Often when you've got a huge number of indexed pages that aren't getting traffic, it's because their topics are too similar. But you could theoretically have a cannibalization problem on a site with three pages, if they're all about roughly the same thing. That's not really what I'm talking about today. I'm talking about a larger-scale problem. So we're specifically talking about the difference between the yellow and green boxes I talked about earlier. So how many of all the URLs that are indexed, are there a large number that Google is not really bothering to send any meaningful traffic to or show up in search results? Why is index bloat an issue? Why do we care? Why is that a problem? So what? I've got lots of indexed pages that don't get any traffic. What's the big issue? Well, the first thing is that we have to theorize about how Google sort of treats these and why it behaves the way it does, and why we see the results we do. This is mostly based on experience within the industry. It's not something that Google has ever sort of codified for us. But we suspect that if you have a lot of pages that are receiving no traffic, that sends a quality signal , which might reflect on your site or certainly parts of your site as a whole. So if you have a huge number of pages that either are very thin, they've got nothing on them, or when you click through to them, they don't answer the question, they're a bit pointless, and you go back to search results, that could reflect on your entire site. So that could be part of the reason we care here. The other is that, as I just mentioned, cannibalization, but also some other technical SEO problems. This can be symptomatic of other issues. If you're generating these large numbers of URLs on your site that are getting indexed, if we think about sort of Page Rank in a very old-fashioned SEO way, that's creating a lot of loss as Google sort of dilutes Page Rank across all of these pages on your site that could be consolidated into the pages that actually have potential to deliver traffic. What are some common causes of index bloat? Some common causes we might have. So if we've got all of these URLs that are indexed and not receiving traffic, in a lot of sites that couldn't happen, right? If you have an editorial policy where you're constantly reviewing and creating pages based on demand, you would hope that this just wouldn't happen. But on a lot of sites, it does happen, and there are two sort of groups of common reasons for that. This is just based on things that I've seen as a consultant in the past. One is if you have blogs or user-generated content, often you will end up generating a lot of thin pages on similar topics. So I've worked on sites in the past where they'll have a blog where they post any kind of business announcements. They've hired a new member of staff. They've opened a new branch. They've won some kind of award. They had a Christmas party. Or any press release, anything like that goes on the blog, and you end up with this huge number of indexed pages, none of which were ever really designed to get search traffic in the first place. And similarly, if you've got a forum section or something like this, users are going to do the same. They're going to generate threads about whatever they happen to be thinking about. Those will all get indexed. That can be a source of traffic, but it can also be a source of a lot of thin URLs on very similar topics. The second sort of group to my mind is listings or products. So if you imagine a real estate website or a used car website or a job listings board, anything like this, a marketplace, you're going to get a lot of these pages that just come and go. A page is going to be created for a job listing. A couple of months later, it will be taken down. This is happening all the time. They're all pretty low value, very specific pages. Most of them will never get any traffic. And similarly on an e-commerce site, on a big e-commerce site, you've got lots of individual products. Some of them are going to be extraordinarily long tail and realistically never get any traffic because they're too similar to some other page. How to reduce index bloat So in both of these cases, you can generate this very large number of URLs that are getting basically no traffic. So what might we actually do about this or decide whether we want to do something about this? 1. Identify URLs with almost no traffic So the first step I would take is identify URLs which have near as dammit no traffic. And a rule of thumb I've often used in the past is are they getting on average less than one click a month or something like this? You can draw a very low bar. And on sites that are affected by this in a big way, you're still going to pick up a lot of pages that are getting literally zero traffic in most cases. Remember, by the way, if you're looking at this from an organic lens, do check other channels as well. You don't want to accidentally remove something that it actually isn't really important to the social or email team or something like this. 2. Improve any pages that are opportunities Next up, improve any that are opportunities. So that's kind of a big catchall statement. But if some of these pages, that you identify, perhaps you used to get a lot of traffic but have become out of date or something like that, or you think they do actually have quality content on them, maybe there's a technical SEO issue that's holding them back, find any that are actually worth doing something with. Perhaps they might have a lot of links, for example. You don't want to just blanket wipe out this sort of latent value that you have. Do something with it if you can. 3. Consolidate or cull pages you\u2019re not able to improve And then what's left, you've got this big bunch of pages that get zero traffic that you don't think are any kind of opportunity. So there's a few different ways you can go, and you're probably going to want to go and mix. So anywhere you have either existing or potential pages that match the intent or are very similar, you're basically doing the same thing. So for example, if you've got one of these very specific product ",
      "content_length": 2284,
      "category": "seo",
      "priority": "medium",
      "base_weight": 0.6,
      "raw_score": 0.6900000000000001,
      "scrape_method": "requests"
    },
    {
      "content_id": "935e76e697b87b6010ec5459b2b8296e",
      "source_id": "moz",
      "source_name": "Moz Blog",
      "title": "How to Claim, Verify, and Manage Google Business Profiles at Scale",
      "url": "https://moz.com/blog/manage-and-verify-gbp-at-scale",
      "author": "",
      "published_at": "2025-09-17T00:00:00",
      "fetched_at": "2025-10-17T17:30:51.155256",
      "content_type": "article",
      "content_text": "Table of Contents If your business has dozens, or even hundreds, of locations on Google Maps, managing them all can quickly become overwhelming. Verification issues, unwanted edits, and policy compliance risks can appear out of nowhere, turning profile maintenance into a time-consuming headache. There IS a right way to claim, verify, and manage Google Business Profiles (GBPs) at scale, to give you a streamlined process to maintain accuracy on your listings at all times, reduce errors when making updates, and keep control of every location efficiently \u2013 without letting them consume your week. Set up your GBP management Claim and Verify All Listings with a Domain Email We all know how to claim and verify a Google listing, but many don\u2019t realize that the account you use to claim and verify a listing really matters. The email you use signals to Google that you are authorized to manage this business profile and can make the verification process and future edits easier. For that reason, always try to use a domain-level email for the brand when claiming or verifying a listing. If your listings are already claimed, audit them for unwanted users and remove anyone unnecessary. You want as few users with direct access to the individual profiles as possible. My recommended set-up is to have a domain email as the primary owner on all individual listings and then add another domain email as a backup owner. To give other users access to manage the listing, I recommend using business groups. How to use GBP business groups Business groups are Google\u2019s solution for giving agency accounts the ability to streamline and more easily manage user access to listings. When you first set up an agency account, you will want to organize all the listings you have access to into business groups. You can organize these however you like, based on your business set-up. For example, if you manage listings for multiple brands, you can organize the groups by brand. If they all belong to a single brand, you can organize them by location instead. For franchises where multiple locations may be owned by the same person, you can create a separate group for that franchisee and give them access to all their locations by adding them to the group. You also have the option to keep all locations in one group. Ultimately, it depends on your preference and how you want to delegate user access. The main function of business groups is to simplify user access management. Instead of adding users directly to individual listings, you can give them access at the group level. This way, users can manage multiple listings at once without needing to be added to each one individually. You can add as many users as you want to each group. Make sure those users are not sending any mass email blasts or making Google Maps edits with the email address connected to the group. If their user account is restricted by Google , it can cause all the locations in the group to be suspended. To create a business group, click \u201cCreate group\u201d next to the group dropdown in your GBP dashboard. You can name it whatever you want, but keep in mind that anyone added to the business group can see the name. You can then add users to the business group by going into the group settings and clicking \u201cManage users\u201d. This section is also where you will find the business group ID. To add business listings to the group, add the business group ID as a user on the listing. Anyone you give access to manage the group will be able to manage each listing added into that group. Bulk verify your locations Bulk verification is available for businesses with 10 or more storefronts. In order to be eligible, you need to provide proof of the legitimacy of each address to Google. This can include storefront photos and business documents such as utility bills, business licenses or registration, leases, or government documents. Make sure your website also lists all your business locations and includes details about your services. During the bulk verification process, Google will audit all your locations to make sure they comply with their guidelines, and they can even help verify unverified storefront locations manually. After the account is bulk verified, and you get the green checkmark, you will be able to add new locations, and they will be verified automatically. Keep in mind, Google has the power to request verification of a business at any time, even if it is part of a bulk verified account, so it\u2019s important to keep relevant documents on hand and maintain proper signage at each location. Sterling Sky has a great guide on the bulk verification process . If your business is eligible to apply, check it out. Avoid common GBP pitfalls Monitor and respond to unwanted updates Set up email alerts to track changes suggested by users or Google. This is a super underrated tip because it lets you catch unwanted edits quickly, so you can correct them fast. You need to ensure each location maintains accurate, up-to-date information across all listings at all times, and you may be surprised how many edits are made on Google Maps daily! If you see the same edit being approved over and over, check the business website to make sure it's consistent with what you want to show on the listing. Google uses the business website as the main source of truth, so maintaining consistency on it is key to minimizing unwanted updates on your Google business listings. API tools can help quickly fix incorrect edits to the profile automatically, but you ultimately need to correct the source of the problem. Too many API edits in a row can raise red flags with Google. Also, make sure the information in the API matches what you want to show in GBP\u00a0at all times. If it doesn\u2019t, a tool with API access could be the culprit for your unwanted Google listings updates!\u00a0You can check if there are any tools with API access to your profiles here . Carefully use APIs for bulk edits GBP API tools can be a dream for bulk edits, and I highly recommend using one (there are many, including Moz Local ). Exercise caution when making mass edits because things can go south quickly. I once helped a business on the GBP community forum that had all their locations go into reverification status after a category edit they made through an API tool. I\u2019m talking dozens of profiles suddenly disappearing from Maps after one edit. In the end, they had to reverify each one of them individually. So be careful out there! Do not push more than one major edit (name, website, phone number, address, category) at a time. Test API edits on a few individual listings first, before pushing to all profiles. Managing multiple locations doesn\u2019t have to be chaotic Moz Local brings all your locations into one dashboard Optimize my local listings Ongoing Oversight to Prevent Suspensions Ensure every location consistently follows Google\u2019s guidelines , especially if they are storefront businesses with the address showing. If the signage isn\u2019t visible in Street View, take a photo of your signage and post it to the Google Business profile. Capture as much of the surrounding area as possible, including street signs, neighboring businesses, monuments, etc. Also, make sure your website reflects all of your locations. Maintain a location hub page on your brand website that lists every location with full NAPs (name, address, phone number). This should always match what\u2019s on Google Maps, especially if you\u2019re trying to get a listing reinstated. Pro Tip: Google requires that the phone number on the GBP listing match what\u2019s on the website. Missing or inconsistent numbers can trigger a suspension or block an appeal. If you\u2019re using dedicated GBP call tracking (which you should!), this can pose a challenge for multi-location businesses. A workaround is to get a second unique number for each location (this can be a tracking number) and place it on the dedicated location page in the body content. Then, add that number to the additional phone number field on the GBP. This number won\u2019t show on the live profile, so it won\u2019t interfere with your GBP call tracking data. Set your multi-location business up for GBP success! Start by establishing a secure and well-organized Google Business Profile management system. This means verifying all locations correctly, assigning proper access through business groups, and setting up processes to monitor updates and prevent errors. With this foundation in place, managing dozens or hundreds of locations becomes manageable, and you can spend more time focusing on executing location-specific local SEO strategies that can actually help grow the business. The author's views are entirely their own (excluding the unlikely event of hypnosis) and may not always reflect the views of Moz. Written by Elizabeth Rule Elizabeth is an expert local SEO analyst who has been working in the industry for over a decade. She has a passion for content creation and loves working with local businesses to develop their website\u2019s authority and expertise through well-written, helpful content. As a Google Business Profile Product Expert, she is also in the unique position to help businesses crack the code to gain valuable visibility in local search maps and help solve complex GBP issues. She currently works as a Local SEO Analyst at Sterling Sky , a Local SEO Agency in Canada and the USA. She is also a faculty member and speaker at Local U SEO Conference . Find her on X @ownyourserp Scale revenue from SEO with Moz Pro Get started with the all-in-one SEO tool marketers trust Start your free trial Get the latest SEO tips and strategies in your inbox Back to Top",
      "content_length": 1637,
      "category": "seo",
      "priority": "medium",
      "base_weight": 0.6,
      "raw_score": 0.65,
      "scrape_method": "requests"
    }
  ]
}